{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  patient_id  breast_density left or right breast  pathology  \\\n",
      "0    P_00001               3                 LEFT  MALIGNANT   \n",
      "1    P_00004               3                 LEFT     BENIGN   \n",
      "2    P_00009               3                RIGHT  MALIGNANT   \n",
      "3    P_00018               2                RIGHT     BENIGN   \n",
      "4    P_00021               1                 LEFT     BENIGN   \n",
      "\n",
      "                        image file path  \n",
      "0   Mass-Training_P_00001_LEFT_CC_1.png  \n",
      "1   Mass-Training_P_00004_LEFT_CC_1.png  \n",
      "2  Mass-Training_P_00009_RIGHT_CC_1.png  \n",
      "3  Mass-Training_P_00018_RIGHT_CC_1.png  \n",
      "4   Mass-Training_P_00021_LEFT_CC_1.png  \n",
      "  patient_id  breast_density left or right breast  pathology  \\\n",
      "0    P_00016               4                 LEFT  MALIGNANT   \n",
      "1    P_00017               2                 LEFT  MALIGNANT   \n",
      "2    P_00032               3                RIGHT     BENIGN   \n",
      "3    P_00037               3                RIGHT  MALIGNANT   \n",
      "4    P_00066               4                 LEFT  MALIGNANT   \n",
      "\n",
      "                    image file path  \n",
      "0   Mass-Test_P_00016_LEFT_CC_1.png  \n",
      "1   Mass-Test_P_00017_LEFT_CC_1.png  \n",
      "2  Mass-Test_P_00032_RIGHT_CC_1.png  \n",
      "3  Mass-Test_P_00037_RIGHT_CC_1.png  \n",
      "4   Mass-Test_P_00066_LEFT_CC_1.png  \n",
      "(607, 5)\n",
      "(177, 5)\n",
      "pathology\n",
      "MALIGNANT                  296\n",
      "BENIGN                     273\n",
      "BENIGN_WITHOUT_CALLBACK     38\n",
      "Name: count, dtype: int64\n",
      "pathology\n",
      "BENIGN                     94\n",
      "MALIGNANT                  67\n",
      "BENIGN_WITHOUT_CALLBACK    16\n",
      "Name: count, dtype: int64\n",
      "breast_density\n",
      "2    276\n",
      "3    156\n",
      "1    128\n",
      "4     47\n",
      "Name: count, dtype: int64\n",
      "breast_density\n",
      "2    78\n",
      "3    53\n",
      "4    24\n",
      "1    22\n",
      "Name: count, dtype: int64\n",
      "left or right breast\n",
      "RIGHT    311\n",
      "LEFT     296\n",
      "Name: count, dtype: int64\n",
      "left or right breast\n",
      "RIGHT    89\n",
      "LEFT     88\n",
      "Name: count, dtype: int64\n",
      "pathology\n",
      "MALIGNANT    296\n",
      "BENIGN       273\n",
      "Name: count, dtype: int64\n",
      "pathology\n",
      "BENIGN       94\n",
      "MALIGNANT    67\n",
      "Name: count, dtype: int64\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "119\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "362\n",
      "363\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "379\n",
      "380\n",
      "381\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "515\n",
      "516\n",
      "517\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "589\n",
      "590\n",
      "591\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "58\n",
      "59\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "88\n",
      "91\n",
      "92\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "126\n",
      "127\n",
      "128\n",
      "130\n",
      "131\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "176\n",
      "569\n",
      "161\n",
      "[array([[0.5511792 , 0.5433378 , 0.53617424, ..., 0.803359  , 0.791674  ,\n",
      "        0.78636485],\n",
      "       [0.5436165 , 0.5578477 , 0.5443515 , ..., 0.7876761 , 0.78640074,\n",
      "        0.7732485 ],\n",
      "       [0.54288626, 0.5282502 , 0.5255513 , ..., 0.77368   , 0.78914994,\n",
      "        0.76570827],\n",
      "       ...,\n",
      "       [0.5588245 , 0.60269797, 0.6075648 , ..., 0.7860899 , 0.79089504,\n",
      "        0.7925696 ],\n",
      "       [0.59595925, 0.60617346, 0.62738955, ..., 0.7956702 , 0.79530025,\n",
      "        0.81266356],\n",
      "       [0.595865  , 0.6152211 , 0.5901918 , ..., 0.7995099 , 0.7962961 ,\n",
      "        0.7895655 ]], dtype=float32), 1]\n",
      "[array([[0.85348177, 0.8672165 , 0.8726692 , ..., 0.7110579 , 0.7106392 ,\n",
      "        0.7118743 ],\n",
      "       [0.853809  , 0.8742977 , 0.87845546, ..., 0.73537976, 0.7413137 ,\n",
      "        0.7369509 ],\n",
      "       [0.85182655, 0.8620218 , 0.860339  , ..., 0.74146295, 0.7482552 ,\n",
      "        0.74287874],\n",
      "       ...,\n",
      "       [0.37626833, 0.37994108, 0.37908718, ..., 0.5187111 , 0.5211535 ,\n",
      "        0.51566   ],\n",
      "       [0.3905567 , 0.38637164, 0.3774745 , ..., 0.51134396, 0.5133927 ,\n",
      "        0.5202659 ],\n",
      "       [0.38769117, 0.38943568, 0.37412596, ..., 0.5155275 , 0.5176502 ,\n",
      "        0.5060874 ]], dtype=float32), 1]\n",
      "(569, 224, 224)\n",
      "(161, 224, 224)\n",
      "0.7262387\n",
      "0.1641206\n",
      "0.738633\n",
      "0.16494963\n",
      "(525, 224, 224)\n",
      "(146, 224, 224)\n",
      "0.7236728\n",
      "0.16735339\n",
      "0.7408603\n",
      "0.15718412\n",
      "float32\n",
      "float32\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Importando as bibliotecas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Carregando os dados\n",
    "\n",
    "WORK_DIR = os.getcwd()\n",
    "UP_DIR = os.path.dirname(WORK_DIR)\n",
    "UP_DIR = os.path.dirname(UP_DIR)\n",
    "DATADIR = os.path.join(UP_DIR, 'data', 'cbis-ddsm')\n",
    "CATEGORIES = ['BENIGN', 'MALIGNANT']\n",
    "\n",
    "# csv com os dados das imagens\n",
    "df_train = pd.read_csv(os.path.join(DATADIR, 'Train', 'clean_mass_train_description2.csv'))\n",
    "df_test = pd.read_csv(os.path.join(DATADIR, 'Test', 'clean_mass_test_description2.csv'))\n",
    "\n",
    "# Verificando os dados\n",
    "print(df_train.head())\n",
    "print(df_test.head())\n",
    "\n",
    "# Verificando o tamanho dos dados\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "\n",
    "# Verificando a distribuição das classes\n",
    "print(df_train['pathology'].value_counts())\n",
    "print(df_test['pathology'].value_counts())\n",
    "\n",
    "# Verificando a distribuição das classes\n",
    "print(df_train['breast_density'].value_counts())\n",
    "print(df_test['breast_density'].value_counts())\n",
    "\n",
    "# Verificando a distribuição das classes\n",
    "print(df_train['left or right breast'].value_counts())\n",
    "print(df_test['left or right breast'].value_counts())\n",
    "\n",
    "\n",
    "# Exclude Begnin withouth callback\n",
    "df_train = df_train[df_train['pathology'] != 'BENIGN_WITHOUT_CALLBACK']\n",
    "df_test = df_test[df_test['pathology'] != 'BENIGN_WITHOUT_CALLBACK']\n",
    "\n",
    "\n",
    "# Verificando a distribuição das classes\n",
    "print(df_train['pathology'].value_counts())\n",
    "print(df_test['pathology'].value_counts())\n",
    "\n",
    "# imports para processamento dos dados \n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Preparando os dados\n",
    "\n",
    "CATEGORIES = ['BENIGN', 'MALIGNANT']\n",
    "IMG_SIZE = 224\n",
    "def create_training_data(df, img_size, categories, datadir):\n",
    "    training_data = []\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            img_array = plt.imread(os.path.join(datadir, row['image file path']),0)\n",
    "            new_array = cv2.resize(img_array, (img_size, img_size))\n",
    "            # Normalizing the data\n",
    "            # To Numpy array\n",
    "            new_array = np.array(new_array)\n",
    "            training_data.append([new_array, categories.index(row['pathology'])])\n",
    "            # Benign = 0, Malignant = 1\n",
    "                        \n",
    "\n",
    "            print(index)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "    return training_data\n",
    "\n",
    "train_data = create_training_data(df_train, IMG_SIZE, CATEGORIES, os.path.join(DATADIR, 'Train', 'Train'))\n",
    "testing_data = create_training_data(df_test, IMG_SIZE, CATEGORIES, os.path.join(DATADIR, 'Test', 'Test'))\n",
    "\n",
    "# Verificando o tamanho dos dados\n",
    "print(len(train_data))\n",
    "print(len(testing_data))\n",
    "\n",
    "# Verificando os dados\n",
    "print(train_data[0])\n",
    "print(testing_data[0])\n",
    "\n",
    "# Embaralhando os dados\n",
    "random.shuffle(train_data)\n",
    "random.shuffle(testing_data)\n",
    "\n",
    "\n",
    "# Separando os dados\n",
    "X_train = []\n",
    "y_train = []\n",
    "for features, label in train_data:\n",
    "    X_train.append(features)\n",
    "    y_train.append(label)\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "for features, label in testing_data:\n",
    "    X_test.append(features)\n",
    "    y_test.append(label)\n",
    "\n",
    "\n",
    "# Convertendo para numpy array\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "\n",
    "# Métricas de media e desvio padrão dos dados\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(np.mean(X_train))\n",
    "print(np.std(X_train))\n",
    "print(np.mean(X_test))\n",
    "print(np.std(X_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Merge data and separate in train and test\n",
    "X = np.concatenate((X_train, X_test), axis=0)\n",
    "y = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Split data val\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Métricas de media e desvio padrão dos dados\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(np.mean(X_train))\n",
    "print(np.std(X_train))\n",
    "print(np.mean(X_test))\n",
    "print(np.std(X_test))\n",
    "\n",
    "#print dtypes\n",
    "print(X_train.dtype)\n",
    "print(X_test.dtype)\n",
    "print(X_train[0].dtype)\n",
    "\n",
    "# Equalizing histogram\n",
    "def equalize_hist(img):\n",
    "    # Convert to uint8\n",
    "    img = (img*255).astype(np.uint8)\n",
    "    img = cv2.equalizeHist(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "X_test_eq = []\n",
    "for img in X_test:\n",
    "    X_test_eq.append(equalize_hist(img))\n",
    "\n",
    "X_train_eq = []\n",
    "for img in X_train:\n",
    "    X_train_eq.append(equalize_hist(img))\n",
    "\n",
    "X_train_eq = np.array(X_train_eq)\n",
    "X_test_eq = np.array(X_test_eq)\n",
    "\n",
    "X_train = X_train_eq\n",
    "X_test = X_test_eq\n",
    "\n",
    "\n",
    "# To 3 channels\n",
    "X_train_o = np.stack((X_train,)*3, axis=-1)\n",
    "X_test_o = np.stack((X_test,)*3, axis=-1)\n",
    "\n",
    "# Save dataset in dataset from hugging face format\n",
    "# create folder for dataset\n",
    "DATASET_DIR = os.path.join(UP_DIR, 'data', 'dataset')\n",
    "if not os.path.exists(DATASET_DIR):\n",
    "    os.makedirs(DATASET_DIR)\n",
    "\n",
    "# Create folder for train, test and val\n",
    "TRAIN_DIR = os.path.join(DATASET_DIR, 'train')\n",
    "TEST_DIR = os.path.join(DATASET_DIR, 'test')\n",
    "VAL_DIR = os.path.join(DATASET_DIR, 'val')\n",
    "\n",
    "if not os.path.exists(TRAIN_DIR):\n",
    "    os.makedirs(TRAIN_DIR)\n",
    "\n",
    "if not os.path.exists(TEST_DIR):\n",
    "    os.makedirs(TEST_DIR)\n",
    "\n",
    "if not os.path.exists(VAL_DIR):\n",
    "    os.makedirs(VAL_DIR)\n",
    "\n",
    "# Create folder for benign and malignant\n",
    "    \n",
    "BENIGN_DIR = os.path.join(TRAIN_DIR, 'benign')\n",
    "MALIGNANT_DIR = os.path.join(TRAIN_DIR, 'malignant')\n",
    "\n",
    "if not os.path.exists(BENIGN_DIR):\n",
    "    os.makedirs(BENIGN_DIR)\n",
    "\n",
    "if not os.path.exists(MALIGNANT_DIR):\n",
    "    os.makedirs(MALIGNANT_DIR)\n",
    "\n",
    "# Create folder for benign and malignant\n",
    "\n",
    "BENIGN_DIR = os.path.join(TEST_DIR, 'benign')\n",
    "MALIGNANT_DIR = os.path.join(TEST_DIR, 'malignant')\n",
    "\n",
    "if not os.path.exists(BENIGN_DIR):\n",
    "    os.makedirs(BENIGN_DIR)\n",
    "\n",
    "if not os.path.exists(MALIGNANT_DIR):\n",
    "    os.makedirs(MALIGNANT_DIR)\n",
    "\n",
    "# Create folder for benign and malignant\n",
    "    \n",
    "BENIGN_DIR = os.path.join(VAL_DIR, 'benign')\n",
    "MALIGNANT_DIR = os.path.join(VAL_DIR, 'malignant')\n",
    "\n",
    "if not os.path.exists(BENIGN_DIR):\n",
    "    os.makedirs(BENIGN_DIR)\n",
    "\n",
    "if not os.path.exists(MALIGNANT_DIR):\n",
    "    os.makedirs(MALIGNANT_DIR)\n",
    "\n",
    "# Save images in folders\n",
    "def save_images(X, y, categories, datadir):\n",
    "    for i in range(len(X)):\n",
    "        if y[i] == 0:\n",
    "            cv2.imwrite(os.path.join(datadir, 'benign', str(i)+'.png'), X[i])\n",
    "        else:\n",
    "            cv2.imwrite(os.path.join(datadir, 'malignant', str(i)+'.png'), X[i])\n",
    "\n",
    "save_images(X_train_o, y_train, CATEGORIES, TRAIN_DIR)\n",
    "\n",
    "save_images(X_test_o, y_test, CATEGORIES, TEST_DIR)\n",
    "\n",
    "save_images(X_val, y_val, CATEGORIES, VAL_DIR)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Import models \n",
    "from tensorflow.keras.applications import VGG16, ResNet50, MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "\n",
    "# Vit models\n",
    "from vit_keras import vit\n",
    "from vit_keras import utils as vit_utils\n",
    "\n",
    "# Pre processing layers\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input as preprocess_input_vgg16\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input as preprocess_input_resnet50\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as preprocess_input_mobilenetv2 \n",
    "\n",
    "# Function for bulk training \n",
    "# Function for resnet50\n",
    "def train_model(X_train, X_test, y_train, y_test, X_val, y_val , training_info, model, callbacks , optimizer, loss, metrics, epochs):\n",
    "   \n",
    "    # Compile model\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    # Train model\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, validation_data=(X_test, y_test), callbacks=callbacks)\n",
    "    # Save history in csv\n",
    "    filename = 'history-'+training_info+'.csv'\n",
    "    history_df = pd.DataFrame(history.history)\n",
    "    # Save to csv in data folder\n",
    "    history_df.to_csv(os.path.join(UP_DIR, 'data', 'history', filename), index=False)\n",
    "\n",
    "    # Plot history\n",
    "    plt.plot(history.history['accuracy'], label='accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "    filename = 'plot-'+training_info+'.png'\n",
    "    plt.savefig(os.path.join(UP_DIR, 'data', 'history', filename))\n",
    "    plt.show()\n",
    "\n",
    "    # PLot Accuracy\n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "    filename = 'plot-loss-'+training_info+'.png'\n",
    "    plt.savefig(os.path.join(UP_DIR, 'data', 'history', filename))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "def train_model_resnet50(X_train, X_test, y_train, y_test, X_val, y_val , lrfunc, training_info , epochs):\n",
    "   # Model\n",
    "    resnet50 = ResNet50(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    # Congelando as camadas\n",
    "    for layer in resnet50.layers:\n",
    "        layer.trainable = False\n",
    "    # Head do modelo\n",
    "    x = resnet50.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    preds = Dense(1, activation='sigmoid')(x)\n",
    "    # Modelo\n",
    "    model = Model(inputs=resnet50.input, outputs=preds)\n",
    "    \n",
    "    # Pre processamento\n",
    "    X_train = preprocess_input_resnet50(X_train)\n",
    "    X_test = preprocess_input_resnet50(X_test)\n",
    "    X_val = preprocess_input_resnet50(X_val)\n",
    "    # Otimizador usando callback\n",
    "    sgd = tf.keras.optimizers.SGD(lr=0.0, momentum=0.9)\n",
    "\n",
    "\n",
    "    # LR Scheduler\n",
    "    lrate = tf.keras.callbacks.LearningRateScheduler(lrfunc, verbose=1)\n",
    "    # Early stopping\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='auto')\n",
    "    # Checkpoint\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "    # Train model\n",
    "    train_model(X_train, X_test, y_train, y_test, X_val, y_val, training_info, model, [lrate, early_stopping, checkpoint], sgd, 'binary_crossentropy', ['accuracy'], epochs)\n",
    "\n",
    "\n",
    "def train_model_vgg16(X_train, X_test, y_train, y_test, X_val, y_val , lrfunc, training_info , epochs):\n",
    "   # Model\n",
    "    vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    # Congelando as camadas\n",
    "    for layer in vgg16.layers:\n",
    "        layer.trainable = False\n",
    "    # Head do modelo\n",
    "    x = vgg16.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    preds = Dense(1, activation='sigmoid')(x)\n",
    "    # Modelo\n",
    "    model = Model(inputs=vgg16.input, outputs=preds)\n",
    "    \n",
    "    # Pre processamento\n",
    "    X_train = preprocess_input_vgg16(X_train)\n",
    "    X_test = preprocess_input_vgg16(X_test)\n",
    "    X_val = preprocess_input_vgg16(X_val)\n",
    "\n",
    "    # Otimizador usando callback\n",
    "    sgd = tf.keras.optimizers.SGD(lr=0.0, momentum=0.9)\n",
    "\n",
    "\n",
    "    # LR Scheduler\n",
    "    lrate = tf.keras.callbacks.LearningRateScheduler(lrfunc, verbose=1)\n",
    "    # Early stopping\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='auto')\n",
    "    # Checkpoint\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "    # Train model\n",
    "    train_model(X_train, X_test, y_train, y_test, X_val, y_val, training_info, model, [lrate, early_stopping, checkpoint], sgd, 'binary_crossentropy', ['accuracy'], epochs)\n",
    "\n",
    "def train_model_mobilenetv2(X_train, X_test, y_train, y_test, X_val, y_val , lrfunc, training_info , epochs):\n",
    "   # Model\n",
    "    mobilenetv2 = MobileNetV2(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    # Congelando as camadas\n",
    "    for layer in mobilenetv2.layers:\n",
    "        layer.trainable = False\n",
    "    # Head do modelo\n",
    "    x = mobilenetv2.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    preds = Dense(1, activation='sigmoid')(x)\n",
    "    # Modelo\n",
    "    model = Model(inputs=mobilenetv2.input, outputs=preds)\n",
    "    \n",
    "    # Pre processamento\n",
    "    X_train = preprocess_input_mobilenetv2(X_train)\n",
    "    X_test = preprocess_input_mobilenetv2(X_test)\n",
    "    X_val = preprocess_input_mobilenetv2(X_val)\n",
    "\n",
    "    # Otimizador usando callback\n",
    "    sgd = tf.keras.optimizers.SGD(lr=0.0, momentum=0.9)\n",
    "\n",
    "\n",
    "    # LR Scheduler\n",
    "    lrate = tf.keras.callbacks.LearningRateScheduler(lrfunc, verbose=1)\n",
    "    # Early stopping\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='auto')\n",
    "    # Checkpoint\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "    # Train model\n",
    "    train_model(X_train, X_test, y_train, y_test, X_val, y_val, training_info, model, [lrate, early_stopping, checkpoint], sgd, 'binary_crossentropy', ['accuracy'], epochs)\n",
    "\n",
    "def train_model_vit(X_train, X_test, y_train, y_test, X_val, y_val , lrfunc, training_info , epochs):\n",
    "    # Model\n",
    "    vit_model = vit.vit_b16(\n",
    "    image_size = IMG_SIZE,\n",
    "    activation = 'sigmoid',\n",
    "    pretrained = True,\n",
    "    include_top = True,\n",
    "    pretrained_top = False,\n",
    "    classes = 1,\n",
    "    weights = 'imagenet21k+imagenet2012',\n",
    "    \n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Pre processamento\n",
    "    X_train = vit.preprocess_inputs(X_train)\n",
    "    X_test = vit.preprocess_inputs(X_test)\n",
    "    X_val = vit.preprocess_inputs(X_val)\n",
    "\n",
    "    \n",
    "    #Only the head is trained\n",
    "    vit_model.trainable = True\n",
    "    for layer in vit_model.layers[:-1]:\n",
    "        layer.trainable = False\n",
    "\n",
    "\n",
    "    # Otimizador usando callback\n",
    "    sgd = tf.keras.optimizers.SGD(lr=0.0, momentum=0.5)\n",
    "\n",
    "    # LR Scheduler\n",
    "    lrate = tf.keras.callbacks.LearningRateScheduler(lrfunc, verbose=1)\n",
    "    # Early stopping\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=8, verbose=1, mode='auto')\n",
    "    # Checkpoint\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(training_info+'.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "    # Train model\n",
    "    train_model(X_train, X_test, y_train, y_test, X_val, y_val, training_info, vit_model, [lrate, early_stopping, checkpoint], sgd, 'binary_crossentropy', ['accuracy'], epochs)\n",
    "\n",
    "# Hugging Face connection\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Train 10 models with different lr peaks\n",
    "\n",
    "# Resnet50\n",
    "for i in range(10):\n",
    "\n",
    "    def lrate(epoch):\n",
    "        # Warmup\n",
    "        wrmup = 20\n",
    "        peak = 0.00005*(i+1)\n",
    "        stay = 10\n",
    "        lenght_decay = 60\n",
    "        if epoch < wrmup:\n",
    "            return peak * (epoch+1) / wrmup\n",
    "        elif epoch < wrmup + stay:\n",
    "            return peak\n",
    "        else:\n",
    "            return peak/2 * (1 + np.cos((epoch-wrmup) / (  lenght_decay - stay - wrmup) * np.pi))\n",
    "\n",
    "    train_model_resnet50(X_train_o, X_test_o, y_train, y_test, X_test_o, y_test, lrate, 'resnet50-'+str(i), 60)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# VGG16\n",
    "for i in range(10):\n",
    "\n",
    "    def lrate(epoch):\n",
    "        # Warmup\n",
    "        wrmup = 20\n",
    "        peak = 0.00005*(i+1)\n",
    "        stay = 10\n",
    "        lenght_decay = 60\n",
    "        if epoch < wrmup:\n",
    "            return peak * (epoch+1) / wrmup\n",
    "        elif epoch < wrmup + stay:\n",
    "            return peak\n",
    "        else:\n",
    "            return peak/2 * (1 + np.cos((epoch-wrmup) / (  lenght_decay - stay - wrmup) * np.pi))\n",
    "\n",
    "    train_model_vgg16(X_train_o, X_test_o, y_train, y_test, X_test_o, y_test,  lrate, 'vgg16-'+str(i), 60)\n",
    "    # Clean vram\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-21 19:06:56.851407: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-21 19:06:57.062070: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-21 19:06:57.062150: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-21 19:06:57.066234: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-21 19:06:57.066334: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-21 19:06:57.066379: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-21 19:06:57.233338: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-21 19:06:57.233426: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-21 19:06:57.233434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-03-21 19:06:57.233494: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-21 19:06:57.233515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4084 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660, pci bus id: 0000:0d:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 2.5e-06.\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-21 19:07:01.287150: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2024-03-21 19:07:04.199340: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f0ddc82fb50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-03-21 19:07:04.199400: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660, Compute Capability 7.5\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1711058824.255857  725010 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - ETA: 0s - loss: 0.7013 - accuracy: 0.4812\n",
      "Epoch 1: val_loss improved from inf to 0.72258, saving model to model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vdgaete/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 11s 308ms/step - loss: 0.7013 - accuracy: 0.4812 - val_loss: 0.7226 - val_accuracy: 0.4521 - lr: 2.5000e-06\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 5e-06.\n",
      "Epoch 2/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.7014 - accuracy: 0.4809\n",
      "Epoch 2: val_loss improved from 0.72258 to 0.72180, saving model to model.h5\n",
      "19/19 [==============================] - 2s 96ms/step - loss: 0.7009 - accuracy: 0.4829 - val_loss: 0.7218 - val_accuracy: 0.4521 - lr: 5.0000e-06\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 7.500000000000001e-06.\n",
      "Epoch 3/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6995 - accuracy: 0.4913\n",
      "Epoch 3: val_loss improved from 0.72180 to 0.72057, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.7002 - accuracy: 0.4897 - val_loss: 0.7206 - val_accuracy: 0.4452 - lr: 7.5000e-06\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 4/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6992 - accuracy: 0.4913\n",
      "Epoch 4: val_loss improved from 0.72057 to 0.71883, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.6993 - accuracy: 0.4897 - val_loss: 0.7188 - val_accuracy: 0.4452 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 1.25e-05.\n",
      "Epoch 5/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6970 - accuracy: 0.4931\n",
      "Epoch 5: val_loss improved from 0.71883 to 0.71684, saving model to model.h5\n",
      "19/19 [==============================] - 2s 98ms/step - loss: 0.6978 - accuracy: 0.4897 - val_loss: 0.7168 - val_accuracy: 0.4452 - lr: 1.2500e-05\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 1.5000000000000002e-05.\n",
      "Epoch 6/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6963 - accuracy: 0.5052\n",
      "Epoch 6: val_loss improved from 0.71684 to 0.71482, saving model to model.h5\n",
      "19/19 [==============================] - 2s 98ms/step - loss: 0.6962 - accuracy: 0.5051 - val_loss: 0.7148 - val_accuracy: 0.4521 - lr: 1.5000e-05\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 1.75e-05.\n",
      "Epoch 7/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6942 - accuracy: 0.5156\n",
      "Epoch 7: val_loss improved from 0.71482 to 0.71291, saving model to model.h5\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.6942 - accuracy: 0.5137 - val_loss: 0.7129 - val_accuracy: 0.4589 - lr: 1.7500e-05\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 2e-05.\n",
      "Epoch 8/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6918 - accuracy: 0.5278\n",
      "Epoch 8: val_loss improved from 0.71291 to 0.70952, saving model to model.h5\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.6918 - accuracy: 0.5291 - val_loss: 0.7095 - val_accuracy: 0.4658 - lr: 2.0000e-05\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 2.25e-05.\n",
      "Epoch 9/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6892 - accuracy: 0.5365\n",
      "Epoch 9: val_loss improved from 0.70952 to 0.70609, saving model to model.h5\n",
      "19/19 [==============================] - 2s 99ms/step - loss: 0.6893 - accuracy: 0.5325 - val_loss: 0.7061 - val_accuracy: 0.4932 - lr: 2.2500e-05\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 2.5e-05.\n",
      "Epoch 10/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6870 - accuracy: 0.5469\n",
      "Epoch 10: val_loss improved from 0.70609 to 0.70273, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.6865 - accuracy: 0.5479 - val_loss: 0.7027 - val_accuracy: 0.5068 - lr: 2.5000e-05\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 2.75e-05.\n",
      "Epoch 11/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6835 - accuracy: 0.5625\n",
      "Epoch 11: val_loss improved from 0.70273 to 0.70119, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.6838 - accuracy: 0.5616 - val_loss: 0.7012 - val_accuracy: 0.5411 - lr: 2.7500e-05\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 3.0000000000000004e-05.\n",
      "Epoch 12/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6807 - accuracy: 0.5781\n",
      "Epoch 12: val_loss improved from 0.70119 to 0.69552, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.6809 - accuracy: 0.5753 - val_loss: 0.6955 - val_accuracy: 0.5753 - lr: 3.0000e-05\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 3.2500000000000004e-05.\n",
      "Epoch 13/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6768 - accuracy: 0.6007\n",
      "Epoch 13: val_loss improved from 0.69552 to 0.69229, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.6772 - accuracy: 0.5976 - val_loss: 0.6923 - val_accuracy: 0.5753 - lr: 3.2500e-05\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 3.5e-05.\n",
      "Epoch 14/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6743 - accuracy: 0.5972\n",
      "Epoch 14: val_loss improved from 0.69229 to 0.68894, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.6740 - accuracy: 0.5993 - val_loss: 0.6889 - val_accuracy: 0.5890 - lr: 3.5000e-05\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 3.7500000000000003e-05.\n",
      "Epoch 15/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6707 - accuracy: 0.6059\n",
      "Epoch 15: val_loss improved from 0.68894 to 0.68478, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.6705 - accuracy: 0.6079 - val_loss: 0.6848 - val_accuracy: 0.6027 - lr: 3.7500e-05\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 4e-05.\n",
      "Epoch 16/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6680 - accuracy: 0.6059\n",
      "Epoch 16: val_loss improved from 0.68478 to 0.68297, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.6683 - accuracy: 0.6027 - val_loss: 0.6830 - val_accuracy: 0.6027 - lr: 4.0000e-05\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 4.25e-05.\n",
      "Epoch 17/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6632 - accuracy: 0.6146\n",
      "Epoch 17: val_loss improved from 0.68297 to 0.67711, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.6636 - accuracy: 0.6113 - val_loss: 0.6771 - val_accuracy: 0.6096 - lr: 4.2500e-05\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 4.5e-05.\n",
      "Epoch 18/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6606 - accuracy: 0.6233\n",
      "Epoch 18: val_loss improved from 0.67711 to 0.67073, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.6610 - accuracy: 0.6216 - val_loss: 0.6707 - val_accuracy: 0.6027 - lr: 4.5000e-05\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 4.75e-05.\n",
      "Epoch 19/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6561 - accuracy: 0.6406\n",
      "Epoch 19: val_loss did not improve from 0.67073\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.6570 - accuracy: 0.6370 - val_loss: 0.6708 - val_accuracy: 0.6370 - lr: 4.7500e-05\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 5e-05.\n",
      "Epoch 20/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6540 - accuracy: 0.6337\n",
      "Epoch 20: val_loss improved from 0.67073 to 0.66833, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.6536 - accuracy: 0.6370 - val_loss: 0.6683 - val_accuracy: 0.6370 - lr: 5.0000e-05\n",
      "\n",
      "Epoch 21: LearningRateScheduler setting learning rate to 5e-05.\n",
      "Epoch 21/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6508 - accuracy: 0.6354\n",
      "Epoch 21: val_loss improved from 0.66833 to 0.66280, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.6499 - accuracy: 0.6370 - val_loss: 0.6628 - val_accuracy: 0.6438 - lr: 5.0000e-05\n",
      "\n",
      "Epoch 22: LearningRateScheduler setting learning rate to 5e-05.\n",
      "Epoch 22/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6487 - accuracy: 0.6215\n",
      "Epoch 22: val_loss improved from 0.66280 to 0.65653, saving model to model.h5\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.6480 - accuracy: 0.6250 - val_loss: 0.6565 - val_accuracy: 0.6575 - lr: 5.0000e-05\n",
      "\n",
      "Epoch 23: LearningRateScheduler setting learning rate to 5e-05.\n",
      "Epoch 23/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6442 - accuracy: 0.6267\n",
      "Epoch 23: val_loss improved from 0.65653 to 0.65512, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.6445 - accuracy: 0.6250 - val_loss: 0.6551 - val_accuracy: 0.6644 - lr: 5.0000e-05\n",
      "\n",
      "Epoch 24: LearningRateScheduler setting learning rate to 5e-05.\n",
      "Epoch 24/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6423 - accuracy: 0.6372\n",
      "Epoch 24: val_loss improved from 0.65512 to 0.65371, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.6415 - accuracy: 0.6387 - val_loss: 0.6537 - val_accuracy: 0.6712 - lr: 5.0000e-05\n",
      "\n",
      "Epoch 25: LearningRateScheduler setting learning rate to 5e-05.\n",
      "Epoch 25/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6400 - accuracy: 0.6441\n",
      "Epoch 25: val_loss improved from 0.65371 to 0.65328, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.6390 - accuracy: 0.6473 - val_loss: 0.6533 - val_accuracy: 0.6712 - lr: 5.0000e-05\n",
      "\n",
      "Epoch 26: LearningRateScheduler setting learning rate to 5e-05.\n",
      "Epoch 26/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6355 - accuracy: 0.6562\n",
      "Epoch 26: val_loss improved from 0.65328 to 0.65156, saving model to model.h5\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.6367 - accuracy: 0.6507 - val_loss: 0.6516 - val_accuracy: 0.6712 - lr: 5.0000e-05\n",
      "\n",
      "Epoch 27: LearningRateScheduler setting learning rate to 5e-05.\n",
      "Epoch 27/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6350 - accuracy: 0.6545\n",
      "Epoch 27: val_loss improved from 0.65156 to 0.64950, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.6350 - accuracy: 0.6558 - val_loss: 0.6495 - val_accuracy: 0.6712 - lr: 5.0000e-05\n",
      "\n",
      "Epoch 28: LearningRateScheduler setting learning rate to 5e-05.\n",
      "Epoch 28/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6319 - accuracy: 0.6545\n",
      "Epoch 28: val_loss improved from 0.64950 to 0.64599, saving model to model.h5\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.6324 - accuracy: 0.6558 - val_loss: 0.6460 - val_accuracy: 0.6712 - lr: 5.0000e-05\n",
      "\n",
      "Epoch 29: LearningRateScheduler setting learning rate to 5e-05.\n",
      "Epoch 29/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6284 - accuracy: 0.6528\n",
      "Epoch 29: val_loss improved from 0.64599 to 0.64270, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.6301 - accuracy: 0.6455 - val_loss: 0.6427 - val_accuracy: 0.6781 - lr: 5.0000e-05\n",
      "\n",
      "Epoch 30: LearningRateScheduler setting learning rate to 5e-05.\n",
      "Epoch 30/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6280 - accuracy: 0.6476\n",
      "Epoch 30: val_loss improved from 0.64270 to 0.64008, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.6283 - accuracy: 0.6455 - val_loss: 0.6401 - val_accuracy: 0.6849 - lr: 5.0000e-05\n",
      "\n",
      "Epoch 31: LearningRateScheduler setting learning rate to 3.7500000000000003e-05.\n",
      "Epoch 31/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6269 - accuracy: 0.6476\n",
      "Epoch 31: val_loss improved from 0.64008 to 0.63950, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.6264 - accuracy: 0.6490 - val_loss: 0.6395 - val_accuracy: 0.6918 - lr: 3.7500e-05\n",
      "\n",
      "Epoch 32: LearningRateScheduler setting learning rate to 3.516841607689501e-05.\n",
      "Epoch 32/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6256 - accuracy: 0.6493\n",
      "Epoch 32: val_loss did not improve from 0.63950\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.6255 - accuracy: 0.6490 - val_loss: 0.6397 - val_accuracy: 0.6849 - lr: 3.5168e-05\n",
      "\n",
      "Epoch 33: LearningRateScheduler setting learning rate to 3.272542485937369e-05.\n",
      "Epoch 33/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6252 - accuracy: 0.6424\n",
      "Epoch 33: val_loss improved from 0.63950 to 0.63494, saving model to model.h5\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.6244 - accuracy: 0.6473 - val_loss: 0.6349 - val_accuracy: 0.6781 - lr: 3.2725e-05\n",
      "\n",
      "Epoch 34: LearningRateScheduler setting learning rate to 3.0197792270443982e-05.\n",
      "Epoch 34/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6241 - accuracy: 0.6493\n",
      "Epoch 34: val_loss improved from 0.63494 to 0.63441, saving model to model.h5\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.6227 - accuracy: 0.6541 - val_loss: 0.6344 - val_accuracy: 0.6781 - lr: 3.0198e-05\n",
      "\n",
      "Epoch 35: LearningRateScheduler setting learning rate to 2.761321158169134e-05.\n",
      "Epoch 35/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6211 - accuracy: 0.6545\n",
      "Epoch 35: val_loss improved from 0.63441 to 0.63332, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.6216 - accuracy: 0.6507 - val_loss: 0.6333 - val_accuracy: 0.6781 - lr: 2.7613e-05\n",
      "\n",
      "Epoch 36: LearningRateScheduler setting learning rate to 2.5e-05.\n",
      "Epoch 36/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6211 - accuracy: 0.6528\n",
      "Epoch 36: val_loss improved from 0.63332 to 0.63310, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.6206 - accuracy: 0.6541 - val_loss: 0.6331 - val_accuracy: 0.6849 - lr: 2.5000e-05\n",
      "\n",
      "Epoch 37: LearningRateScheduler setting learning rate to 2.238678841830867e-05.\n",
      "Epoch 37/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6221 - accuracy: 0.6510\n",
      "Epoch 37: val_loss improved from 0.63310 to 0.63167, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.6199 - accuracy: 0.6558 - val_loss: 0.6317 - val_accuracy: 0.6781 - lr: 2.2387e-05\n",
      "\n",
      "Epoch 38: LearningRateScheduler setting learning rate to 1.980220772955602e-05.\n",
      "Epoch 38/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6190 - accuracy: 0.6528\n",
      "Epoch 38: val_loss did not improve from 0.63167\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.6189 - accuracy: 0.6524 - val_loss: 0.6321 - val_accuracy: 0.6849 - lr: 1.9802e-05\n",
      "\n",
      "Epoch 39: LearningRateScheduler setting learning rate to 1.7274575140626318e-05.\n",
      "Epoch 39/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6189 - accuracy: 0.6528\n",
      "Epoch 39: val_loss did not improve from 0.63167\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.6184 - accuracy: 0.6541 - val_loss: 0.6322 - val_accuracy: 0.6918 - lr: 1.7275e-05\n",
      "\n",
      "Epoch 40: LearningRateScheduler setting learning rate to 1.4831583923104999e-05.\n",
      "Epoch 40/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6175 - accuracy: 0.6580\n",
      "Epoch 40: val_loss did not improve from 0.63167\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.6177 - accuracy: 0.6575 - val_loss: 0.6317 - val_accuracy: 0.6918 - lr: 1.4832e-05\n",
      "\n",
      "Epoch 41: LearningRateScheduler setting learning rate to 1.2500000000000006e-05.\n",
      "Epoch 41/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6173 - accuracy: 0.6562\n",
      "Epoch 41: val_loss did not improve from 0.63167\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.6176 - accuracy: 0.6558 - val_loss: 0.6327 - val_accuracy: 0.6918 - lr: 1.2500e-05\n",
      "\n",
      "Epoch 42: LearningRateScheduler setting learning rate to 1.0305368692688174e-05.\n",
      "Epoch 42/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6170 - accuracy: 0.6597\n",
      "Epoch 42: val_loss did not improve from 0.63167\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.6170 - accuracy: 0.6592 - val_loss: 0.6324 - val_accuracy: 0.6918 - lr: 1.0305e-05\n",
      "\n",
      "Epoch 43: LearningRateScheduler setting learning rate to 8.271734841028553e-06.\n",
      "Epoch 43/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6170 - accuracy: 0.6562\n",
      "Epoch 43: val_loss improved from 0.63167 to 0.63126, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.6166 - accuracy: 0.6558 - val_loss: 0.6313 - val_accuracy: 0.6918 - lr: 8.2717e-06\n",
      "\n",
      "Epoch 44: LearningRateScheduler setting learning rate to 6.421379363065142e-06.\n",
      "Epoch 44/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6155 - accuracy: 0.6545\n",
      "Epoch 44: val_loss improved from 0.63126 to 0.63102, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.6163 - accuracy: 0.6541 - val_loss: 0.6310 - val_accuracy: 0.6918 - lr: 6.4214e-06\n",
      "\n",
      "Epoch 45: LearningRateScheduler setting learning rate to 4.7745751406263165e-06.\n",
      "Epoch 45/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6165 - accuracy: 0.6545\n",
      "Epoch 45: val_loss improved from 0.63102 to 0.63100, saving model to model.h5\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.6161 - accuracy: 0.6541 - val_loss: 0.6310 - val_accuracy: 0.6918 - lr: 4.7746e-06\n",
      "\n",
      "Epoch 46: LearningRateScheduler setting learning rate to 3.3493649053890326e-06.\n",
      "Epoch 46/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6185 - accuracy: 0.6528\n",
      "Epoch 46: val_loss improved from 0.63100 to 0.63058, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.6160 - accuracy: 0.6575 - val_loss: 0.6306 - val_accuracy: 0.6918 - lr: 3.3494e-06\n",
      "\n",
      "Epoch 47: LearningRateScheduler setting learning rate to 2.1613635589349756e-06.\n",
      "Epoch 47/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6157 - accuracy: 0.6545\n",
      "Epoch 47: val_loss did not improve from 0.63058\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.6158 - accuracy: 0.6558 - val_loss: 0.6307 - val_accuracy: 0.6918 - lr: 2.1614e-06\n",
      "\n",
      "Epoch 48: LearningRateScheduler setting learning rate to 1.2235870926211619e-06.\n",
      "Epoch 48/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6135 - accuracy: 0.6615\n",
      "Epoch 48: val_loss did not improve from 0.63058\n",
      "19/19 [==============================] - 1s 69ms/step - loss: 0.6158 - accuracy: 0.6558 - val_loss: 0.6306 - val_accuracy: 0.6918 - lr: 1.2236e-06\n",
      "\n",
      "Epoch 49: LearningRateScheduler setting learning rate to 5.463099816548579e-07.\n",
      "Epoch 49/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6135 - accuracy: 0.6597\n",
      "Epoch 49: val_loss did not improve from 0.63058\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.6157 - accuracy: 0.6558 - val_loss: 0.6306 - val_accuracy: 0.6918 - lr: 5.4631e-07\n",
      "\n",
      "Epoch 50: LearningRateScheduler setting learning rate to 1.3695261579316777e-07.\n",
      "Epoch 50/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6157 - accuracy: 0.6545\n",
      "Epoch 50: val_loss did not improve from 0.63058\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.6157 - accuracy: 0.6558 - val_loss: 0.6306 - val_accuracy: 0.6918 - lr: 1.3695e-07\n",
      "\n",
      "Epoch 51: LearningRateScheduler setting learning rate to 0.0.\n",
      "Epoch 51/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6172 - accuracy: 0.6528\n",
      "Epoch 51: val_loss did not improve from 0.63058\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.6157 - accuracy: 0.6558 - val_loss: 0.6306 - val_accuracy: 0.6918 - lr: 0.0000e+00\n",
      "\n",
      "Epoch 52: LearningRateScheduler setting learning rate to 1.3695261579316777e-07.\n",
      "Epoch 52/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6136 - accuracy: 0.6580\n",
      "Epoch 52: val_loss did not improve from 0.63058\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.6157 - accuracy: 0.6558 - val_loss: 0.6306 - val_accuracy: 0.6918 - lr: 1.3695e-07\n",
      "\n",
      "Epoch 53: LearningRateScheduler setting learning rate to 5.463099816548579e-07.\n",
      "Epoch 53/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6159 - accuracy: 0.6580\n",
      "Epoch 53: val_loss did not improve from 0.63058\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.6157 - accuracy: 0.6558 - val_loss: 0.6306 - val_accuracy: 0.6918 - lr: 5.4631e-07\n",
      "\n",
      "Epoch 54: LearningRateScheduler setting learning rate to 1.2235870926211619e-06.\n",
      "Epoch 54/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6158 - accuracy: 0.6562\n",
      "Epoch 54: val_loss did not improve from 0.63058\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.6157 - accuracy: 0.6558 - val_loss: 0.6306 - val_accuracy: 0.6918 - lr: 1.2236e-06\n",
      "\n",
      "Epoch 55: LearningRateScheduler setting learning rate to 2.161363558934973e-06.\n",
      "Epoch 55/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6161 - accuracy: 0.6562\n",
      "Epoch 55: val_loss did not improve from 0.63058\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.6157 - accuracy: 0.6558 - val_loss: 0.6306 - val_accuracy: 0.6918 - lr: 2.1614e-06\n",
      "\n",
      "Epoch 56: LearningRateScheduler setting learning rate to 3.349364905389035e-06.\n",
      "Epoch 56/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6143 - accuracy: 0.6580\n",
      "Epoch 56: val_loss improved from 0.63058 to 0.63048, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.6156 - accuracy: 0.6558 - val_loss: 0.6305 - val_accuracy: 0.6918 - lr: 3.3494e-06\n",
      "\n",
      "Epoch 57: LearningRateScheduler setting learning rate to 4.7745751406263114e-06.\n",
      "Epoch 57/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6151 - accuracy: 0.6562\n",
      "Epoch 57: val_loss improved from 0.63048 to 0.63027, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.6155 - accuracy: 0.6558 - val_loss: 0.6303 - val_accuracy: 0.6918 - lr: 4.7746e-06\n",
      "\n",
      "Epoch 58: LearningRateScheduler setting learning rate to 6.421379363065144e-06.\n",
      "Epoch 58/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6150 - accuracy: 0.6580\n",
      "Epoch 58: val_loss improved from 0.63027 to 0.62987, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.6154 - accuracy: 0.6592 - val_loss: 0.6299 - val_accuracy: 0.6918 - lr: 6.4214e-06\n",
      "\n",
      "Epoch 59: LearningRateScheduler setting learning rate to 8.27173484102854e-06.\n",
      "Epoch 59/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6154 - accuracy: 0.6580\n",
      "Epoch 59: val_loss improved from 0.62987 to 0.62980, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.6152 - accuracy: 0.6592 - val_loss: 0.6298 - val_accuracy: 0.6918 - lr: 8.2717e-06\n",
      "\n",
      "Epoch 60: LearningRateScheduler setting learning rate to 1.030536869268817e-05.\n",
      "Epoch 60/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6137 - accuracy: 0.6632\n",
      "Epoch 60: val_loss improved from 0.62980 to 0.62943, saving model to model.h5\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.6150 - accuracy: 0.6592 - val_loss: 0.6294 - val_accuracy: 0.6918 - lr: 1.0305e-05\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 5e-06.\n",
      "Epoch 1/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.7585 - accuracy: 0.5069\n",
      "Epoch 1: val_loss improved from inf to 0.80915, saving model to model.h5\n",
      "19/19 [==============================] - 4s 127ms/step - loss: 0.7561 - accuracy: 0.5103 - val_loss: 0.8092 - val_accuracy: 0.4384 - lr: 5.0000e-06\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 2/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.7462 - accuracy: 0.5069\n",
      "Epoch 2: val_loss improved from 0.80915 to 0.79229, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.7480 - accuracy: 0.5051 - val_loss: 0.7923 - val_accuracy: 0.4384 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 1.5000000000000002e-05.\n",
      "Epoch 3/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.7334 - accuracy: 0.5069\n",
      "Epoch 3: val_loss improved from 0.79229 to 0.76817, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.7348 - accuracy: 0.5034 - val_loss: 0.7682 - val_accuracy: 0.4384 - lr: 1.5000e-05\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 2e-05.\n",
      "Epoch 4/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.7210 - accuracy: 0.4948\n",
      "Epoch 4: val_loss improved from 0.76817 to 0.74470, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.7204 - accuracy: 0.4966 - val_loss: 0.7447 - val_accuracy: 0.4315 - lr: 2.0000e-05\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 2.5e-05.\n",
      "Epoch 5/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.7078 - accuracy: 0.4913\n",
      "Epoch 5: val_loss improved from 0.74470 to 0.72906, saving model to model.h5\n",
      "19/19 [==============================] - 2s 96ms/step - loss: 0.7095 - accuracy: 0.4914 - val_loss: 0.7291 - val_accuracy: 0.4932 - lr: 2.5000e-05\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 3.0000000000000004e-05.\n",
      "Epoch 6/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.7004 - accuracy: 0.5156\n",
      "Epoch 6: val_loss improved from 0.72906 to 0.71470, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.7014 - accuracy: 0.5120 - val_loss: 0.7147 - val_accuracy: 0.4932 - lr: 3.0000e-05\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 3.5e-05.\n",
      "Epoch 7/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6930 - accuracy: 0.5503\n",
      "Epoch 7: val_loss improved from 0.71470 to 0.70357, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.6940 - accuracy: 0.5514 - val_loss: 0.7036 - val_accuracy: 0.5137 - lr: 3.5000e-05\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 4e-05.\n",
      "Epoch 8/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6874 - accuracy: 0.5625\n",
      "Epoch 8: val_loss improved from 0.70357 to 0.69414, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.6877 - accuracy: 0.5634 - val_loss: 0.6941 - val_accuracy: 0.5548 - lr: 4.0000e-05\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 4.5e-05.\n",
      "Epoch 9/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6822 - accuracy: 0.5747\n",
      "Epoch 9: val_loss improved from 0.69414 to 0.68876, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.6812 - accuracy: 0.5788 - val_loss: 0.6888 - val_accuracy: 0.5822 - lr: 4.5000e-05\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 5e-05.\n",
      "Epoch 10/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6742 - accuracy: 0.5938\n",
      "Epoch 10: val_loss improved from 0.68876 to 0.68100, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.6739 - accuracy: 0.5959 - val_loss: 0.6810 - val_accuracy: 0.5959 - lr: 5.0000e-05\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 5.5e-05.\n",
      "Epoch 11/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6671 - accuracy: 0.6128\n",
      "Epoch 11: val_loss improved from 0.68100 to 0.67577, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.6672 - accuracy: 0.6130 - val_loss: 0.6758 - val_accuracy: 0.5822 - lr: 5.5000e-05\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 6.000000000000001e-05.\n",
      "Epoch 12/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6612 - accuracy: 0.6302\n",
      "Epoch 12: val_loss improved from 0.67577 to 0.66844, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.6609 - accuracy: 0.6301 - val_loss: 0.6684 - val_accuracy: 0.5822 - lr: 6.0000e-05\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 6.500000000000001e-05.\n",
      "Epoch 13/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6545 - accuracy: 0.6302\n",
      "Epoch 13: val_loss improved from 0.66844 to 0.65871, saving model to model.h5\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.6543 - accuracy: 0.6301 - val_loss: 0.6587 - val_accuracy: 0.6164 - lr: 6.5000e-05\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 7e-05.\n",
      "Epoch 14/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6477 - accuracy: 0.6319\n",
      "Epoch 14: val_loss improved from 0.65871 to 0.65422, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.6472 - accuracy: 0.6336 - val_loss: 0.6542 - val_accuracy: 0.6438 - lr: 7.0000e-05\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 7.500000000000001e-05.\n",
      "Epoch 15/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6423 - accuracy: 0.6337\n",
      "Epoch 15: val_loss improved from 0.65422 to 0.65049, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.6416 - accuracy: 0.6353 - val_loss: 0.6505 - val_accuracy: 0.6644 - lr: 7.5000e-05\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 8e-05.\n",
      "Epoch 16/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6373 - accuracy: 0.6406\n",
      "Epoch 16: val_loss improved from 0.65049 to 0.64220, saving model to model.h5\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.6362 - accuracy: 0.6438 - val_loss: 0.6422 - val_accuracy: 0.6575 - lr: 8.0000e-05\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 8.5e-05.\n",
      "Epoch 17/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6297 - accuracy: 0.6528\n",
      "Epoch 17: val_loss improved from 0.64220 to 0.63786, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.6307 - accuracy: 0.6473 - val_loss: 0.6379 - val_accuracy: 0.6507 - lr: 8.5000e-05\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 9e-05.\n",
      "Epoch 18/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6272 - accuracy: 0.6580\n",
      "Epoch 18: val_loss improved from 0.63786 to 0.62972, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.6258 - accuracy: 0.6627 - val_loss: 0.6297 - val_accuracy: 0.6644 - lr: 9.0000e-05\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 9.5e-05.\n",
      "Epoch 19/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6229 - accuracy: 0.6597\n",
      "Epoch 19: val_loss improved from 0.62972 to 0.62595, saving model to model.h5\n",
      "19/19 [==============================] - 2s 95ms/step - loss: 0.6217 - accuracy: 0.6610 - val_loss: 0.6259 - val_accuracy: 0.6712 - lr: 9.5000e-05\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 20/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6188 - accuracy: 0.6701\n",
      "Epoch 20: val_loss did not improve from 0.62595\n",
      "19/19 [==============================] - 1s 66ms/step - loss: 0.6190 - accuracy: 0.6695 - val_loss: 0.6309 - val_accuracy: 0.6712 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 21: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 21/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6158 - accuracy: 0.6649\n",
      "Epoch 21: val_loss improved from 0.62595 to 0.61681, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.6156 - accuracy: 0.6644 - val_loss: 0.6168 - val_accuracy: 0.6644 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 22: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 22/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6084 - accuracy: 0.6753\n",
      "Epoch 22: val_loss did not improve from 0.61681\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.6094 - accuracy: 0.6712 - val_loss: 0.6168 - val_accuracy: 0.6849 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 23: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 23/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6044 - accuracy: 0.6753\n",
      "Epoch 23: val_loss improved from 0.61681 to 0.61279, saving model to model.h5\n",
      "19/19 [==============================] - 2s 96ms/step - loss: 0.6065 - accuracy: 0.6712 - val_loss: 0.6128 - val_accuracy: 0.6849 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 24: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 24/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6034 - accuracy: 0.6823\n",
      "Epoch 24: val_loss did not improve from 0.61279\n",
      "19/19 [==============================] - 1s 66ms/step - loss: 0.6023 - accuracy: 0.6849 - val_loss: 0.6169 - val_accuracy: 0.6986 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 25: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 25/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6011 - accuracy: 0.6892\n",
      "Epoch 25: val_loss improved from 0.61279 to 0.61265, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.6019 - accuracy: 0.6901 - val_loss: 0.6126 - val_accuracy: 0.6918 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 26: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 26/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5985 - accuracy: 0.6823\n",
      "Epoch 26: val_loss improved from 0.61265 to 0.60676, saving model to model.h5\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.5982 - accuracy: 0.6815 - val_loss: 0.6068 - val_accuracy: 0.6918 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 27: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 27/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5937 - accuracy: 0.6858\n",
      "Epoch 27: val_loss did not improve from 0.60676\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5942 - accuracy: 0.6832 - val_loss: 0.6086 - val_accuracy: 0.6918 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 28: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 28/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5939 - accuracy: 0.6927\n",
      "Epoch 28: val_loss did not improve from 0.60676\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5935 - accuracy: 0.6935 - val_loss: 0.6069 - val_accuracy: 0.6918 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 29: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 29/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5918 - accuracy: 0.6823\n",
      "Epoch 29: val_loss improved from 0.60676 to 0.60637, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.5915 - accuracy: 0.6849 - val_loss: 0.6064 - val_accuracy: 0.6986 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 30: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 30/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5863 - accuracy: 0.7031\n",
      "Epoch 30: val_loss improved from 0.60637 to 0.60240, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.5870 - accuracy: 0.7021 - val_loss: 0.6024 - val_accuracy: 0.6918 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 31: LearningRateScheduler setting learning rate to 7.500000000000001e-05.\n",
      "Epoch 31/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5828 - accuracy: 0.6910\n",
      "Epoch 31: val_loss improved from 0.60240 to 0.59766, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.5853 - accuracy: 0.6901 - val_loss: 0.5977 - val_accuracy: 0.6918 - lr: 7.5000e-05\n",
      "\n",
      "Epoch 32: LearningRateScheduler setting learning rate to 7.033683215379002e-05.\n",
      "Epoch 32/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5832 - accuracy: 0.7031\n",
      "Epoch 32: val_loss did not improve from 0.59766\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5822 - accuracy: 0.7055 - val_loss: 0.6023 - val_accuracy: 0.7055 - lr: 7.0337e-05\n",
      "\n",
      "Epoch 33: LearningRateScheduler setting learning rate to 6.545084971874738e-05.\n",
      "Epoch 33/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5787 - accuracy: 0.7101\n",
      "Epoch 33: val_loss did not improve from 0.59766\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5815 - accuracy: 0.7072 - val_loss: 0.6027 - val_accuracy: 0.7055 - lr: 6.5451e-05\n",
      "\n",
      "Epoch 34: LearningRateScheduler setting learning rate to 6.0395584540887963e-05.\n",
      "Epoch 34/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5804 - accuracy: 0.7083\n",
      "Epoch 34: val_loss did not improve from 0.59766\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5796 - accuracy: 0.7089 - val_loss: 0.6002 - val_accuracy: 0.7055 - lr: 6.0396e-05\n",
      "\n",
      "Epoch 35: LearningRateScheduler setting learning rate to 5.522642316338268e-05.\n",
      "Epoch 35/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5785 - accuracy: 0.7066\n",
      "Epoch 35: val_loss improved from 0.59766 to 0.59731, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.5786 - accuracy: 0.7055 - val_loss: 0.5973 - val_accuracy: 0.6986 - lr: 5.5226e-05\n",
      "\n",
      "Epoch 36: LearningRateScheduler setting learning rate to 5e-05.\n",
      "Epoch 36/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5772 - accuracy: 0.7031\n",
      "Epoch 36: val_loss did not improve from 0.59731\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5769 - accuracy: 0.7003 - val_loss: 0.5975 - val_accuracy: 0.7055 - lr: 5.0000e-05\n",
      "\n",
      "Epoch 37: LearningRateScheduler setting learning rate to 4.477357683661734e-05.\n",
      "Epoch 37/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5752 - accuracy: 0.7066\n",
      "Epoch 37: val_loss did not improve from 0.59731\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5760 - accuracy: 0.7055 - val_loss: 0.5988 - val_accuracy: 0.7055 - lr: 4.4774e-05\n",
      "\n",
      "Epoch 38: LearningRateScheduler setting learning rate to 3.960441545911204e-05.\n",
      "Epoch 38/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5764 - accuracy: 0.7049\n",
      "Epoch 38: val_loss did not improve from 0.59731\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5749 - accuracy: 0.7072 - val_loss: 0.5993 - val_accuracy: 0.7055 - lr: 3.9604e-05\n",
      "\n",
      "Epoch 39: LearningRateScheduler setting learning rate to 3.4549150281252636e-05.\n",
      "Epoch 39/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5765 - accuracy: 0.7066\n",
      "Epoch 39: val_loss improved from 0.59731 to 0.59723, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.5741 - accuracy: 0.7089 - val_loss: 0.5972 - val_accuracy: 0.7123 - lr: 3.4549e-05\n",
      "\n",
      "Epoch 40: LearningRateScheduler setting learning rate to 2.9663167846209998e-05.\n",
      "Epoch 40/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5752 - accuracy: 0.7031\n",
      "Epoch 40: val_loss improved from 0.59723 to 0.59580, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.5735 - accuracy: 0.7072 - val_loss: 0.5958 - val_accuracy: 0.7123 - lr: 2.9663e-05\n",
      "\n",
      "Epoch 41: LearningRateScheduler setting learning rate to 2.500000000000001e-05.\n",
      "Epoch 41/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5733 - accuracy: 0.6997\n",
      "Epoch 41: val_loss did not improve from 0.59580\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5725 - accuracy: 0.7038 - val_loss: 0.5968 - val_accuracy: 0.7123 - lr: 2.5000e-05\n",
      "\n",
      "Epoch 42: LearningRateScheduler setting learning rate to 2.061073738537635e-05.\n",
      "Epoch 42/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5731 - accuracy: 0.7083\n",
      "Epoch 42: val_loss did not improve from 0.59580\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5724 - accuracy: 0.7089 - val_loss: 0.5980 - val_accuracy: 0.7055 - lr: 2.0611e-05\n",
      "\n",
      "Epoch 43: LearningRateScheduler setting learning rate to 1.6543469682057106e-05.\n",
      "Epoch 43/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5706 - accuracy: 0.7101\n",
      "Epoch 43: val_loss did not improve from 0.59580\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5717 - accuracy: 0.7072 - val_loss: 0.5978 - val_accuracy: 0.7055 - lr: 1.6543e-05\n",
      "\n",
      "Epoch 44: LearningRateScheduler setting learning rate to 1.2842758726130283e-05.\n",
      "Epoch 44/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5733 - accuracy: 0.7066\n",
      "Epoch 44: val_loss did not improve from 0.59580\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5716 - accuracy: 0.7072 - val_loss: 0.5967 - val_accuracy: 0.7055 - lr: 1.2843e-05\n",
      "\n",
      "Epoch 45: LearningRateScheduler setting learning rate to 9.549150281252633e-06.\n",
      "Epoch 45/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5692 - accuracy: 0.7135\n",
      "Epoch 45: val_loss did not improve from 0.59580\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5710 - accuracy: 0.7123 - val_loss: 0.5962 - val_accuracy: 0.7123 - lr: 9.5492e-06\n",
      "\n",
      "Epoch 46: LearningRateScheduler setting learning rate to 6.698729810778065e-06.\n",
      "Epoch 46/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5699 - accuracy: 0.7118\n",
      "Epoch 46: val_loss did not improve from 0.59580\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5707 - accuracy: 0.7106 - val_loss: 0.5965 - val_accuracy: 0.7055 - lr: 6.6987e-06\n",
      "\n",
      "Epoch 47: LearningRateScheduler setting learning rate to 4.322727117869951e-06.\n",
      "Epoch 47/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5697 - accuracy: 0.7118\n",
      "Epoch 47: val_loss did not improve from 0.59580\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5706 - accuracy: 0.7123 - val_loss: 0.5961 - val_accuracy: 0.7123 - lr: 4.3227e-06\n",
      "\n",
      "Epoch 48: LearningRateScheduler setting learning rate to 2.4471741852423237e-06.\n",
      "Epoch 48/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5701 - accuracy: 0.7135\n",
      "Epoch 48: val_loss did not improve from 0.59580\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5704 - accuracy: 0.7123 - val_loss: 0.5960 - val_accuracy: 0.7123 - lr: 2.4472e-06\n",
      "\n",
      "Epoch 49: LearningRateScheduler setting learning rate to 1.0926199633097157e-06.\n",
      "Epoch 49/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5692 - accuracy: 0.7153\n",
      "Epoch 49: val_loss did not improve from 0.59580\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5703 - accuracy: 0.7123 - val_loss: 0.5961 - val_accuracy: 0.7123 - lr: 1.0926e-06\n",
      "\n",
      "Epoch 50: LearningRateScheduler setting learning rate to 2.7390523158633554e-07.\n",
      "Epoch 50/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5703 - accuracy: 0.7101\n",
      "Epoch 50: val_loss did not improve from 0.59580\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5703 - accuracy: 0.7123 - val_loss: 0.5961 - val_accuracy: 0.7123 - lr: 2.7391e-07\n",
      "Epoch 50: early stopping\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 7.500000000000001e-06.\n",
      "Epoch 1/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.7370 - accuracy: 0.4601\n",
      "Epoch 1: val_loss improved from inf to 0.71786, saving model to model.h5\n",
      "19/19 [==============================] - 4s 129ms/step - loss: 0.7373 - accuracy: 0.4606 - val_loss: 0.7179 - val_accuracy: 0.4863 - lr: 7.5000e-06\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 1.5000000000000002e-05.\n",
      "Epoch 2/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.7329 - accuracy: 0.4531\n",
      "Epoch 2: val_loss improved from 0.71786 to 0.71544, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.7316 - accuracy: 0.4555 - val_loss: 0.7154 - val_accuracy: 0.5205 - lr: 1.5000e-05\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 2.25e-05.\n",
      "Epoch 3/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.7231 - accuracy: 0.4375\n",
      "Epoch 3: val_loss improved from 0.71544 to 0.71376, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.7236 - accuracy: 0.4384 - val_loss: 0.7138 - val_accuracy: 0.4932 - lr: 2.2500e-05\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 3.0000000000000004e-05.\n",
      "Epoch 4/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.7148 - accuracy: 0.4497\n",
      "Epoch 4: val_loss improved from 0.71376 to 0.71213, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.7146 - accuracy: 0.4486 - val_loss: 0.7121 - val_accuracy: 0.4726 - lr: 3.0000e-05\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 3.7500000000000003e-05.\n",
      "Epoch 5/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.7075 - accuracy: 0.4792\n",
      "Epoch 5: val_loss improved from 0.71213 to 0.70886, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.7079 - accuracy: 0.4777 - val_loss: 0.7089 - val_accuracy: 0.4726 - lr: 3.7500e-05\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 4.5e-05.\n",
      "Epoch 6/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.7022 - accuracy: 0.5017\n",
      "Epoch 6: val_loss improved from 0.70886 to 0.70383, saving model to model.h5\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.7014 - accuracy: 0.5068 - val_loss: 0.7038 - val_accuracy: 0.5000 - lr: 4.5000e-05\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 5.250000000000001e-05.\n",
      "Epoch 7/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6950 - accuracy: 0.5226\n",
      "Epoch 7: val_loss improved from 0.70383 to 0.69933, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.6944 - accuracy: 0.5240 - val_loss: 0.6993 - val_accuracy: 0.5068 - lr: 5.2500e-05\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 6.000000000000001e-05.\n",
      "Epoch 8/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6859 - accuracy: 0.5486\n",
      "Epoch 8: val_loss improved from 0.69933 to 0.68943, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.6870 - accuracy: 0.5462 - val_loss: 0.6894 - val_accuracy: 0.5548 - lr: 6.0000e-05\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 6.75e-05.\n",
      "Epoch 9/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6781 - accuracy: 0.5642\n",
      "Epoch 9: val_loss improved from 0.68943 to 0.68197, saving model to model.h5\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.6797 - accuracy: 0.5616 - val_loss: 0.6820 - val_accuracy: 0.5959 - lr: 6.7500e-05\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 7.500000000000001e-05.\n",
      "Epoch 10/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6712 - accuracy: 0.5920\n",
      "Epoch 10: val_loss improved from 0.68197 to 0.67540, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.6726 - accuracy: 0.5890 - val_loss: 0.6754 - val_accuracy: 0.6301 - lr: 7.5000e-05\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 8.250000000000001e-05.\n",
      "Epoch 11/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6655 - accuracy: 0.6267\n",
      "Epoch 11: val_loss improved from 0.67540 to 0.66389, saving model to model.h5\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.6658 - accuracy: 0.6250 - val_loss: 0.6639 - val_accuracy: 0.6575 - lr: 8.2500e-05\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 9e-05.\n",
      "Epoch 12/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6591 - accuracy: 0.6233\n",
      "Epoch 12: val_loss did not improve from 0.66389\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.6582 - accuracy: 0.6267 - val_loss: 0.6645 - val_accuracy: 0.6849 - lr: 9.0000e-05\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 9.750000000000001e-05.\n",
      "Epoch 13/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6519 - accuracy: 0.6389\n",
      "Epoch 13: val_loss improved from 0.66389 to 0.65207, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.6519 - accuracy: 0.6370 - val_loss: 0.6521 - val_accuracy: 0.6918 - lr: 9.7500e-05\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 0.00010500000000000002.\n",
      "Epoch 14/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6439 - accuracy: 0.6510\n",
      "Epoch 14: val_loss improved from 0.65207 to 0.64678, saving model to model.h5\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.6443 - accuracy: 0.6507 - val_loss: 0.6468 - val_accuracy: 0.7192 - lr: 1.0500e-04\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 0.00011250000000000001.\n",
      "Epoch 15/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6397 - accuracy: 0.6580\n",
      "Epoch 15: val_loss improved from 0.64678 to 0.63849, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.6389 - accuracy: 0.6610 - val_loss: 0.6385 - val_accuracy: 0.7123 - lr: 1.1250e-04\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 0.00012000000000000002.\n",
      "Epoch 16/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6324 - accuracy: 0.6684\n",
      "Epoch 16: val_loss improved from 0.63849 to 0.63559, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.6321 - accuracy: 0.6695 - val_loss: 0.6356 - val_accuracy: 0.7123 - lr: 1.2000e-04\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 0.0001275.\n",
      "Epoch 17/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6261 - accuracy: 0.6719\n",
      "Epoch 17: val_loss improved from 0.63559 to 0.63222, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.6269 - accuracy: 0.6695 - val_loss: 0.6322 - val_accuracy: 0.6986 - lr: 1.2750e-04\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 0.000135.\n",
      "Epoch 18/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6234 - accuracy: 0.6788\n",
      "Epoch 18: val_loss improved from 0.63222 to 0.62588, saving model to model.h5\n",
      "19/19 [==============================] - 2s 96ms/step - loss: 0.6229 - accuracy: 0.6798 - val_loss: 0.6259 - val_accuracy: 0.7123 - lr: 1.3500e-04\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 0.0001425.\n",
      "Epoch 19/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6184 - accuracy: 0.6806\n",
      "Epoch 19: val_loss improved from 0.62588 to 0.62240, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.6162 - accuracy: 0.6832 - val_loss: 0.6224 - val_accuracy: 0.7123 - lr: 1.4250e-04\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 0.00015000000000000001.\n",
      "Epoch 20/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6125 - accuracy: 0.6823\n",
      "Epoch 20: val_loss improved from 0.62240 to 0.61606, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.6119 - accuracy: 0.6849 - val_loss: 0.6161 - val_accuracy: 0.6986 - lr: 1.5000e-04\n",
      "\n",
      "Epoch 21: LearningRateScheduler setting learning rate to 0.00015000000000000001.\n",
      "Epoch 21/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6066 - accuracy: 0.6840\n",
      "Epoch 21: val_loss improved from 0.61606 to 0.61482, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.6069 - accuracy: 0.6815 - val_loss: 0.6148 - val_accuracy: 0.6986 - lr: 1.5000e-04\n",
      "\n",
      "Epoch 22: LearningRateScheduler setting learning rate to 0.00015000000000000001.\n",
      "Epoch 22/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6029 - accuracy: 0.6840\n",
      "Epoch 22: val_loss improved from 0.61482 to 0.61058, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.6022 - accuracy: 0.6866 - val_loss: 0.6106 - val_accuracy: 0.7055 - lr: 1.5000e-04\n",
      "\n",
      "Epoch 23: LearningRateScheduler setting learning rate to 0.00015000000000000001.\n",
      "Epoch 23/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5982 - accuracy: 0.6788\n",
      "Epoch 23: val_loss did not improve from 0.61058\n",
      "19/19 [==============================] - 1s 66ms/step - loss: 0.5971 - accuracy: 0.6815 - val_loss: 0.6118 - val_accuracy: 0.7466 - lr: 1.5000e-04\n",
      "\n",
      "Epoch 24: LearningRateScheduler setting learning rate to 0.00015000000000000001.\n",
      "Epoch 24/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5930 - accuracy: 0.6840\n",
      "Epoch 24: val_loss improved from 0.61058 to 0.60438, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.5929 - accuracy: 0.6832 - val_loss: 0.6044 - val_accuracy: 0.6986 - lr: 1.5000e-04\n",
      "\n",
      "Epoch 25: LearningRateScheduler setting learning rate to 0.00015000000000000001.\n",
      "Epoch 25/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5901 - accuracy: 0.6875\n",
      "Epoch 25: val_loss did not improve from 0.60438\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5891 - accuracy: 0.6901 - val_loss: 0.6062 - val_accuracy: 0.7260 - lr: 1.5000e-04\n",
      "\n",
      "Epoch 26: LearningRateScheduler setting learning rate to 0.00015000000000000001.\n",
      "Epoch 26/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5852 - accuracy: 0.7031\n",
      "Epoch 26: val_loss improved from 0.60438 to 0.59880, saving model to model.h5\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.5852 - accuracy: 0.7003 - val_loss: 0.5988 - val_accuracy: 0.6918 - lr: 1.5000e-04\n",
      "\n",
      "Epoch 27: LearningRateScheduler setting learning rate to 0.00015000000000000001.\n",
      "Epoch 27/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5860 - accuracy: 0.6875\n",
      "Epoch 27: val_loss did not improve from 0.59880\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5850 - accuracy: 0.6901 - val_loss: 0.6048 - val_accuracy: 0.7329 - lr: 1.5000e-04\n",
      "\n",
      "Epoch 28: LearningRateScheduler setting learning rate to 0.00015000000000000001.\n",
      "Epoch 28/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5791 - accuracy: 0.7014\n",
      "Epoch 28: val_loss improved from 0.59880 to 0.59456, saving model to model.h5\n",
      "19/19 [==============================] - 2s 95ms/step - loss: 0.5772 - accuracy: 0.7038 - val_loss: 0.5946 - val_accuracy: 0.6918 - lr: 1.5000e-04\n",
      "\n",
      "Epoch 29: LearningRateScheduler setting learning rate to 0.00015000000000000001.\n",
      "Epoch 29/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5777 - accuracy: 0.6962\n",
      "Epoch 29: val_loss did not improve from 0.59456\n",
      "19/19 [==============================] - 1s 66ms/step - loss: 0.5764 - accuracy: 0.6969 - val_loss: 0.5974 - val_accuracy: 0.7260 - lr: 1.5000e-04\n",
      "\n",
      "Epoch 30: LearningRateScheduler setting learning rate to 0.00015000000000000001.\n",
      "Epoch 30/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5740 - accuracy: 0.7101\n",
      "Epoch 30: val_loss improved from 0.59456 to 0.59082, saving model to model.h5\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.5724 - accuracy: 0.7123 - val_loss: 0.5908 - val_accuracy: 0.6986 - lr: 1.5000e-04\n",
      "\n",
      "Epoch 31: LearningRateScheduler setting learning rate to 0.00011250000000000001.\n",
      "Epoch 31/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5720 - accuracy: 0.7049\n",
      "Epoch 31: val_loss did not improve from 0.59082\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5728 - accuracy: 0.7038 - val_loss: 0.6006 - val_accuracy: 0.7329 - lr: 1.1250e-04\n",
      "\n",
      "Epoch 32: LearningRateScheduler setting learning rate to 0.00010550524823068504.\n",
      "Epoch 32/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5644 - accuracy: 0.7135\n",
      "Epoch 32: val_loss improved from 0.59082 to 0.59076, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.5641 - accuracy: 0.7123 - val_loss: 0.5908 - val_accuracy: 0.6986 - lr: 1.0551e-04\n",
      "\n",
      "Epoch 33: LearningRateScheduler setting learning rate to 9.817627457812107e-05.\n",
      "Epoch 33/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5620 - accuracy: 0.7222\n",
      "Epoch 33: val_loss improved from 0.59076 to 0.58945, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.5628 - accuracy: 0.7192 - val_loss: 0.5894 - val_accuracy: 0.6986 - lr: 9.8176e-05\n",
      "\n",
      "Epoch 34: LearningRateScheduler setting learning rate to 9.059337681133195e-05.\n",
      "Epoch 34/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5623 - accuracy: 0.7240\n",
      "Epoch 34: val_loss improved from 0.58945 to 0.58693, saving model to model.h5\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.5617 - accuracy: 0.7243 - val_loss: 0.5869 - val_accuracy: 0.7055 - lr: 9.0593e-05\n",
      "\n",
      "Epoch 35: LearningRateScheduler setting learning rate to 8.283963474507402e-05.\n",
      "Epoch 35/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5591 - accuracy: 0.7222\n",
      "Epoch 35: val_loss did not improve from 0.58693\n",
      "19/19 [==============================] - 1s 69ms/step - loss: 0.5579 - accuracy: 0.7243 - val_loss: 0.5923 - val_accuracy: 0.7329 - lr: 8.2840e-05\n",
      "\n",
      "Epoch 36: LearningRateScheduler setting learning rate to 7.500000000000001e-05.\n",
      "Epoch 36/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5571 - accuracy: 0.7135\n",
      "Epoch 36: val_loss did not improve from 0.58693\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5574 - accuracy: 0.7123 - val_loss: 0.5918 - val_accuracy: 0.7329 - lr: 7.5000e-05\n",
      "\n",
      "Epoch 37: LearningRateScheduler setting learning rate to 6.716036525492601e-05.\n",
      "Epoch 37/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5561 - accuracy: 0.7205\n",
      "Epoch 37: val_loss did not improve from 0.58693\n",
      "19/19 [==============================] - 1s 66ms/step - loss: 0.5554 - accuracy: 0.7226 - val_loss: 0.5876 - val_accuracy: 0.7123 - lr: 6.7160e-05\n",
      "\n",
      "Epoch 38: LearningRateScheduler setting learning rate to 5.940662318866807e-05.\n",
      "Epoch 38/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5559 - accuracy: 0.7257\n",
      "Epoch 38: val_loss improved from 0.58693 to 0.58608, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.5539 - accuracy: 0.7277 - val_loss: 0.5861 - val_accuracy: 0.7123 - lr: 5.9407e-05\n",
      "\n",
      "Epoch 39: LearningRateScheduler setting learning rate to 5.1823725421878954e-05.\n",
      "Epoch 39/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5530 - accuracy: 0.7274\n",
      "Epoch 39: val_loss did not improve from 0.58608\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5532 - accuracy: 0.7277 - val_loss: 0.5864 - val_accuracy: 0.7192 - lr: 5.1824e-05\n",
      "\n",
      "Epoch 40: LearningRateScheduler setting learning rate to 4.4494751769315e-05.\n",
      "Epoch 40/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5479 - accuracy: 0.7344\n",
      "Epoch 40: val_loss improved from 0.58608 to 0.58552, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.5517 - accuracy: 0.7295 - val_loss: 0.5855 - val_accuracy: 0.7123 - lr: 4.4495e-05\n",
      "\n",
      "Epoch 41: LearningRateScheduler setting learning rate to 3.750000000000002e-05.\n",
      "Epoch 41/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5510 - accuracy: 0.7257\n",
      "Epoch 41: val_loss did not improve from 0.58552\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5508 - accuracy: 0.7277 - val_loss: 0.5880 - val_accuracy: 0.7260 - lr: 3.7500e-05\n",
      "\n",
      "Epoch 42: LearningRateScheduler setting learning rate to 3.0916106078064525e-05.\n",
      "Epoch 42/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5490 - accuracy: 0.7326\n",
      "Epoch 42: val_loss did not improve from 0.58552\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5500 - accuracy: 0.7312 - val_loss: 0.5862 - val_accuracy: 0.7123 - lr: 3.0916e-05\n",
      "\n",
      "Epoch 43: LearningRateScheduler setting learning rate to 2.481520452308566e-05.\n",
      "Epoch 43/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5498 - accuracy: 0.7292\n",
      "Epoch 43: val_loss did not improve from 0.58552\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5493 - accuracy: 0.7312 - val_loss: 0.5864 - val_accuracy: 0.7192 - lr: 2.4815e-05\n",
      "\n",
      "Epoch 44: LearningRateScheduler setting learning rate to 1.9264138089195424e-05.\n",
      "Epoch 44/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5492 - accuracy: 0.7292\n",
      "Epoch 44: val_loss did not improve from 0.58552\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5487 - accuracy: 0.7295 - val_loss: 0.5867 - val_accuracy: 0.7192 - lr: 1.9264e-05\n",
      "\n",
      "Epoch 45: LearningRateScheduler setting learning rate to 1.432372542187895e-05.\n",
      "Epoch 45/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5476 - accuracy: 0.7292\n",
      "Epoch 45: val_loss did not improve from 0.58552\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5483 - accuracy: 0.7295 - val_loss: 0.5859 - val_accuracy: 0.7123 - lr: 1.4324e-05\n",
      "\n",
      "Epoch 46: LearningRateScheduler setting learning rate to 1.0048094716167097e-05.\n",
      "Epoch 46/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5488 - accuracy: 0.7292\n",
      "Epoch 46: val_loss did not improve from 0.58552\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5482 - accuracy: 0.7295 - val_loss: 0.5865 - val_accuracy: 0.7260 - lr: 1.0048e-05\n",
      "\n",
      "Epoch 47: LearningRateScheduler setting learning rate to 6.484090676804927e-06.\n",
      "Epoch 47/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5482 - accuracy: 0.7292\n",
      "Epoch 47: val_loss did not improve from 0.58552\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5478 - accuracy: 0.7277 - val_loss: 0.5864 - val_accuracy: 0.7260 - lr: 6.4841e-06\n",
      "\n",
      "Epoch 48: LearningRateScheduler setting learning rate to 3.6707612778634854e-06.\n",
      "Epoch 48/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5492 - accuracy: 0.7257\n",
      "Epoch 48: val_loss did not improve from 0.58552\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5476 - accuracy: 0.7277 - val_loss: 0.5862 - val_accuracy: 0.7260 - lr: 3.6708e-06\n",
      "\n",
      "Epoch 49: LearningRateScheduler setting learning rate to 1.6389299449645735e-06.\n",
      "Epoch 49/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5496 - accuracy: 0.7257\n",
      "Epoch 49: val_loss did not improve from 0.58552\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5475 - accuracy: 0.7277 - val_loss: 0.5862 - val_accuracy: 0.7192 - lr: 1.6389e-06\n",
      "\n",
      "Epoch 50: LearningRateScheduler setting learning rate to 4.108578473795033e-07.\n",
      "Epoch 50/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5464 - accuracy: 0.7274\n",
      "Epoch 50: val_loss did not improve from 0.58552\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5475 - accuracy: 0.7277 - val_loss: 0.5861 - val_accuracy: 0.7192 - lr: 4.1086e-07\n",
      "Epoch 50: early stopping\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 1/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6994 - accuracy: 0.4826\n",
      "Epoch 1: val_loss improved from inf to 0.71334, saving model to model.h5\n",
      "19/19 [==============================] - 5s 143ms/step - loss: 0.7014 - accuracy: 0.4777 - val_loss: 0.7133 - val_accuracy: 0.4315 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 2e-05.\n",
      "Epoch 2/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.7006 - accuracy: 0.4896\n",
      "Epoch 2: val_loss improved from 0.71334 to 0.71181, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.6995 - accuracy: 0.4949 - val_loss: 0.7118 - val_accuracy: 0.4589 - lr: 2.0000e-05\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 3.0000000000000004e-05.\n",
      "Epoch 3/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6963 - accuracy: 0.5122\n",
      "Epoch 3: val_loss improved from 0.71181 to 0.70888, saving model to model.h5\n",
      "19/19 [==============================] - 2s 96ms/step - loss: 0.6963 - accuracy: 0.5120 - val_loss: 0.7089 - val_accuracy: 0.4795 - lr: 3.0000e-05\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 4e-05.\n",
      "Epoch 4/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6915 - accuracy: 0.5486\n",
      "Epoch 4: val_loss improved from 0.70888 to 0.70420, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.6917 - accuracy: 0.5462 - val_loss: 0.7042 - val_accuracy: 0.4658 - lr: 4.0000e-05\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 5e-05.\n",
      "Epoch 5/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6860 - accuracy: 0.5590\n",
      "Epoch 5: val_loss improved from 0.70420 to 0.69874, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.6860 - accuracy: 0.5599 - val_loss: 0.6987 - val_accuracy: 0.5137 - lr: 5.0000e-05\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 6.000000000000001e-05.\n",
      "Epoch 6/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6802 - accuracy: 0.5660\n",
      "Epoch 6: val_loss improved from 0.69874 to 0.68891, saving model to model.h5\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.6801 - accuracy: 0.5634 - val_loss: 0.6889 - val_accuracy: 0.5068 - lr: 6.0000e-05\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 7e-05.\n",
      "Epoch 7/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6726 - accuracy: 0.5868\n",
      "Epoch 7: val_loss improved from 0.68891 to 0.68489, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.6735 - accuracy: 0.5822 - val_loss: 0.6849 - val_accuracy: 0.5342 - lr: 7.0000e-05\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 8e-05.\n",
      "Epoch 8/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6672 - accuracy: 0.5920\n",
      "Epoch 8: val_loss improved from 0.68489 to 0.67657, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.6664 - accuracy: 0.5942 - val_loss: 0.6766 - val_accuracy: 0.5548 - lr: 8.0000e-05\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 9e-05.\n",
      "Epoch 9/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6584 - accuracy: 0.6076\n",
      "Epoch 9: val_loss improved from 0.67657 to 0.66642, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.6585 - accuracy: 0.6062 - val_loss: 0.6664 - val_accuracy: 0.6027 - lr: 9.0000e-05\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 10/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6529 - accuracy: 0.6250\n",
      "Epoch 10: val_loss improved from 0.66642 to 0.66034, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.6522 - accuracy: 0.6284 - val_loss: 0.6603 - val_accuracy: 0.6027 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 0.00011.\n",
      "Epoch 11/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6464 - accuracy: 0.6267\n",
      "Epoch 11: val_loss improved from 0.66034 to 0.65544, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.6450 - accuracy: 0.6301 - val_loss: 0.6554 - val_accuracy: 0.6096 - lr: 1.1000e-04\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 0.00012000000000000002.\n",
      "Epoch 12/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6401 - accuracy: 0.6337\n",
      "Epoch 12: val_loss improved from 0.65544 to 0.64871, saving model to model.h5\n",
      "19/19 [==============================] - 2s 95ms/step - loss: 0.6396 - accuracy: 0.6353 - val_loss: 0.6487 - val_accuracy: 0.6370 - lr: 1.2000e-04\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 0.00013000000000000002.\n",
      "Epoch 13/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6334 - accuracy: 0.6528\n",
      "Epoch 13: val_loss improved from 0.64871 to 0.64034, saving model to model.h5\n",
      "19/19 [==============================] - 2s 96ms/step - loss: 0.6350 - accuracy: 0.6507 - val_loss: 0.6403 - val_accuracy: 0.6370 - lr: 1.3000e-04\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 0.00014.\n",
      "Epoch 14/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6273 - accuracy: 0.6615\n",
      "Epoch 14: val_loss improved from 0.64034 to 0.63587, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.6275 - accuracy: 0.6592 - val_loss: 0.6359 - val_accuracy: 0.6438 - lr: 1.4000e-04\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 0.00015000000000000001.\n",
      "Epoch 15/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6223 - accuracy: 0.6632\n",
      "Epoch 15: val_loss improved from 0.63587 to 0.63160, saving model to model.h5\n",
      "19/19 [==============================] - 2s 96ms/step - loss: 0.6230 - accuracy: 0.6627 - val_loss: 0.6316 - val_accuracy: 0.6644 - lr: 1.5000e-04\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 0.00016.\n",
      "Epoch 16/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6164 - accuracy: 0.6597\n",
      "Epoch 16: val_loss improved from 0.63160 to 0.62621, saving model to model.h5\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.6183 - accuracy: 0.6575 - val_loss: 0.6262 - val_accuracy: 0.6575 - lr: 1.6000e-04\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 0.00017.\n",
      "Epoch 17/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6145 - accuracy: 0.6753\n",
      "Epoch 17: val_loss did not improve from 0.62621\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.6132 - accuracy: 0.6781 - val_loss: 0.6266 - val_accuracy: 0.6644 - lr: 1.7000e-04\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 0.00018.\n",
      "Epoch 18/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6099 - accuracy: 0.6701\n",
      "Epoch 18: val_loss improved from 0.62621 to 0.62008, saving model to model.h5\n",
      "19/19 [==============================] - 2s 96ms/step - loss: 0.6096 - accuracy: 0.6695 - val_loss: 0.6201 - val_accuracy: 0.6644 - lr: 1.8000e-04\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 0.00019.\n",
      "Epoch 19/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6031 - accuracy: 0.6701\n",
      "Epoch 19: val_loss improved from 0.62008 to 0.61307, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.6038 - accuracy: 0.6712 - val_loss: 0.6131 - val_accuracy: 0.6644 - lr: 1.9000e-04\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 0.0002.\n",
      "Epoch 20/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6010 - accuracy: 0.6788\n",
      "Epoch 20: val_loss improved from 0.61307 to 0.61115, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.6001 - accuracy: 0.6781 - val_loss: 0.6111 - val_accuracy: 0.6575 - lr: 2.0000e-04\n",
      "\n",
      "Epoch 21: LearningRateScheduler setting learning rate to 0.0002.\n",
      "Epoch 21/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5955 - accuracy: 0.6840\n",
      "Epoch 21: val_loss improved from 0.61115 to 0.60850, saving model to model.h5\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.5965 - accuracy: 0.6832 - val_loss: 0.6085 - val_accuracy: 0.6644 - lr: 2.0000e-04\n",
      "\n",
      "Epoch 22: LearningRateScheduler setting learning rate to 0.0002.\n",
      "Epoch 22/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5882 - accuracy: 0.6875\n",
      "Epoch 22: val_loss improved from 0.60850 to 0.60275, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.5914 - accuracy: 0.6832 - val_loss: 0.6028 - val_accuracy: 0.6644 - lr: 2.0000e-04\n",
      "\n",
      "Epoch 23: LearningRateScheduler setting learning rate to 0.0002.\n",
      "Epoch 23/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5876 - accuracy: 0.6997\n",
      "Epoch 23: val_loss did not improve from 0.60275\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5888 - accuracy: 0.6969 - val_loss: 0.6063 - val_accuracy: 0.6712 - lr: 2.0000e-04\n",
      "\n",
      "Epoch 24: LearningRateScheduler setting learning rate to 0.0002.\n",
      "Epoch 24/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5828 - accuracy: 0.6944\n",
      "Epoch 24: val_loss improved from 0.60275 to 0.59949, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.5848 - accuracy: 0.6918 - val_loss: 0.5995 - val_accuracy: 0.6849 - lr: 2.0000e-04\n",
      "\n",
      "Epoch 25: LearningRateScheduler setting learning rate to 0.0002.\n",
      "Epoch 25/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5812 - accuracy: 0.7014\n",
      "Epoch 25: val_loss did not improve from 0.59949\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5807 - accuracy: 0.6986 - val_loss: 0.6025 - val_accuracy: 0.6712 - lr: 2.0000e-04\n",
      "\n",
      "Epoch 26: LearningRateScheduler setting learning rate to 0.0002.\n",
      "Epoch 26/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5810 - accuracy: 0.6997\n",
      "Epoch 26: val_loss improved from 0.59949 to 0.59916, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.5794 - accuracy: 0.7003 - val_loss: 0.5992 - val_accuracy: 0.6712 - lr: 2.0000e-04\n",
      "\n",
      "Epoch 27: LearningRateScheduler setting learning rate to 0.0002.\n",
      "Epoch 27/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5755 - accuracy: 0.7049\n",
      "Epoch 27: val_loss improved from 0.59916 to 0.59773, saving model to model.h5\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.5733 - accuracy: 0.7072 - val_loss: 0.5977 - val_accuracy: 0.6849 - lr: 2.0000e-04\n",
      "\n",
      "Epoch 28: LearningRateScheduler setting learning rate to 0.0002.\n",
      "Epoch 28/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5699 - accuracy: 0.7153\n",
      "Epoch 28: val_loss improved from 0.59773 to 0.59345, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.5695 - accuracy: 0.7140 - val_loss: 0.5934 - val_accuracy: 0.6781 - lr: 2.0000e-04\n",
      "\n",
      "Epoch 29: LearningRateScheduler setting learning rate to 0.0002.\n",
      "Epoch 29/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5650 - accuracy: 0.7170\n",
      "Epoch 29: val_loss improved from 0.59345 to 0.59234, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.5663 - accuracy: 0.7140 - val_loss: 0.5923 - val_accuracy: 0.6712 - lr: 2.0000e-04\n",
      "\n",
      "Epoch 30: LearningRateScheduler setting learning rate to 0.0002.\n",
      "Epoch 30/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5620 - accuracy: 0.7257\n",
      "Epoch 30: val_loss improved from 0.59234 to 0.59173, saving model to model.h5\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.5642 - accuracy: 0.7226 - val_loss: 0.5917 - val_accuracy: 0.6712 - lr: 2.0000e-04\n",
      "\n",
      "Epoch 31: LearningRateScheduler setting learning rate to 0.00015000000000000001.\n",
      "Epoch 31/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5590 - accuracy: 0.7205\n",
      "Epoch 31: val_loss improved from 0.59173 to 0.59139, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.5604 - accuracy: 0.7192 - val_loss: 0.5914 - val_accuracy: 0.6781 - lr: 1.5000e-04\n",
      "\n",
      "Epoch 32: LearningRateScheduler setting learning rate to 0.00014067366430758004.\n",
      "Epoch 32/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5595 - accuracy: 0.7240\n",
      "Epoch 32: val_loss improved from 0.59139 to 0.58959, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.5586 - accuracy: 0.7243 - val_loss: 0.5896 - val_accuracy: 0.6781 - lr: 1.4067e-04\n",
      "\n",
      "Epoch 33: LearningRateScheduler setting learning rate to 0.00013090169943749476.\n",
      "Epoch 33/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5525 - accuracy: 0.7188\n",
      "Epoch 33: val_loss improved from 0.58959 to 0.58583, saving model to model.h5\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.5551 - accuracy: 0.7175 - val_loss: 0.5858 - val_accuracy: 0.7192 - lr: 1.3090e-04\n",
      "\n",
      "Epoch 34: LearningRateScheduler setting learning rate to 0.00012079116908177593.\n",
      "Epoch 34/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5544 - accuracy: 0.7257\n",
      "Epoch 34: val_loss did not improve from 0.58583\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5550 - accuracy: 0.7243 - val_loss: 0.5863 - val_accuracy: 0.7192 - lr: 1.2079e-04\n",
      "\n",
      "Epoch 35: LearningRateScheduler setting learning rate to 0.00011045284632676536.\n",
      "Epoch 35/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5492 - accuracy: 0.7309\n",
      "Epoch 35: val_loss did not improve from 0.58583\n",
      "19/19 [==============================] - 1s 69ms/step - loss: 0.5511 - accuracy: 0.7277 - val_loss: 0.5876 - val_accuracy: 0.6849 - lr: 1.1045e-04\n",
      "\n",
      "Epoch 36: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 36/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5525 - accuracy: 0.7257\n",
      "Epoch 36: val_loss did not improve from 0.58583\n",
      "19/19 [==============================] - 1s 69ms/step - loss: 0.5514 - accuracy: 0.7277 - val_loss: 0.5913 - val_accuracy: 0.6849 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 37: LearningRateScheduler setting learning rate to 8.954715367323468e-05.\n",
      "Epoch 37/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5498 - accuracy: 0.7378\n",
      "Epoch 37: val_loss improved from 0.58583 to 0.58238, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.5481 - accuracy: 0.7397 - val_loss: 0.5824 - val_accuracy: 0.7055 - lr: 8.9547e-05\n",
      "\n",
      "Epoch 38: LearningRateScheduler setting learning rate to 7.920883091822408e-05.\n",
      "Epoch 38/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5500 - accuracy: 0.7292\n",
      "Epoch 38: val_loss did not improve from 0.58238\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5477 - accuracy: 0.7329 - val_loss: 0.5863 - val_accuracy: 0.6986 - lr: 7.9209e-05\n",
      "\n",
      "Epoch 39: LearningRateScheduler setting learning rate to 6.909830056250527e-05.\n",
      "Epoch 39/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5452 - accuracy: 0.7378\n",
      "Epoch 39: val_loss did not improve from 0.58238\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5457 - accuracy: 0.7363 - val_loss: 0.5860 - val_accuracy: 0.6986 - lr: 6.9098e-05\n",
      "\n",
      "Epoch 40: LearningRateScheduler setting learning rate to 5.9326335692419995e-05.\n",
      "Epoch 40/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5442 - accuracy: 0.7413\n",
      "Epoch 40: val_loss did not improve from 0.58238\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5444 - accuracy: 0.7380 - val_loss: 0.5853 - val_accuracy: 0.6986 - lr: 5.9326e-05\n",
      "\n",
      "Epoch 41: LearningRateScheduler setting learning rate to 5.000000000000002e-05.\n",
      "Epoch 41/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5463 - accuracy: 0.7344\n",
      "Epoch 41: val_loss did not improve from 0.58238\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5437 - accuracy: 0.7380 - val_loss: 0.5853 - val_accuracy: 0.7123 - lr: 5.0000e-05\n",
      "\n",
      "Epoch 42: LearningRateScheduler setting learning rate to 4.12214747707527e-05.\n",
      "Epoch 42/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5427 - accuracy: 0.7326\n",
      "Epoch 42: val_loss did not improve from 0.58238\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5429 - accuracy: 0.7329 - val_loss: 0.5868 - val_accuracy: 0.6918 - lr: 4.1221e-05\n",
      "\n",
      "Epoch 43: LearningRateScheduler setting learning rate to 3.308693936411421e-05.\n",
      "Epoch 43/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5429 - accuracy: 0.7361\n",
      "Epoch 43: val_loss did not improve from 0.58238\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5425 - accuracy: 0.7380 - val_loss: 0.5844 - val_accuracy: 0.7192 - lr: 3.3087e-05\n",
      "\n",
      "Epoch 44: LearningRateScheduler setting learning rate to 2.5685517452260567e-05.\n",
      "Epoch 44/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5426 - accuracy: 0.7326\n",
      "Epoch 44: val_loss did not improve from 0.58238\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5416 - accuracy: 0.7363 - val_loss: 0.5849 - val_accuracy: 0.7123 - lr: 2.5686e-05\n",
      "\n",
      "Epoch 45: LearningRateScheduler setting learning rate to 1.9098300562505266e-05.\n",
      "Epoch 45/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5420 - accuracy: 0.7361\n",
      "Epoch 45: val_loss did not improve from 0.58238\n",
      "19/19 [==============================] - 1s 69ms/step - loss: 0.5412 - accuracy: 0.7363 - val_loss: 0.5845 - val_accuracy: 0.7123 - lr: 1.9098e-05\n",
      "\n",
      "Epoch 46: LearningRateScheduler setting learning rate to 1.339745962155613e-05.\n",
      "Epoch 46/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5408 - accuracy: 0.7361\n",
      "Epoch 46: val_loss did not improve from 0.58238\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5407 - accuracy: 0.7363 - val_loss: 0.5845 - val_accuracy: 0.7123 - lr: 1.3397e-05\n",
      "\n",
      "Epoch 47: LearningRateScheduler setting learning rate to 8.645454235739903e-06.\n",
      "Epoch 47/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5366 - accuracy: 0.7413\n",
      "Epoch 47: val_loss did not improve from 0.58238\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5405 - accuracy: 0.7380 - val_loss: 0.5839 - val_accuracy: 0.7192 - lr: 8.6455e-06\n",
      "Epoch 47: early stopping\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 1.25e-05.\n",
      "Epoch 1/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.7774 - accuracy: 0.5052\n",
      "Epoch 1: val_loss improved from inf to 0.80098, saving model to model.h5\n",
      "19/19 [==============================] - 5s 140ms/step - loss: 0.7749 - accuracy: 0.5068 - val_loss: 0.8010 - val_accuracy: 0.4384 - lr: 1.2500e-05\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 2.5e-05.\n",
      "Epoch 2/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.7564 - accuracy: 0.5087\n",
      "Epoch 2: val_loss improved from 0.80098 to 0.76748, saving model to model.h5\n",
      "19/19 [==============================] - 2s 97ms/step - loss: 0.7551 - accuracy: 0.5086 - val_loss: 0.7675 - val_accuracy: 0.4315 - lr: 2.5000e-05\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 3.7500000000000003e-05.\n",
      "Epoch 3/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.7328 - accuracy: 0.4931\n",
      "Epoch 3: val_loss improved from 0.76748 to 0.73282, saving model to model.h5\n",
      "19/19 [==============================] - 2s 95ms/step - loss: 0.7327 - accuracy: 0.4932 - val_loss: 0.7328 - val_accuracy: 0.4041 - lr: 3.7500e-05\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 5e-05.\n",
      "Epoch 4/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.7148 - accuracy: 0.4931\n",
      "Epoch 4: val_loss improved from 0.73282 to 0.71361, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.7152 - accuracy: 0.4914 - val_loss: 0.7136 - val_accuracy: 0.4178 - lr: 5.0000e-05\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 6.25e-05.\n",
      "Epoch 5/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.7053 - accuracy: 0.5052\n",
      "Epoch 5: val_loss improved from 0.71361 to 0.70112, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.7053 - accuracy: 0.5051 - val_loss: 0.7011 - val_accuracy: 0.4452 - lr: 6.2500e-05\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 7.500000000000001e-05.\n",
      "Epoch 6/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6948 - accuracy: 0.5347\n",
      "Epoch 6: val_loss improved from 0.70112 to 0.68568, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.6947 - accuracy: 0.5342 - val_loss: 0.6857 - val_accuracy: 0.5548 - lr: 7.5000e-05\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 8.75e-05.\n",
      "Epoch 7/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6854 - accuracy: 0.5712\n",
      "Epoch 7: val_loss improved from 0.68568 to 0.67737, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.6856 - accuracy: 0.5702 - val_loss: 0.6774 - val_accuracy: 0.5616 - lr: 8.7500e-05\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 8/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6741 - accuracy: 0.6059\n",
      "Epoch 8: val_loss improved from 0.67737 to 0.66906, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.6743 - accuracy: 0.6045 - val_loss: 0.6691 - val_accuracy: 0.5822 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.00011250000000000001.\n",
      "Epoch 9/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6643 - accuracy: 0.6181\n",
      "Epoch 9: val_loss improved from 0.66906 to 0.65835, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.6651 - accuracy: 0.6164 - val_loss: 0.6583 - val_accuracy: 0.6575 - lr: 1.1250e-04\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.000125.\n",
      "Epoch 10/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6568 - accuracy: 0.6215\n",
      "Epoch 10: val_loss improved from 0.65835 to 0.64891, saving model to model.h5\n",
      "19/19 [==============================] - 2s 96ms/step - loss: 0.6565 - accuracy: 0.6233 - val_loss: 0.6489 - val_accuracy: 0.6781 - lr: 1.2500e-04\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 0.00013749999999999998.\n",
      "Epoch 11/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6476 - accuracy: 0.6302\n",
      "Epoch 11: val_loss improved from 0.64891 to 0.63996, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.6475 - accuracy: 0.6318 - val_loss: 0.6400 - val_accuracy: 0.6781 - lr: 1.3750e-04\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 0.00015000000000000001.\n",
      "Epoch 12/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6405 - accuracy: 0.6476\n",
      "Epoch 12: val_loss improved from 0.63996 to 0.63084, saving model to model.h5\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.6399 - accuracy: 0.6473 - val_loss: 0.6308 - val_accuracy: 0.6712 - lr: 1.5000e-04\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 0.00016250000000000002.\n",
      "Epoch 13/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6306 - accuracy: 0.6562\n",
      "Epoch 13: val_loss improved from 0.63084 to 0.62848, saving model to model.h5\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.6317 - accuracy: 0.6541 - val_loss: 0.6285 - val_accuracy: 0.6712 - lr: 1.6250e-04\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 0.000175.\n",
      "Epoch 14/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6261 - accuracy: 0.6562\n",
      "Epoch 14: val_loss improved from 0.62848 to 0.62043, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.6259 - accuracy: 0.6575 - val_loss: 0.6204 - val_accuracy: 0.6644 - lr: 1.7500e-04\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 0.0001875.\n",
      "Epoch 15/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6216 - accuracy: 0.6476\n",
      "Epoch 15: val_loss improved from 0.62043 to 0.61703, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.6202 - accuracy: 0.6490 - val_loss: 0.6170 - val_accuracy: 0.6712 - lr: 1.8750e-04\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 0.0002.\n",
      "Epoch 16/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6135 - accuracy: 0.6615\n",
      "Epoch 16: val_loss improved from 0.61703 to 0.60966, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.6142 - accuracy: 0.6575 - val_loss: 0.6097 - val_accuracy: 0.6849 - lr: 2.0000e-04\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 0.00021250000000000002.\n",
      "Epoch 17/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6087 - accuracy: 0.6667\n",
      "Epoch 17: val_loss improved from 0.60966 to 0.60752, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.6085 - accuracy: 0.6661 - val_loss: 0.6075 - val_accuracy: 0.6918 - lr: 2.1250e-04\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 0.00022500000000000002.\n",
      "Epoch 18/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6018 - accuracy: 0.6771\n",
      "Epoch 18: val_loss improved from 0.60752 to 0.60411, saving model to model.h5\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.6032 - accuracy: 0.6764 - val_loss: 0.6041 - val_accuracy: 0.6986 - lr: 2.2500e-04\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 0.0002375.\n",
      "Epoch 19/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5996 - accuracy: 0.6736\n",
      "Epoch 19: val_loss did not improve from 0.60411\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.6001 - accuracy: 0.6747 - val_loss: 0.6074 - val_accuracy: 0.6918 - lr: 2.3750e-04\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 0.00025.\n",
      "Epoch 20/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5913 - accuracy: 0.6840\n",
      "Epoch 20: val_loss improved from 0.60411 to 0.59844, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.5929 - accuracy: 0.6815 - val_loss: 0.5984 - val_accuracy: 0.6986 - lr: 2.5000e-04\n",
      "\n",
      "Epoch 21: LearningRateScheduler setting learning rate to 0.00025.\n",
      "Epoch 21/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5846 - accuracy: 0.6962\n",
      "Epoch 21: val_loss improved from 0.59844 to 0.59700, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.5890 - accuracy: 0.6884 - val_loss: 0.5970 - val_accuracy: 0.6918 - lr: 2.5000e-04\n",
      "\n",
      "Epoch 22: LearningRateScheduler setting learning rate to 0.00025.\n",
      "Epoch 22/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5817 - accuracy: 0.6910\n",
      "Epoch 22: val_loss improved from 0.59700 to 0.59684, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.5828 - accuracy: 0.6901 - val_loss: 0.5968 - val_accuracy: 0.6986 - lr: 2.5000e-04\n",
      "\n",
      "Epoch 23: LearningRateScheduler setting learning rate to 0.00025.\n",
      "Epoch 23/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5789 - accuracy: 0.6944\n",
      "Epoch 23: val_loss improved from 0.59684 to 0.59565, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.5781 - accuracy: 0.6952 - val_loss: 0.5956 - val_accuracy: 0.7055 - lr: 2.5000e-04\n",
      "\n",
      "Epoch 24: LearningRateScheduler setting learning rate to 0.00025.\n",
      "Epoch 24/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5715 - accuracy: 0.7014\n",
      "Epoch 24: val_loss improved from 0.59565 to 0.59057, saving model to model.h5\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.5740 - accuracy: 0.6986 - val_loss: 0.5906 - val_accuracy: 0.6986 - lr: 2.5000e-04\n",
      "\n",
      "Epoch 25: LearningRateScheduler setting learning rate to 0.00025.\n",
      "Epoch 25/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5739 - accuracy: 0.6944\n",
      "Epoch 25: val_loss did not improve from 0.59057\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5735 - accuracy: 0.6969 - val_loss: 0.5931 - val_accuracy: 0.6849 - lr: 2.5000e-04\n",
      "\n",
      "Epoch 26: LearningRateScheduler setting learning rate to 0.00025.\n",
      "Epoch 26/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5639 - accuracy: 0.7014\n",
      "Epoch 26: val_loss improved from 0.59057 to 0.58669, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.5658 - accuracy: 0.7003 - val_loss: 0.5867 - val_accuracy: 0.7055 - lr: 2.5000e-04\n",
      "\n",
      "Epoch 27: LearningRateScheduler setting learning rate to 0.00025.\n",
      "Epoch 27/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5657 - accuracy: 0.7066\n",
      "Epoch 27: val_loss improved from 0.58669 to 0.58552, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.5633 - accuracy: 0.7089 - val_loss: 0.5855 - val_accuracy: 0.7055 - lr: 2.5000e-04\n",
      "\n",
      "Epoch 28: LearningRateScheduler setting learning rate to 0.00025.\n",
      "Epoch 28/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5572 - accuracy: 0.7170\n",
      "Epoch 28: val_loss improved from 0.58552 to 0.58465, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.5575 - accuracy: 0.7158 - val_loss: 0.5847 - val_accuracy: 0.6986 - lr: 2.5000e-04\n",
      "\n",
      "Epoch 29: LearningRateScheduler setting learning rate to 0.00025.\n",
      "Epoch 29/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5564 - accuracy: 0.7014\n",
      "Epoch 29: val_loss improved from 0.58465 to 0.58083, saving model to model.h5\n",
      "19/19 [==============================] - 2s 96ms/step - loss: 0.5559 - accuracy: 0.7038 - val_loss: 0.5808 - val_accuracy: 0.7123 - lr: 2.5000e-04\n",
      "\n",
      "Epoch 30: LearningRateScheduler setting learning rate to 0.00025.\n",
      "Epoch 30/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5556 - accuracy: 0.7222\n",
      "Epoch 30: val_loss did not improve from 0.58083\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5546 - accuracy: 0.7226 - val_loss: 0.5840 - val_accuracy: 0.7055 - lr: 2.5000e-04\n",
      "\n",
      "Epoch 31: LearningRateScheduler setting learning rate to 0.0001875.\n",
      "Epoch 31/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5447 - accuracy: 0.7240\n",
      "Epoch 31: val_loss did not improve from 0.58083\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5462 - accuracy: 0.7226 - val_loss: 0.5819 - val_accuracy: 0.7123 - lr: 1.8750e-04\n",
      "\n",
      "Epoch 32: LearningRateScheduler setting learning rate to 0.00017584208038447504.\n",
      "Epoch 32/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5436 - accuracy: 0.7222\n",
      "Epoch 32: val_loss improved from 0.58083 to 0.57811, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.5437 - accuracy: 0.7226 - val_loss: 0.5781 - val_accuracy: 0.7123 - lr: 1.7584e-04\n",
      "\n",
      "Epoch 33: LearningRateScheduler setting learning rate to 0.00016362712429686844.\n",
      "Epoch 33/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5400 - accuracy: 0.7309\n",
      "Epoch 33: val_loss did not improve from 0.57811\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5401 - accuracy: 0.7295 - val_loss: 0.5804 - val_accuracy: 0.7055 - lr: 1.6363e-04\n",
      "\n",
      "Epoch 34: LearningRateScheduler setting learning rate to 0.0001509889613522199.\n",
      "Epoch 34/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5388 - accuracy: 0.7326\n",
      "Epoch 34: val_loss did not improve from 0.57811\n",
      "19/19 [==============================] - 1s 72ms/step - loss: 0.5382 - accuracy: 0.7312 - val_loss: 0.5805 - val_accuracy: 0.7055 - lr: 1.5099e-04\n",
      "\n",
      "Epoch 35: LearningRateScheduler setting learning rate to 0.0001380660579084567.\n",
      "Epoch 35/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5380 - accuracy: 0.7292\n",
      "Epoch 35: val_loss improved from 0.57811 to 0.57478, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.5358 - accuracy: 0.7312 - val_loss: 0.5748 - val_accuracy: 0.7055 - lr: 1.3807e-04\n",
      "\n",
      "Epoch 36: LearningRateScheduler setting learning rate to 0.000125.\n",
      "Epoch 36/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5351 - accuracy: 0.7309\n",
      "Epoch 36: val_loss did not improve from 0.57478\n",
      "19/19 [==============================] - 1s 66ms/step - loss: 0.5342 - accuracy: 0.7329 - val_loss: 0.5773 - val_accuracy: 0.7123 - lr: 1.2500e-04\n",
      "\n",
      "Epoch 37: LearningRateScheduler setting learning rate to 0.00011193394209154334.\n",
      "Epoch 37/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5322 - accuracy: 0.7326\n",
      "Epoch 37: val_loss did not improve from 0.57478\n",
      "19/19 [==============================] - 1s 66ms/step - loss: 0.5322 - accuracy: 0.7329 - val_loss: 0.5765 - val_accuracy: 0.7055 - lr: 1.1193e-04\n",
      "\n",
      "Epoch 38: LearningRateScheduler setting learning rate to 9.90110386477801e-05.\n",
      "Epoch 38/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5300 - accuracy: 0.7344\n",
      "Epoch 38: val_loss did not improve from 0.57478\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5308 - accuracy: 0.7346 - val_loss: 0.5749 - val_accuracy: 0.7055 - lr: 9.9011e-05\n",
      "\n",
      "Epoch 39: LearningRateScheduler setting learning rate to 8.637287570313158e-05.\n",
      "Epoch 39/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5279 - accuracy: 0.7448\n",
      "Epoch 39: val_loss improved from 0.57478 to 0.57334, saving model to model.h5\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.5297 - accuracy: 0.7432 - val_loss: 0.5733 - val_accuracy: 0.7123 - lr: 8.6373e-05\n",
      "\n",
      "Epoch 40: LearningRateScheduler setting learning rate to 7.4157919615525e-05.\n",
      "Epoch 40/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5267 - accuracy: 0.7431\n",
      "Epoch 40: val_loss did not improve from 0.57334\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5282 - accuracy: 0.7432 - val_loss: 0.5739 - val_accuracy: 0.7123 - lr: 7.4158e-05\n",
      "\n",
      "Epoch 41: LearningRateScheduler setting learning rate to 6.250000000000003e-05.\n",
      "Epoch 41/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5282 - accuracy: 0.7517\n",
      "Epoch 41: val_loss did not improve from 0.57334\n",
      "19/19 [==============================] - 1s 66ms/step - loss: 0.5271 - accuracy: 0.7517 - val_loss: 0.5753 - val_accuracy: 0.7123 - lr: 6.2500e-05\n",
      "\n",
      "Epoch 42: LearningRateScheduler setting learning rate to 5.152684346344087e-05.\n",
      "Epoch 42/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5278 - accuracy: 0.7448\n",
      "Epoch 42: val_loss improved from 0.57334 to 0.57248, saving model to model.h5\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.5266 - accuracy: 0.7449 - val_loss: 0.5725 - val_accuracy: 0.7055 - lr: 5.1527e-05\n",
      "\n",
      "Epoch 43: LearningRateScheduler setting learning rate to 4.1358674205142764e-05.\n",
      "Epoch 43/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5254 - accuracy: 0.7465\n",
      "Epoch 43: val_loss did not improve from 0.57248\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5252 - accuracy: 0.7466 - val_loss: 0.5747 - val_accuracy: 0.7123 - lr: 4.1359e-05\n",
      "\n",
      "Epoch 44: LearningRateScheduler setting learning rate to 3.2106896815325704e-05.\n",
      "Epoch 44/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5254 - accuracy: 0.7552\n",
      "Epoch 44: val_loss did not improve from 0.57248\n",
      "19/19 [==============================] - 1s 66ms/step - loss: 0.5244 - accuracy: 0.7568 - val_loss: 0.5749 - val_accuracy: 0.7192 - lr: 3.2107e-05\n",
      "\n",
      "Epoch 45: LearningRateScheduler setting learning rate to 2.3872875703131582e-05.\n",
      "Epoch 45/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5212 - accuracy: 0.7587\n",
      "Epoch 45: val_loss did not improve from 0.57248\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5239 - accuracy: 0.7534 - val_loss: 0.5739 - val_accuracy: 0.7123 - lr: 2.3873e-05\n",
      "\n",
      "Epoch 46: LearningRateScheduler setting learning rate to 1.6746824526945162e-05.\n",
      "Epoch 46/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5234 - accuracy: 0.7569\n",
      "Epoch 46: val_loss did not improve from 0.57248\n",
      "19/19 [==============================] - 2s 81ms/step - loss: 0.5234 - accuracy: 0.7586 - val_loss: 0.5752 - val_accuracy: 0.7192 - lr: 1.6747e-05\n",
      "\n",
      "Epoch 47: LearningRateScheduler setting learning rate to 1.0806817794674877e-05.\n",
      "Epoch 47/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5233 - accuracy: 0.7569\n",
      "Epoch 47: val_loss did not improve from 0.57248\n",
      "19/19 [==============================] - 1s 73ms/step - loss: 0.5233 - accuracy: 0.7586 - val_loss: 0.5751 - val_accuracy: 0.7192 - lr: 1.0807e-05\n",
      "\n",
      "Epoch 48: LearningRateScheduler setting learning rate to 6.117935463105809e-06.\n",
      "Epoch 48/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5238 - accuracy: 0.7552\n",
      "Epoch 48: val_loss did not improve from 0.57248\n",
      "19/19 [==============================] - 1s 69ms/step - loss: 0.5230 - accuracy: 0.7586 - val_loss: 0.5744 - val_accuracy: 0.7192 - lr: 6.1179e-06\n",
      "\n",
      "Epoch 49: LearningRateScheduler setting learning rate to 2.731549908274289e-06.\n",
      "Epoch 49/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5196 - accuracy: 0.7569\n",
      "Epoch 49: val_loss did not improve from 0.57248\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5229 - accuracy: 0.7551 - val_loss: 0.5742 - val_accuracy: 0.7123 - lr: 2.7316e-06\n",
      "\n",
      "Epoch 50: LearningRateScheduler setting learning rate to 6.847630789658387e-07.\n",
      "Epoch 50/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5236 - accuracy: 0.7587\n",
      "Epoch 50: val_loss did not improve from 0.57248\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5229 - accuracy: 0.7568 - val_loss: 0.5744 - val_accuracy: 0.7192 - lr: 6.8476e-07\n",
      "\n",
      "Epoch 51: LearningRateScheduler setting learning rate to 0.0.\n",
      "Epoch 51/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5222 - accuracy: 0.7569\n",
      "Epoch 51: val_loss did not improve from 0.57248\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5228 - accuracy: 0.7568 - val_loss: 0.5744 - val_accuracy: 0.7192 - lr: 0.0000e+00\n",
      "\n",
      "Epoch 52: LearningRateScheduler setting learning rate to 6.847630789658387e-07.\n",
      "Epoch 52/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5227 - accuracy: 0.7552\n",
      "Epoch 52: val_loss did not improve from 0.57248\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5228 - accuracy: 0.7568 - val_loss: 0.5743 - val_accuracy: 0.7192 - lr: 6.8476e-07\n",
      "Epoch 52: early stopping\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 1.5000000000000002e-05.\n",
      "Epoch 1/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.7451 - accuracy: 0.4878\n",
      "Epoch 1: val_loss improved from inf to 0.71788, saving model to model.h5\n",
      "19/19 [==============================] - 5s 130ms/step - loss: 0.7474 - accuracy: 0.4812 - val_loss: 0.7179 - val_accuracy: 0.5479 - lr: 1.5000e-05\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 3.0000000000000004e-05.\n",
      "Epoch 2/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.7282 - accuracy: 0.4722\n",
      "Epoch 2: val_loss improved from 0.71788 to 0.70520, saving model to model.h5\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.7263 - accuracy: 0.4760 - val_loss: 0.7052 - val_accuracy: 0.5274 - lr: 3.0000e-05\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 4.5e-05.\n",
      "Epoch 3/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.7060 - accuracy: 0.4792\n",
      "Epoch 3: val_loss improved from 0.70520 to 0.70015, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.7067 - accuracy: 0.4760 - val_loss: 0.7002 - val_accuracy: 0.4795 - lr: 4.5000e-05\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 6.000000000000001e-05.\n",
      "Epoch 4/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6959 - accuracy: 0.5017\n",
      "Epoch 4: val_loss improved from 0.70015 to 0.69532, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.6957 - accuracy: 0.5000 - val_loss: 0.6953 - val_accuracy: 0.4932 - lr: 6.0000e-05\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 7.500000000000001e-05.\n",
      "Epoch 5/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6887 - accuracy: 0.5330\n",
      "Epoch 5: val_loss improved from 0.69532 to 0.68863, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.6886 - accuracy: 0.5342 - val_loss: 0.6886 - val_accuracy: 0.5274 - lr: 7.5000e-05\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 9e-05.\n",
      "Epoch 6/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6807 - accuracy: 0.5226\n",
      "Epoch 6: val_loss improved from 0.68863 to 0.67452, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.6806 - accuracy: 0.5223 - val_loss: 0.6745 - val_accuracy: 0.6027 - lr: 9.0000e-05\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.00010500000000000002.\n",
      "Epoch 7/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6707 - accuracy: 0.5642\n",
      "Epoch 7: val_loss improved from 0.67452 to 0.66939, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.6696 - accuracy: 0.5651 - val_loss: 0.6694 - val_accuracy: 0.5822 - lr: 1.0500e-04\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.00012000000000000002.\n",
      "Epoch 8/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6619 - accuracy: 0.5868\n",
      "Epoch 8: val_loss improved from 0.66939 to 0.65664, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.6611 - accuracy: 0.5890 - val_loss: 0.6566 - val_accuracy: 0.6164 - lr: 1.2000e-04\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.000135.\n",
      "Epoch 9/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6513 - accuracy: 0.6128\n",
      "Epoch 9: val_loss improved from 0.65664 to 0.64718, saving model to model.h5\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.6515 - accuracy: 0.6147 - val_loss: 0.6472 - val_accuracy: 0.6507 - lr: 1.3500e-04\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.00015000000000000001.\n",
      "Epoch 10/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6429 - accuracy: 0.6389\n",
      "Epoch 10: val_loss improved from 0.64718 to 0.63794, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.6436 - accuracy: 0.6370 - val_loss: 0.6379 - val_accuracy: 0.6712 - lr: 1.5000e-04\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 0.00016500000000000003.\n",
      "Epoch 11/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6360 - accuracy: 0.6493\n",
      "Epoch 11: val_loss improved from 0.63794 to 0.63142, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.6357 - accuracy: 0.6490 - val_loss: 0.6314 - val_accuracy: 0.6781 - lr: 1.6500e-04\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 0.00018.\n",
      "Epoch 12/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6349 - accuracy: 0.6458\n",
      "Epoch 12: val_loss improved from 0.63142 to 0.62579, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.6340 - accuracy: 0.6455 - val_loss: 0.6258 - val_accuracy: 0.6644 - lr: 1.8000e-04\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 0.00019500000000000002.\n",
      "Epoch 13/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6248 - accuracy: 0.6597\n",
      "Epoch 13: val_loss improved from 0.62579 to 0.62248, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.6247 - accuracy: 0.6610 - val_loss: 0.6225 - val_accuracy: 0.6712 - lr: 1.9500e-04\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 0.00021000000000000004.\n",
      "Epoch 14/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6188 - accuracy: 0.6528\n",
      "Epoch 14: val_loss improved from 0.62248 to 0.61322, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.6163 - accuracy: 0.6558 - val_loss: 0.6132 - val_accuracy: 0.6849 - lr: 2.1000e-04\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 0.00022500000000000002.\n",
      "Epoch 15/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6102 - accuracy: 0.6667\n",
      "Epoch 15: val_loss improved from 0.61322 to 0.61020, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.6093 - accuracy: 0.6678 - val_loss: 0.6102 - val_accuracy: 0.6849 - lr: 2.2500e-04\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 0.00024000000000000003.\n",
      "Epoch 16/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6042 - accuracy: 0.6736\n",
      "Epoch 16: val_loss improved from 0.61020 to 0.60583, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.6041 - accuracy: 0.6729 - val_loss: 0.6058 - val_accuracy: 0.6986 - lr: 2.4000e-04\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 0.000255.\n",
      "Epoch 17/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6001 - accuracy: 0.6719\n",
      "Epoch 17: val_loss improved from 0.60583 to 0.60400, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.5983 - accuracy: 0.6747 - val_loss: 0.6040 - val_accuracy: 0.6986 - lr: 2.5500e-04\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 0.00027.\n",
      "Epoch 18/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5929 - accuracy: 0.6632\n",
      "Epoch 18: val_loss improved from 0.60400 to 0.59771, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.5930 - accuracy: 0.6627 - val_loss: 0.5977 - val_accuracy: 0.6918 - lr: 2.7000e-04\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 0.000285.\n",
      "Epoch 19/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5871 - accuracy: 0.7014\n",
      "Epoch 19: val_loss did not improve from 0.59771\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5878 - accuracy: 0.6986 - val_loss: 0.6036 - val_accuracy: 0.6986 - lr: 2.8500e-04\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 0.00030000000000000003.\n",
      "Epoch 20/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5854 - accuracy: 0.6719\n",
      "Epoch 20: val_loss improved from 0.59771 to 0.59441, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.5842 - accuracy: 0.6747 - val_loss: 0.5944 - val_accuracy: 0.7055 - lr: 3.0000e-04\n",
      "\n",
      "Epoch 21: LearningRateScheduler setting learning rate to 0.00030000000000000003.\n",
      "Epoch 21/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5717 - accuracy: 0.7049\n",
      "Epoch 21: val_loss did not improve from 0.59441\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5748 - accuracy: 0.7038 - val_loss: 0.5984 - val_accuracy: 0.6986 - lr: 3.0000e-04\n",
      "\n",
      "Epoch 22: LearningRateScheduler setting learning rate to 0.00030000000000000003.\n",
      "Epoch 22/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5742 - accuracy: 0.6997\n",
      "Epoch 22: val_loss improved from 0.59441 to 0.58924, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.5735 - accuracy: 0.7021 - val_loss: 0.5892 - val_accuracy: 0.6986 - lr: 3.0000e-04\n",
      "\n",
      "Epoch 23: LearningRateScheduler setting learning rate to 0.00030000000000000003.\n",
      "Epoch 23/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5615 - accuracy: 0.7049\n",
      "Epoch 23: val_loss improved from 0.58924 to 0.58624, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.5659 - accuracy: 0.7003 - val_loss: 0.5862 - val_accuracy: 0.7329 - lr: 3.0000e-04\n",
      "\n",
      "Epoch 24: LearningRateScheduler setting learning rate to 0.00030000000000000003.\n",
      "Epoch 24/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5561 - accuracy: 0.7309\n",
      "Epoch 24: val_loss did not improve from 0.58624\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5591 - accuracy: 0.7277 - val_loss: 0.5877 - val_accuracy: 0.7192 - lr: 3.0000e-04\n",
      "\n",
      "Epoch 25: LearningRateScheduler setting learning rate to 0.00030000000000000003.\n",
      "Epoch 25/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5563 - accuracy: 0.7326\n",
      "Epoch 25: val_loss did not improve from 0.58624\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5552 - accuracy: 0.7346 - val_loss: 0.5864 - val_accuracy: 0.7260 - lr: 3.0000e-04\n",
      "\n",
      "Epoch 26: LearningRateScheduler setting learning rate to 0.00030000000000000003.\n",
      "Epoch 26/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5505 - accuracy: 0.7483\n",
      "Epoch 26: val_loss did not improve from 0.58624\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5498 - accuracy: 0.7500 - val_loss: 0.5891 - val_accuracy: 0.7123 - lr: 3.0000e-04\n",
      "\n",
      "Epoch 27: LearningRateScheduler setting learning rate to 0.00030000000000000003.\n",
      "Epoch 27/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5484 - accuracy: 0.7361\n",
      "Epoch 27: val_loss improved from 0.58624 to 0.58348, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.5487 - accuracy: 0.7346 - val_loss: 0.5835 - val_accuracy: 0.7123 - lr: 3.0000e-04\n",
      "\n",
      "Epoch 28: LearningRateScheduler setting learning rate to 0.00030000000000000003.\n",
      "Epoch 28/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5385 - accuracy: 0.7344\n",
      "Epoch 28: val_loss improved from 0.58348 to 0.58178, saving model to model.h5\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.5412 - accuracy: 0.7295 - val_loss: 0.5818 - val_accuracy: 0.7192 - lr: 3.0000e-04\n",
      "\n",
      "Epoch 29: LearningRateScheduler setting learning rate to 0.00030000000000000003.\n",
      "Epoch 29/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5370 - accuracy: 0.7465\n",
      "Epoch 29: val_loss improved from 0.58178 to 0.57941, saving model to model.h5\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.5353 - accuracy: 0.7483 - val_loss: 0.5794 - val_accuracy: 0.7192 - lr: 3.0000e-04\n",
      "\n",
      "Epoch 30: LearningRateScheduler setting learning rate to 0.00030000000000000003.\n",
      "Epoch 30/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5283 - accuracy: 0.7569\n",
      "Epoch 30: val_loss did not improve from 0.57941\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5290 - accuracy: 0.7551 - val_loss: 0.5837 - val_accuracy: 0.7329 - lr: 3.0000e-04\n",
      "\n",
      "Epoch 31: LearningRateScheduler setting learning rate to 0.00022500000000000002.\n",
      "Epoch 31/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5257 - accuracy: 0.7639\n",
      "Epoch 31: val_loss improved from 0.57941 to 0.57804, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.5265 - accuracy: 0.7637 - val_loss: 0.5780 - val_accuracy: 0.7397 - lr: 2.2500e-04\n",
      "\n",
      "Epoch 32: LearningRateScheduler setting learning rate to 0.0002110104964613701.\n",
      "Epoch 32/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5236 - accuracy: 0.7639\n",
      "Epoch 32: val_loss improved from 0.57804 to 0.57389, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.5220 - accuracy: 0.7654 - val_loss: 0.5739 - val_accuracy: 0.7192 - lr: 2.1101e-04\n",
      "\n",
      "Epoch 33: LearningRateScheduler setting learning rate to 0.00019635254915624213.\n",
      "Epoch 33/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5181 - accuracy: 0.7691\n",
      "Epoch 33: val_loss did not improve from 0.57389\n",
      "19/19 [==============================] - 1s 69ms/step - loss: 0.5183 - accuracy: 0.7688 - val_loss: 0.5749 - val_accuracy: 0.7329 - lr: 1.9635e-04\n",
      "\n",
      "Epoch 34: LearningRateScheduler setting learning rate to 0.0001811867536226639.\n",
      "Epoch 34/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5164 - accuracy: 0.7726\n",
      "Epoch 34: val_loss improved from 0.57389 to 0.57100, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.5161 - accuracy: 0.7705 - val_loss: 0.5710 - val_accuracy: 0.7192 - lr: 1.8119e-04\n",
      "\n",
      "Epoch 35: LearningRateScheduler setting learning rate to 0.00016567926949014804.\n",
      "Epoch 35/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5116 - accuracy: 0.7812\n",
      "Epoch 35: val_loss did not improve from 0.57100\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5129 - accuracy: 0.7808 - val_loss: 0.5741 - val_accuracy: 0.7397 - lr: 1.6568e-04\n",
      "\n",
      "Epoch 36: LearningRateScheduler setting learning rate to 0.00015000000000000001.\n",
      "Epoch 36/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5107 - accuracy: 0.7830\n",
      "Epoch 36: val_loss did not improve from 0.57100\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5109 - accuracy: 0.7825 - val_loss: 0.5718 - val_accuracy: 0.7192 - lr: 1.5000e-04\n",
      "\n",
      "Epoch 37: LearningRateScheduler setting learning rate to 0.00013432073050985202.\n",
      "Epoch 37/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5124 - accuracy: 0.7743\n",
      "Epoch 37: val_loss did not improve from 0.57100\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5105 - accuracy: 0.7774 - val_loss: 0.5730 - val_accuracy: 0.7329 - lr: 1.3432e-04\n",
      "\n",
      "Epoch 38: LearningRateScheduler setting learning rate to 0.00011881324637733614.\n",
      "Epoch 38/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5073 - accuracy: 0.7760\n",
      "Epoch 38: val_loss improved from 0.57100 to 0.56816, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.5063 - accuracy: 0.7791 - val_loss: 0.5682 - val_accuracy: 0.7123 - lr: 1.1881e-04\n",
      "\n",
      "Epoch 39: LearningRateScheduler setting learning rate to 0.00010364745084375791.\n",
      "Epoch 39/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5061 - accuracy: 0.7778\n",
      "Epoch 39: val_loss did not improve from 0.56816\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5077 - accuracy: 0.7740 - val_loss: 0.5710 - val_accuracy: 0.7329 - lr: 1.0365e-04\n",
      "\n",
      "Epoch 40: LearningRateScheduler setting learning rate to 8.898950353863e-05.\n",
      "Epoch 40/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5023 - accuracy: 0.7830\n",
      "Epoch 40: val_loss improved from 0.56816 to 0.56800, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.5035 - accuracy: 0.7825 - val_loss: 0.5680 - val_accuracy: 0.7260 - lr: 8.8990e-05\n",
      "\n",
      "Epoch 41: LearningRateScheduler setting learning rate to 7.500000000000003e-05.\n",
      "Epoch 41/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5006 - accuracy: 0.7830\n",
      "Epoch 41: val_loss did not improve from 0.56800\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5026 - accuracy: 0.7808 - val_loss: 0.5682 - val_accuracy: 0.7192 - lr: 7.5000e-05\n",
      "\n",
      "Epoch 42: LearningRateScheduler setting learning rate to 6.183221215612905e-05.\n",
      "Epoch 42/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5004 - accuracy: 0.7847\n",
      "Epoch 42: val_loss did not improve from 0.56800\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5011 - accuracy: 0.7860 - val_loss: 0.5704 - val_accuracy: 0.7329 - lr: 6.1832e-05\n",
      "\n",
      "Epoch 43: LearningRateScheduler setting learning rate to 4.963040904617132e-05.\n",
      "Epoch 43/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5014 - accuracy: 0.7778\n",
      "Epoch 43: val_loss did not improve from 0.56800\n",
      "19/19 [==============================] - 1s 69ms/step - loss: 0.5012 - accuracy: 0.7774 - val_loss: 0.5718 - val_accuracy: 0.7260 - lr: 4.9630e-05\n",
      "\n",
      "Epoch 44: LearningRateScheduler setting learning rate to 3.852827617839085e-05.\n",
      "Epoch 44/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5005 - accuracy: 0.7830\n",
      "Epoch 44: val_loss did not improve from 0.56800\n",
      "19/19 [==============================] - 1s 69ms/step - loss: 0.5000 - accuracy: 0.7842 - val_loss: 0.5702 - val_accuracy: 0.7260 - lr: 3.8528e-05\n",
      "\n",
      "Epoch 45: LearningRateScheduler setting learning rate to 2.86474508437579e-05.\n",
      "Epoch 45/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4994 - accuracy: 0.7865\n",
      "Epoch 45: val_loss did not improve from 0.56800\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.4990 - accuracy: 0.7877 - val_loss: 0.5690 - val_accuracy: 0.7260 - lr: 2.8647e-05\n",
      "\n",
      "Epoch 46: LearningRateScheduler setting learning rate to 2.0096189432334194e-05.\n",
      "Epoch 46/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5001 - accuracy: 0.7882\n",
      "Epoch 46: val_loss did not improve from 0.56800\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.4986 - accuracy: 0.7894 - val_loss: 0.5693 - val_accuracy: 0.7192 - lr: 2.0096e-05\n",
      "\n",
      "Epoch 47: LearningRateScheduler setting learning rate to 1.2968181353609854e-05.\n",
      "Epoch 47/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4990 - accuracy: 0.7865\n",
      "Epoch 47: val_loss did not improve from 0.56800\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.4983 - accuracy: 0.7877 - val_loss: 0.5690 - val_accuracy: 0.7260 - lr: 1.2968e-05\n",
      "\n",
      "Epoch 48: LearningRateScheduler setting learning rate to 7.341522555726971e-06.\n",
      "Epoch 48/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5005 - accuracy: 0.7847\n",
      "Epoch 48: val_loss did not improve from 0.56800\n",
      "19/19 [==============================] - 1s 69ms/step - loss: 0.4980 - accuracy: 0.7877 - val_loss: 0.5692 - val_accuracy: 0.7260 - lr: 7.3415e-06\n",
      "\n",
      "Epoch 49: LearningRateScheduler setting learning rate to 3.277859889929147e-06.\n",
      "Epoch 49/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4968 - accuracy: 0.7917\n",
      "Epoch 49: val_loss did not improve from 0.56800\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.4980 - accuracy: 0.7911 - val_loss: 0.5693 - val_accuracy: 0.7260 - lr: 3.2779e-06\n",
      "\n",
      "Epoch 50: LearningRateScheduler setting learning rate to 8.217156947590066e-07.\n",
      "Epoch 50/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4962 - accuracy: 0.7917\n",
      "Epoch 50: val_loss did not improve from 0.56800\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.4979 - accuracy: 0.7911 - val_loss: 0.5693 - val_accuracy: 0.7260 - lr: 8.2172e-07\n",
      "Epoch 50: early stopping\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 1.75e-05.\n",
      "Epoch 1/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.7192 - accuracy: 0.5226\n",
      "Epoch 1: val_loss improved from inf to 0.74291, saving model to model.h5\n",
      "19/19 [==============================] - 4s 128ms/step - loss: 0.7203 - accuracy: 0.5223 - val_loss: 0.7429 - val_accuracy: 0.4452 - lr: 1.7500e-05\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 3.5e-05.\n",
      "Epoch 2/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.7033 - accuracy: 0.5382\n",
      "Epoch 2: val_loss improved from 0.74291 to 0.70996, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.7030 - accuracy: 0.5394 - val_loss: 0.7100 - val_accuracy: 0.5205 - lr: 3.5000e-05\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 5.2499999999999995e-05.\n",
      "Epoch 3/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6899 - accuracy: 0.5486\n",
      "Epoch 3: val_loss improved from 0.70996 to 0.68644, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.6891 - accuracy: 0.5514 - val_loss: 0.6864 - val_accuracy: 0.5548 - lr: 5.2500e-05\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 7e-05.\n",
      "Epoch 4/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6806 - accuracy: 0.5590\n",
      "Epoch 4: val_loss improved from 0.68644 to 0.67371, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.6788 - accuracy: 0.5634 - val_loss: 0.6737 - val_accuracy: 0.5753 - lr: 7.0000e-05\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 8.75e-05.\n",
      "Epoch 5/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6710 - accuracy: 0.5885\n",
      "Epoch 5: val_loss improved from 0.67371 to 0.66555, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.6707 - accuracy: 0.5873 - val_loss: 0.6655 - val_accuracy: 0.6096 - lr: 8.7500e-05\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.00010499999999999999.\n",
      "Epoch 6/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6644 - accuracy: 0.5972\n",
      "Epoch 6: val_loss improved from 0.66555 to 0.66196, saving model to model.h5\n",
      "19/19 [==============================] - 2s 96ms/step - loss: 0.6658 - accuracy: 0.5942 - val_loss: 0.6620 - val_accuracy: 0.5753 - lr: 1.0500e-04\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.0001225.\n",
      "Epoch 7/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6538 - accuracy: 0.6198\n",
      "Epoch 7: val_loss improved from 0.66196 to 0.64562, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.6521 - accuracy: 0.6233 - val_loss: 0.6456 - val_accuracy: 0.6233 - lr: 1.2250e-04\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.00014.\n",
      "Epoch 8/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6452 - accuracy: 0.6493\n",
      "Epoch 8: val_loss improved from 0.64562 to 0.63935, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.6445 - accuracy: 0.6507 - val_loss: 0.6394 - val_accuracy: 0.6438 - lr: 1.4000e-04\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.0001575.\n",
      "Epoch 9/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6333 - accuracy: 0.6441\n",
      "Epoch 9: val_loss improved from 0.63935 to 0.62789, saving model to model.h5\n",
      "19/19 [==============================] - 2s 95ms/step - loss: 0.6348 - accuracy: 0.6421 - val_loss: 0.6279 - val_accuracy: 0.6644 - lr: 1.5750e-04\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.000175.\n",
      "Epoch 10/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6273 - accuracy: 0.6562\n",
      "Epoch 10: val_loss improved from 0.62789 to 0.62745, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.6261 - accuracy: 0.6592 - val_loss: 0.6274 - val_accuracy: 0.6849 - lr: 1.7500e-04\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 0.00019250000000000002.\n",
      "Epoch 11/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6212 - accuracy: 0.6545\n",
      "Epoch 11: val_loss improved from 0.62745 to 0.61868, saving model to model.h5\n",
      "19/19 [==============================] - 2s 97ms/step - loss: 0.6199 - accuracy: 0.6558 - val_loss: 0.6187 - val_accuracy: 0.6918 - lr: 1.9250e-04\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 0.00020999999999999998.\n",
      "Epoch 12/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6106 - accuracy: 0.6632\n",
      "Epoch 12: val_loss improved from 0.61868 to 0.61735, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.6123 - accuracy: 0.6592 - val_loss: 0.6174 - val_accuracy: 0.7055 - lr: 2.1000e-04\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 0.0002275.\n",
      "Epoch 13/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6034 - accuracy: 0.6736\n",
      "Epoch 13: val_loss improved from 0.61735 to 0.60616, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.6048 - accuracy: 0.6695 - val_loss: 0.6062 - val_accuracy: 0.7192 - lr: 2.2750e-04\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 0.000245.\n",
      "Epoch 14/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5984 - accuracy: 0.6701\n",
      "Epoch 14: val_loss improved from 0.60616 to 0.60612, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.5975 - accuracy: 0.6729 - val_loss: 0.6061 - val_accuracy: 0.6986 - lr: 2.4500e-04\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 0.00026250000000000004.\n",
      "Epoch 15/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5956 - accuracy: 0.6632\n",
      "Epoch 15: val_loss improved from 0.60612 to 0.59524, saving model to model.h5\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.5947 - accuracy: 0.6661 - val_loss: 0.5952 - val_accuracy: 0.7329 - lr: 2.6250e-04\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 0.00028.\n",
      "Epoch 16/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5922 - accuracy: 0.6840\n",
      "Epoch 16: val_loss did not improve from 0.59524\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5896 - accuracy: 0.6884 - val_loss: 0.6063 - val_accuracy: 0.6986 - lr: 2.8000e-04\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 0.00029749999999999997.\n",
      "Epoch 17/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5810 - accuracy: 0.6736\n",
      "Epoch 17: val_loss improved from 0.59524 to 0.58728, saving model to model.h5\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.5788 - accuracy: 0.6764 - val_loss: 0.5873 - val_accuracy: 0.7397 - lr: 2.9750e-04\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 0.000315.\n",
      "Epoch 18/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5729 - accuracy: 0.7049\n",
      "Epoch 18: val_loss did not improve from 0.58728\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5723 - accuracy: 0.7072 - val_loss: 0.5894 - val_accuracy: 0.7260 - lr: 3.1500e-04\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 0.0003325.\n",
      "Epoch 19/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5676 - accuracy: 0.6997\n",
      "Epoch 19: val_loss improved from 0.58728 to 0.58151, saving model to model.h5\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.5665 - accuracy: 0.7021 - val_loss: 0.5815 - val_accuracy: 0.7329 - lr: 3.3250e-04\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 0.00035.\n",
      "Epoch 20/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5574 - accuracy: 0.7135\n",
      "Epoch 20: val_loss improved from 0.58151 to 0.58057, saving model to model.h5\n",
      "19/19 [==============================] - 2s 95ms/step - loss: 0.5608 - accuracy: 0.7089 - val_loss: 0.5806 - val_accuracy: 0.7329 - lr: 3.5000e-04\n",
      "\n",
      "Epoch 21: LearningRateScheduler setting learning rate to 0.00035.\n",
      "Epoch 21/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5567 - accuracy: 0.7083\n",
      "Epoch 21: val_loss improved from 0.58057 to 0.57689, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.5566 - accuracy: 0.7106 - val_loss: 0.5769 - val_accuracy: 0.7397 - lr: 3.5000e-04\n",
      "\n",
      "Epoch 22: LearningRateScheduler setting learning rate to 0.00035.\n",
      "Epoch 22/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5501 - accuracy: 0.7396\n",
      "Epoch 22: val_loss did not improve from 0.57689\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5502 - accuracy: 0.7380 - val_loss: 0.5863 - val_accuracy: 0.7260 - lr: 3.5000e-04\n",
      "\n",
      "Epoch 23: LearningRateScheduler setting learning rate to 0.00035.\n",
      "Epoch 23/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5433 - accuracy: 0.7274\n",
      "Epoch 23: val_loss improved from 0.57689 to 0.57365, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.5432 - accuracy: 0.7277 - val_loss: 0.5736 - val_accuracy: 0.7260 - lr: 3.5000e-04\n",
      "\n",
      "Epoch 24: LearningRateScheduler setting learning rate to 0.00035.\n",
      "Epoch 24/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5366 - accuracy: 0.7500\n",
      "Epoch 24: val_loss did not improve from 0.57365\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5376 - accuracy: 0.7483 - val_loss: 0.5825 - val_accuracy: 0.7260 - lr: 3.5000e-04\n",
      "\n",
      "Epoch 25: LearningRateScheduler setting learning rate to 0.00035.\n",
      "Epoch 25/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5342 - accuracy: 0.7344\n",
      "Epoch 25: val_loss improved from 0.57365 to 0.57346, saving model to model.h5\n",
      "19/19 [==============================] - 2s 95ms/step - loss: 0.5325 - accuracy: 0.7363 - val_loss: 0.5735 - val_accuracy: 0.7260 - lr: 3.5000e-04\n",
      "\n",
      "Epoch 26: LearningRateScheduler setting learning rate to 0.00035.\n",
      "Epoch 26/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5271 - accuracy: 0.7309\n",
      "Epoch 26: val_loss improved from 0.57346 to 0.57076, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.5260 - accuracy: 0.7312 - val_loss: 0.5708 - val_accuracy: 0.7260 - lr: 3.5000e-04\n",
      "\n",
      "Epoch 27: LearningRateScheduler setting learning rate to 0.00035.\n",
      "Epoch 27/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5233 - accuracy: 0.7552\n",
      "Epoch 27: val_loss did not improve from 0.57076\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5220 - accuracy: 0.7568 - val_loss: 0.5759 - val_accuracy: 0.7123 - lr: 3.5000e-04\n",
      "\n",
      "Epoch 28: LearningRateScheduler setting learning rate to 0.00035.\n",
      "Epoch 28/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5251 - accuracy: 0.7378\n",
      "Epoch 28: val_loss improved from 0.57076 to 0.56808, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.5254 - accuracy: 0.7363 - val_loss: 0.5681 - val_accuracy: 0.7192 - lr: 3.5000e-04\n",
      "\n",
      "Epoch 29: LearningRateScheduler setting learning rate to 0.00035.\n",
      "Epoch 29/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5161 - accuracy: 0.7517\n",
      "Epoch 29: val_loss improved from 0.56808 to 0.56458, saving model to model.h5\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.5148 - accuracy: 0.7551 - val_loss: 0.5646 - val_accuracy: 0.7123 - lr: 3.5000e-04\n",
      "\n",
      "Epoch 30: LearningRateScheduler setting learning rate to 0.00035.\n",
      "Epoch 30/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5145 - accuracy: 0.7622\n",
      "Epoch 30: val_loss improved from 0.56458 to 0.56286, saving model to model.h5\n",
      "19/19 [==============================] - 2s 95ms/step - loss: 0.5139 - accuracy: 0.7637 - val_loss: 0.5629 - val_accuracy: 0.7397 - lr: 3.5000e-04\n",
      "\n",
      "Epoch 31: LearningRateScheduler setting learning rate to 0.0002625.\n",
      "Epoch 31/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5075 - accuracy: 0.7812\n",
      "Epoch 31: val_loss did not improve from 0.56286\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5068 - accuracy: 0.7808 - val_loss: 0.5704 - val_accuracy: 0.7329 - lr: 2.6250e-04\n",
      "\n",
      "Epoch 32: LearningRateScheduler setting learning rate to 0.00024617891253826506.\n",
      "Epoch 32/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4994 - accuracy: 0.7708\n",
      "Epoch 32: val_loss did not improve from 0.56286\n",
      "19/19 [==============================] - 1s 69ms/step - loss: 0.4997 - accuracy: 0.7688 - val_loss: 0.5662 - val_accuracy: 0.7123 - lr: 2.4618e-04\n",
      "\n",
      "Epoch 33: LearningRateScheduler setting learning rate to 0.0002290779740156158.\n",
      "Epoch 33/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4929 - accuracy: 0.7674\n",
      "Epoch 33: val_loss improved from 0.56286 to 0.56142, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.4938 - accuracy: 0.7671 - val_loss: 0.5614 - val_accuracy: 0.7123 - lr: 2.2908e-04\n",
      "\n",
      "Epoch 34: LearningRateScheduler setting learning rate to 0.00021138454589310785.\n",
      "Epoch 34/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4953 - accuracy: 0.7830\n",
      "Epoch 34: val_loss did not improve from 0.56142\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.4935 - accuracy: 0.7825 - val_loss: 0.5666 - val_accuracy: 0.7123 - lr: 2.1138e-04\n",
      "\n",
      "Epoch 35: LearningRateScheduler setting learning rate to 0.00019329248107183937.\n",
      "Epoch 35/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4962 - accuracy: 0.7622\n",
      "Epoch 35: val_loss did not improve from 0.56142\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.4938 - accuracy: 0.7637 - val_loss: 0.5620 - val_accuracy: 0.7192 - lr: 1.9329e-04\n",
      "\n",
      "Epoch 36: LearningRateScheduler setting learning rate to 0.000175.\n",
      "Epoch 36/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4857 - accuracy: 0.7917\n",
      "Epoch 36: val_loss did not improve from 0.56142\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.4863 - accuracy: 0.7894 - val_loss: 0.5623 - val_accuracy: 0.7123 - lr: 1.7500e-04\n",
      "\n",
      "Epoch 37: LearningRateScheduler setting learning rate to 0.00015670751892816066.\n",
      "Epoch 37/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4824 - accuracy: 0.7934\n",
      "Epoch 37: val_loss did not improve from 0.56142\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.4832 - accuracy: 0.7911 - val_loss: 0.5635 - val_accuracy: 0.7192 - lr: 1.5671e-04\n",
      "\n",
      "Epoch 38: LearningRateScheduler setting learning rate to 0.00013861545410689214.\n",
      "Epoch 38/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4831 - accuracy: 0.7830\n",
      "Epoch 38: val_loss did not improve from 0.56142\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.4812 - accuracy: 0.7860 - val_loss: 0.5632 - val_accuracy: 0.7123 - lr: 1.3862e-04\n",
      "\n",
      "Epoch 39: LearningRateScheduler setting learning rate to 0.00012092202598438422.\n",
      "Epoch 39/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4807 - accuracy: 0.7865\n",
      "Epoch 39: val_loss did not improve from 0.56142\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.4791 - accuracy: 0.7894 - val_loss: 0.5615 - val_accuracy: 0.7260 - lr: 1.2092e-04\n",
      "\n",
      "Epoch 40: LearningRateScheduler setting learning rate to 0.00010382108746173499.\n",
      "Epoch 40/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4789 - accuracy: 0.7882\n",
      "Epoch 40: val_loss improved from 0.56142 to 0.56091, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.4773 - accuracy: 0.7877 - val_loss: 0.5609 - val_accuracy: 0.7192 - lr: 1.0382e-04\n",
      "\n",
      "Epoch 41: LearningRateScheduler setting learning rate to 8.750000000000004e-05.\n",
      "Epoch 41/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4750 - accuracy: 0.7882\n",
      "Epoch 41: val_loss did not improve from 0.56091\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.4760 - accuracy: 0.7877 - val_loss: 0.5623 - val_accuracy: 0.7123 - lr: 8.7500e-05\n",
      "\n",
      "Epoch 42: LearningRateScheduler setting learning rate to 7.213758084881722e-05.\n",
      "Epoch 42/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4733 - accuracy: 0.7986\n",
      "Epoch 42: val_loss improved from 0.56091 to 0.56040, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.4745 - accuracy: 0.7962 - val_loss: 0.5604 - val_accuracy: 0.7260 - lr: 7.2138e-05\n",
      "\n",
      "Epoch 43: LearningRateScheduler setting learning rate to 5.7902143887199866e-05.\n",
      "Epoch 43/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4727 - accuracy: 0.7969\n",
      "Epoch 43: val_loss did not improve from 0.56040\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.4737 - accuracy: 0.7962 - val_loss: 0.5620 - val_accuracy: 0.7192 - lr: 5.7902e-05\n",
      "\n",
      "Epoch 44: LearningRateScheduler setting learning rate to 4.494965554145599e-05.\n",
      "Epoch 44/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4747 - accuracy: 0.7969\n",
      "Epoch 44: val_loss did not improve from 0.56040\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.4730 - accuracy: 0.7979 - val_loss: 0.5622 - val_accuracy: 0.7055 - lr: 4.4950e-05\n",
      "\n",
      "Epoch 45: LearningRateScheduler setting learning rate to 3.342202598438421e-05.\n",
      "Epoch 45/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4726 - accuracy: 0.7969\n",
      "Epoch 45: val_loss did not improve from 0.56040\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.4722 - accuracy: 0.7979 - val_loss: 0.5609 - val_accuracy: 0.7192 - lr: 3.3422e-05\n",
      "\n",
      "Epoch 46: LearningRateScheduler setting learning rate to 2.3445554337723226e-05.\n",
      "Epoch 46/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4731 - accuracy: 0.7969\n",
      "Epoch 46: val_loss did not improve from 0.56040\n",
      "19/19 [==============================] - 1s 71ms/step - loss: 0.4716 - accuracy: 0.7979 - val_loss: 0.5614 - val_accuracy: 0.7192 - lr: 2.3446e-05\n",
      "\n",
      "Epoch 47: LearningRateScheduler setting learning rate to 1.5129544912544829e-05.\n",
      "Epoch 47/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4728 - accuracy: 0.7951\n",
      "Epoch 47: val_loss did not improve from 0.56040\n",
      "19/19 [==============================] - 1s 69ms/step - loss: 0.4713 - accuracy: 0.7979 - val_loss: 0.5609 - val_accuracy: 0.7123 - lr: 1.5130e-05\n",
      "\n",
      "Epoch 48: LearningRateScheduler setting learning rate to 8.565109648348132e-06.\n",
      "Epoch 48/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4707 - accuracy: 0.7986\n",
      "Epoch 48: val_loss did not improve from 0.56040\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.4709 - accuracy: 0.7979 - val_loss: 0.5606 - val_accuracy: 0.7192 - lr: 8.5651e-06\n",
      "\n",
      "Epoch 49: LearningRateScheduler setting learning rate to 3.824169871584004e-06.\n",
      "Epoch 49/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4687 - accuracy: 0.7969\n",
      "Epoch 49: val_loss did not improve from 0.56040\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.4708 - accuracy: 0.7945 - val_loss: 0.5605 - val_accuracy: 0.7192 - lr: 3.8242e-06\n",
      "\n",
      "Epoch 50: LearningRateScheduler setting learning rate to 9.586683105521743e-07.\n",
      "Epoch 50/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4695 - accuracy: 0.7986\n",
      "Epoch 50: val_loss did not improve from 0.56040\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.4707 - accuracy: 0.7962 - val_loss: 0.5606 - val_accuracy: 0.7192 - lr: 9.5867e-07\n",
      "\n",
      "Epoch 51: LearningRateScheduler setting learning rate to 0.0.\n",
      "Epoch 51/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4708 - accuracy: 0.7986\n",
      "Epoch 51: val_loss did not improve from 0.56040\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.4707 - accuracy: 0.7979 - val_loss: 0.5607 - val_accuracy: 0.7192 - lr: 0.0000e+00\n",
      "\n",
      "Epoch 52: LearningRateScheduler setting learning rate to 9.586683105521743e-07.\n",
      "Epoch 52/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4693 - accuracy: 0.8003\n",
      "Epoch 52: val_loss did not improve from 0.56040\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.4707 - accuracy: 0.7979 - val_loss: 0.5607 - val_accuracy: 0.7192 - lr: 9.5867e-07\n",
      "Epoch 52: early stopping\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 2e-05.\n",
      "Epoch 1/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.7844 - accuracy: 0.5156\n",
      "Epoch 1: val_loss improved from inf to 0.81302, saving model to model.h5\n",
      "19/19 [==============================] - 5s 135ms/step - loss: 0.7827 - accuracy: 0.5171 - val_loss: 0.8130 - val_accuracy: 0.4452 - lr: 2.0000e-05\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 4e-05.\n",
      "Epoch 2/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.7350 - accuracy: 0.5191\n",
      "Epoch 2: val_loss improved from 0.81302 to 0.73619, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.7347 - accuracy: 0.5188 - val_loss: 0.7362 - val_accuracy: 0.4658 - lr: 4.0000e-05\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 6.000000000000001e-05.\n",
      "Epoch 3/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6929 - accuracy: 0.5469\n",
      "Epoch 3: val_loss improved from 0.73619 to 0.68969, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.6936 - accuracy: 0.5479 - val_loss: 0.6897 - val_accuracy: 0.5616 - lr: 6.0000e-05\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 8e-05.\n",
      "Epoch 4/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6795 - accuracy: 0.5694\n",
      "Epoch 4: val_loss improved from 0.68969 to 0.67151, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.6794 - accuracy: 0.5702 - val_loss: 0.6715 - val_accuracy: 0.5822 - lr: 8.0000e-05\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 5/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6681 - accuracy: 0.6042\n",
      "Epoch 5: val_loss improved from 0.67151 to 0.66655, saving model to model.h5\n",
      "19/19 [==============================] - 2s 95ms/step - loss: 0.6682 - accuracy: 0.6027 - val_loss: 0.6665 - val_accuracy: 0.6370 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.00012000000000000002.\n",
      "Epoch 6/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6583 - accuracy: 0.6233\n",
      "Epoch 6: val_loss improved from 0.66655 to 0.65514, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.6581 - accuracy: 0.6250 - val_loss: 0.6551 - val_accuracy: 0.6575 - lr: 1.2000e-04\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.00014.\n",
      "Epoch 7/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6487 - accuracy: 0.6285\n",
      "Epoch 7: val_loss improved from 0.65514 to 0.64731, saving model to model.h5\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.6484 - accuracy: 0.6284 - val_loss: 0.6473 - val_accuracy: 0.6849 - lr: 1.4000e-04\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.00016.\n",
      "Epoch 8/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6395 - accuracy: 0.6372\n",
      "Epoch 8: val_loss improved from 0.64731 to 0.63420, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.6393 - accuracy: 0.6370 - val_loss: 0.6342 - val_accuracy: 0.6986 - lr: 1.6000e-04\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.00018.\n",
      "Epoch 9/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6306 - accuracy: 0.6701\n",
      "Epoch 9: val_loss improved from 0.63420 to 0.62417, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.6308 - accuracy: 0.6695 - val_loss: 0.6242 - val_accuracy: 0.7055 - lr: 1.8000e-04\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.0002.\n",
      "Epoch 10/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6275 - accuracy: 0.6406\n",
      "Epoch 10: val_loss improved from 0.62417 to 0.62209, saving model to model.h5\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.6270 - accuracy: 0.6404 - val_loss: 0.6221 - val_accuracy: 0.7123 - lr: 2.0000e-04\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 0.00022.\n",
      "Epoch 11/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6141 - accuracy: 0.6736\n",
      "Epoch 11: val_loss improved from 0.62209 to 0.61053, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.6156 - accuracy: 0.6712 - val_loss: 0.6105 - val_accuracy: 0.7055 - lr: 2.2000e-04\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 0.00024000000000000003.\n",
      "Epoch 12/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6104 - accuracy: 0.6684\n",
      "Epoch 12: val_loss did not improve from 0.61053\n",
      "19/19 [==============================] - 1s 66ms/step - loss: 0.6089 - accuracy: 0.6712 - val_loss: 0.6106 - val_accuracy: 0.7192 - lr: 2.4000e-04\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 0.00026000000000000003.\n",
      "Epoch 13/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6004 - accuracy: 0.6701\n",
      "Epoch 13: val_loss improved from 0.61053 to 0.61033, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.6023 - accuracy: 0.6678 - val_loss: 0.6103 - val_accuracy: 0.7123 - lr: 2.6000e-04\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 0.00028.\n",
      "Epoch 14/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5942 - accuracy: 0.6858\n",
      "Epoch 14: val_loss improved from 0.61033 to 0.60560, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.5967 - accuracy: 0.6815 - val_loss: 0.6056 - val_accuracy: 0.7192 - lr: 2.8000e-04\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 0.00030000000000000003.\n",
      "Epoch 15/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5882 - accuracy: 0.6927\n",
      "Epoch 15: val_loss improved from 0.60560 to 0.60062, saving model to model.h5\n",
      "19/19 [==============================] - 2s 95ms/step - loss: 0.5894 - accuracy: 0.6901 - val_loss: 0.6006 - val_accuracy: 0.7055 - lr: 3.0000e-04\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 0.00032.\n",
      "Epoch 16/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5817 - accuracy: 0.6875\n",
      "Epoch 16: val_loss improved from 0.60062 to 0.59576, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.5834 - accuracy: 0.6849 - val_loss: 0.5958 - val_accuracy: 0.6849 - lr: 3.2000e-04\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 0.00034.\n",
      "Epoch 17/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5761 - accuracy: 0.7083\n",
      "Epoch 17: val_loss improved from 0.59576 to 0.59088, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.5775 - accuracy: 0.7072 - val_loss: 0.5909 - val_accuracy: 0.6849 - lr: 3.4000e-04\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 0.00036.\n",
      "Epoch 18/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5731 - accuracy: 0.6979\n",
      "Epoch 18: val_loss did not improve from 0.59088\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5720 - accuracy: 0.7021 - val_loss: 0.5918 - val_accuracy: 0.7123 - lr: 3.6000e-04\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 0.00038.\n",
      "Epoch 19/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5610 - accuracy: 0.7014\n",
      "Epoch 19: val_loss improved from 0.59088 to 0.58732, saving model to model.h5\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.5635 - accuracy: 0.6986 - val_loss: 0.5873 - val_accuracy: 0.6849 - lr: 3.8000e-04\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 0.0004.\n",
      "Epoch 20/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5541 - accuracy: 0.7205\n",
      "Epoch 20: val_loss did not improve from 0.58732\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5566 - accuracy: 0.7192 - val_loss: 0.5900 - val_accuracy: 0.7123 - lr: 4.0000e-04\n",
      "\n",
      "Epoch 21: LearningRateScheduler setting learning rate to 0.0004.\n",
      "Epoch 21/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5520 - accuracy: 0.7292\n",
      "Epoch 21: val_loss did not improve from 0.58732\n",
      "19/19 [==============================] - 1s 69ms/step - loss: 0.5532 - accuracy: 0.7260 - val_loss: 0.5912 - val_accuracy: 0.7123 - lr: 4.0000e-04\n",
      "\n",
      "Epoch 22: LearningRateScheduler setting learning rate to 0.0004.\n",
      "Epoch 22/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5434 - accuracy: 0.7431\n",
      "Epoch 22: val_loss did not improve from 0.58732\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5444 - accuracy: 0.7414 - val_loss: 0.5884 - val_accuracy: 0.7123 - lr: 4.0000e-04\n",
      "\n",
      "Epoch 23: LearningRateScheduler setting learning rate to 0.0004.\n",
      "Epoch 23/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5437 - accuracy: 0.7344\n",
      "Epoch 23: val_loss improved from 0.58732 to 0.57801, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.5419 - accuracy: 0.7380 - val_loss: 0.5780 - val_accuracy: 0.6849 - lr: 4.0000e-04\n",
      "\n",
      "Epoch 24: LearningRateScheduler setting learning rate to 0.0004.\n",
      "Epoch 24/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5434 - accuracy: 0.7344\n",
      "Epoch 24: val_loss did not improve from 0.57801\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5433 - accuracy: 0.7329 - val_loss: 0.5930 - val_accuracy: 0.7055 - lr: 4.0000e-04\n",
      "\n",
      "Epoch 25: LearningRateScheduler setting learning rate to 0.0004.\n",
      "Epoch 25/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5326 - accuracy: 0.7309\n",
      "Epoch 25: val_loss did not improve from 0.57801\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5311 - accuracy: 0.7346 - val_loss: 0.5972 - val_accuracy: 0.6918 - lr: 4.0000e-04\n",
      "\n",
      "Epoch 26: LearningRateScheduler setting learning rate to 0.0004.\n",
      "Epoch 26/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5283 - accuracy: 0.7396\n",
      "Epoch 26: val_loss improved from 0.57801 to 0.57635, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.5291 - accuracy: 0.7397 - val_loss: 0.5763 - val_accuracy: 0.6986 - lr: 4.0000e-04\n",
      "\n",
      "Epoch 27: LearningRateScheduler setting learning rate to 0.0004.\n",
      "Epoch 27/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5175 - accuracy: 0.7517\n",
      "Epoch 27: val_loss did not improve from 0.57635\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5170 - accuracy: 0.7517 - val_loss: 0.5777 - val_accuracy: 0.7192 - lr: 4.0000e-04\n",
      "\n",
      "Epoch 28: LearningRateScheduler setting learning rate to 0.0004.\n",
      "Epoch 28/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5073 - accuracy: 0.7830\n",
      "Epoch 28: val_loss did not improve from 0.57635\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5105 - accuracy: 0.7757 - val_loss: 0.5809 - val_accuracy: 0.7329 - lr: 4.0000e-04\n",
      "\n",
      "Epoch 29: LearningRateScheduler setting learning rate to 0.0004.\n",
      "Epoch 29/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5048 - accuracy: 0.7812\n",
      "Epoch 29: val_loss improved from 0.57635 to 0.57455, saving model to model.h5\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.5060 - accuracy: 0.7791 - val_loss: 0.5745 - val_accuracy: 0.7123 - lr: 4.0000e-04\n",
      "\n",
      "Epoch 30: LearningRateScheduler setting learning rate to 0.0004.\n",
      "Epoch 30/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5002 - accuracy: 0.7743\n",
      "Epoch 30: val_loss did not improve from 0.57455\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5000 - accuracy: 0.7757 - val_loss: 0.5826 - val_accuracy: 0.7397 - lr: 4.0000e-04\n",
      "\n",
      "Epoch 31: LearningRateScheduler setting learning rate to 0.00030000000000000003.\n",
      "Epoch 31/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4931 - accuracy: 0.7882\n",
      "Epoch 31: val_loss did not improve from 0.57455\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.4971 - accuracy: 0.7842 - val_loss: 0.5820 - val_accuracy: 0.7329 - lr: 3.0000e-04\n",
      "\n",
      "Epoch 32: LearningRateScheduler setting learning rate to 0.0002813473286151601.\n",
      "Epoch 32/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4898 - accuracy: 0.7899\n",
      "Epoch 32: val_loss improved from 0.57455 to 0.57356, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.4904 - accuracy: 0.7894 - val_loss: 0.5736 - val_accuracy: 0.7123 - lr: 2.8135e-04\n",
      "\n",
      "Epoch 33: LearningRateScheduler setting learning rate to 0.00026180339887498953.\n",
      "Epoch 33/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4882 - accuracy: 0.8021\n",
      "Epoch 33: val_loss did not improve from 0.57356\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.4874 - accuracy: 0.8031 - val_loss: 0.5814 - val_accuracy: 0.7192 - lr: 2.6180e-04\n",
      "\n",
      "Epoch 34: LearningRateScheduler setting learning rate to 0.00024158233816355185.\n",
      "Epoch 34/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4842 - accuracy: 0.7882\n",
      "Epoch 34: val_loss did not improve from 0.57356\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.4854 - accuracy: 0.7877 - val_loss: 0.5742 - val_accuracy: 0.7260 - lr: 2.4158e-04\n",
      "\n",
      "Epoch 35: LearningRateScheduler setting learning rate to 0.00022090569265353072.\n",
      "Epoch 35/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4832 - accuracy: 0.7934\n",
      "Epoch 35: val_loss did not improve from 0.57356\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.4838 - accuracy: 0.7928 - val_loss: 0.5758 - val_accuracy: 0.7397 - lr: 2.2091e-04\n",
      "\n",
      "Epoch 36: LearningRateScheduler setting learning rate to 0.0002.\n",
      "Epoch 36/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4793 - accuracy: 0.7934\n",
      "Epoch 36: val_loss did not improve from 0.57356\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.4793 - accuracy: 0.7945 - val_loss: 0.5754 - val_accuracy: 0.7329 - lr: 2.0000e-04\n",
      "\n",
      "Epoch 37: LearningRateScheduler setting learning rate to 0.00017909430734646935.\n",
      "Epoch 37/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4752 - accuracy: 0.8021\n",
      "Epoch 37: val_loss did not improve from 0.57356\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.4759 - accuracy: 0.8014 - val_loss: 0.5766 - val_accuracy: 0.7260 - lr: 1.7909e-04\n",
      "\n",
      "Epoch 38: LearningRateScheduler setting learning rate to 0.00015841766183644817.\n",
      "Epoch 38/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4744 - accuracy: 0.7986\n",
      "Epoch 38: val_loss improved from 0.57356 to 0.57146, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.4735 - accuracy: 0.7979 - val_loss: 0.5715 - val_accuracy: 0.7260 - lr: 1.5842e-04\n",
      "\n",
      "Epoch 39: LearningRateScheduler setting learning rate to 0.00013819660112501054.\n",
      "Epoch 39/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4737 - accuracy: 0.8021\n",
      "Epoch 39: val_loss did not improve from 0.57146\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.4740 - accuracy: 0.7997 - val_loss: 0.5742 - val_accuracy: 0.7260 - lr: 1.3820e-04\n",
      "\n",
      "Epoch 40: LearningRateScheduler setting learning rate to 0.00011865267138483999.\n",
      "Epoch 40/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4694 - accuracy: 0.8090\n",
      "Epoch 40: val_loss did not improve from 0.57146\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.4714 - accuracy: 0.8048 - val_loss: 0.5741 - val_accuracy: 0.7329 - lr: 1.1865e-04\n",
      "\n",
      "Epoch 41: LearningRateScheduler setting learning rate to 0.00010000000000000005.\n",
      "Epoch 41/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4693 - accuracy: 0.8038\n",
      "Epoch 41: val_loss did not improve from 0.57146\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.4699 - accuracy: 0.8048 - val_loss: 0.5775 - val_accuracy: 0.7260 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 42: LearningRateScheduler setting learning rate to 8.24429495415054e-05.\n",
      "Epoch 42/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4666 - accuracy: 0.8090\n",
      "Epoch 42: val_loss improved from 0.57146 to 0.57127, saving model to model.h5\n",
      "19/19 [==============================] - 2s 97ms/step - loss: 0.4675 - accuracy: 0.8099 - val_loss: 0.5713 - val_accuracy: 0.7123 - lr: 8.2443e-05\n",
      "\n",
      "Epoch 43: LearningRateScheduler setting learning rate to 6.617387872822842e-05.\n",
      "Epoch 43/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4646 - accuracy: 0.8073\n",
      "Epoch 43: val_loss did not improve from 0.57127\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.4673 - accuracy: 0.8031 - val_loss: 0.5768 - val_accuracy: 0.7192 - lr: 6.6174e-05\n",
      "\n",
      "Epoch 44: LearningRateScheduler setting learning rate to 5.1371034904521134e-05.\n",
      "Epoch 44/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4640 - accuracy: 0.8073\n",
      "Epoch 44: val_loss did not improve from 0.57127\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.4654 - accuracy: 0.8065 - val_loss: 0.5752 - val_accuracy: 0.7329 - lr: 5.1371e-05\n",
      "\n",
      "Epoch 45: LearningRateScheduler setting learning rate to 3.819660112501053e-05.\n",
      "Epoch 45/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4645 - accuracy: 0.8125\n",
      "Epoch 45: val_loss did not improve from 0.57127\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.4644 - accuracy: 0.8116 - val_loss: 0.5727 - val_accuracy: 0.7329 - lr: 3.8197e-05\n",
      "\n",
      "Epoch 46: LearningRateScheduler setting learning rate to 2.679491924311226e-05.\n",
      "Epoch 46/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4634 - accuracy: 0.8142\n",
      "Epoch 46: val_loss did not improve from 0.57127\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.4643 - accuracy: 0.8116 - val_loss: 0.5736 - val_accuracy: 0.7397 - lr: 2.6795e-05\n",
      "\n",
      "Epoch 47: LearningRateScheduler setting learning rate to 1.7290908471479805e-05.\n",
      "Epoch 47/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4665 - accuracy: 0.8125\n",
      "Epoch 47: val_loss did not improve from 0.57127\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.4638 - accuracy: 0.8151 - val_loss: 0.5765 - val_accuracy: 0.7192 - lr: 1.7291e-05\n",
      "\n",
      "Epoch 48: LearningRateScheduler setting learning rate to 9.788696740969295e-06.\n",
      "Epoch 48/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4639 - accuracy: 0.8090\n",
      "Epoch 48: val_loss did not improve from 0.57127\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.4636 - accuracy: 0.8099 - val_loss: 0.5762 - val_accuracy: 0.7192 - lr: 9.7887e-06\n",
      "\n",
      "Epoch 49: LearningRateScheduler setting learning rate to 4.370479853238863e-06.\n",
      "Epoch 49/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4658 - accuracy: 0.8108\n",
      "Epoch 49: val_loss did not improve from 0.57127\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.4633 - accuracy: 0.8116 - val_loss: 0.5758 - val_accuracy: 0.7192 - lr: 4.3705e-06\n",
      "\n",
      "Epoch 50: LearningRateScheduler setting learning rate to 1.0956209263453421e-06.\n",
      "Epoch 50/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4635 - accuracy: 0.8142\n",
      "Epoch 50: val_loss did not improve from 0.57127\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.4633 - accuracy: 0.8134 - val_loss: 0.5755 - val_accuracy: 0.7192 - lr: 1.0956e-06\n",
      "\n",
      "Epoch 51: LearningRateScheduler setting learning rate to 0.0.\n",
      "Epoch 51/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4600 - accuracy: 0.8160\n",
      "Epoch 51: val_loss did not improve from 0.57127\n",
      "19/19 [==============================] - 1s 66ms/step - loss: 0.4632 - accuracy: 0.8134 - val_loss: 0.5755 - val_accuracy: 0.7192 - lr: 0.0000e+00\n",
      "\n",
      "Epoch 52: LearningRateScheduler setting learning rate to 1.0956209263453421e-06.\n",
      "Epoch 52/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4599 - accuracy: 0.8160\n",
      "Epoch 52: val_loss did not improve from 0.57127\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.4632 - accuracy: 0.8134 - val_loss: 0.5755 - val_accuracy: 0.7192 - lr: 1.0956e-06\n",
      "Epoch 52: early stopping\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 2.25e-05.\n",
      "Epoch 1/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.7197 - accuracy: 0.4722\n",
      "Epoch 1: val_loss improved from inf to 0.72665, saving model to model.h5\n",
      "19/19 [==============================] - 4s 130ms/step - loss: 0.7211 - accuracy: 0.4692 - val_loss: 0.7267 - val_accuracy: 0.4178 - lr: 2.2500e-05\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 4.5e-05.\n",
      "Epoch 2/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.7126 - accuracy: 0.4913\n",
      "Epoch 2: val_loss improved from 0.72665 to 0.72188, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.7129 - accuracy: 0.4897 - val_loss: 0.7219 - val_accuracy: 0.3904 - lr: 4.5000e-05\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 6.75e-05.\n",
      "Epoch 3/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.7037 - accuracy: 0.4688\n",
      "Epoch 3: val_loss improved from 0.72188 to 0.71546, saving model to model.h5\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.7028 - accuracy: 0.4709 - val_loss: 0.7155 - val_accuracy: 0.4384 - lr: 6.7500e-05\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 9e-05.\n",
      "Epoch 4/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6914 - accuracy: 0.5278\n",
      "Epoch 4: val_loss improved from 0.71546 to 0.69949, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.6904 - accuracy: 0.5325 - val_loss: 0.6995 - val_accuracy: 0.4384 - lr: 9.0000e-05\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.00011250000000000001.\n",
      "Epoch 5/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6780 - accuracy: 0.5729\n",
      "Epoch 5: val_loss improved from 0.69949 to 0.68396, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.6773 - accuracy: 0.5753 - val_loss: 0.6840 - val_accuracy: 0.5205 - lr: 1.1250e-04\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.000135.\n",
      "Epoch 6/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6613 - accuracy: 0.6267\n",
      "Epoch 6: val_loss improved from 0.68396 to 0.67405, saving model to model.h5\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.6622 - accuracy: 0.6233 - val_loss: 0.6741 - val_accuracy: 0.5411 - lr: 1.3500e-04\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.00015750000000000003.\n",
      "Epoch 7/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6509 - accuracy: 0.6250\n",
      "Epoch 7: val_loss improved from 0.67405 to 0.65621, saving model to model.h5\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.6502 - accuracy: 0.6284 - val_loss: 0.6562 - val_accuracy: 0.6370 - lr: 1.5750e-04\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.00018.\n",
      "Epoch 8/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6365 - accuracy: 0.6580\n",
      "Epoch 8: val_loss improved from 0.65621 to 0.64669, saving model to model.h5\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.6371 - accuracy: 0.6558 - val_loss: 0.6467 - val_accuracy: 0.6575 - lr: 1.8000e-04\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.00020250000000000004.\n",
      "Epoch 9/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6255 - accuracy: 0.6684\n",
      "Epoch 9: val_loss improved from 0.64669 to 0.63633, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.6248 - accuracy: 0.6695 - val_loss: 0.6363 - val_accuracy: 0.6781 - lr: 2.0250e-04\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.00022500000000000002.\n",
      "Epoch 10/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6156 - accuracy: 0.6719\n",
      "Epoch 10: val_loss improved from 0.63633 to 0.62669, saving model to model.h5\n",
      "19/19 [==============================] - 2s 95ms/step - loss: 0.6150 - accuracy: 0.6712 - val_loss: 0.6267 - val_accuracy: 0.6918 - lr: 2.2500e-04\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 0.0002475.\n",
      "Epoch 11/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6102 - accuracy: 0.6806\n",
      "Epoch 11: val_loss improved from 0.62669 to 0.62277, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.6090 - accuracy: 0.6815 - val_loss: 0.6228 - val_accuracy: 0.6781 - lr: 2.4750e-04\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 0.00027.\n",
      "Epoch 12/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6014 - accuracy: 0.6684\n",
      "Epoch 12: val_loss improved from 0.62277 to 0.61263, saving model to model.h5\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.5980 - accuracy: 0.6729 - val_loss: 0.6126 - val_accuracy: 0.7260 - lr: 2.7000e-04\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 0.0002925.\n",
      "Epoch 13/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5897 - accuracy: 0.7066\n",
      "Epoch 13: val_loss improved from 0.61263 to 0.61116, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.5894 - accuracy: 0.7072 - val_loss: 0.6112 - val_accuracy: 0.6849 - lr: 2.9250e-04\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 0.00031500000000000007.\n",
      "Epoch 14/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5864 - accuracy: 0.6840\n",
      "Epoch 14: val_loss improved from 0.61116 to 0.60635, saving model to model.h5\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.5862 - accuracy: 0.6832 - val_loss: 0.6063 - val_accuracy: 0.7055 - lr: 3.1500e-04\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 0.0003375.\n",
      "Epoch 15/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5763 - accuracy: 0.6997\n",
      "Epoch 15: val_loss improved from 0.60635 to 0.59242, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.5774 - accuracy: 0.6969 - val_loss: 0.5924 - val_accuracy: 0.7260 - lr: 3.3750e-04\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 0.00036.\n",
      "Epoch 16/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5750 - accuracy: 0.6892\n",
      "Epoch 16: val_loss did not improve from 0.59242\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5731 - accuracy: 0.6918 - val_loss: 0.6039 - val_accuracy: 0.6918 - lr: 3.6000e-04\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 0.00038250000000000003.\n",
      "Epoch 17/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5613 - accuracy: 0.7188\n",
      "Epoch 17: val_loss did not improve from 0.59242\n",
      "19/19 [==============================] - 1s 70ms/step - loss: 0.5631 - accuracy: 0.7158 - val_loss: 0.5926 - val_accuracy: 0.7397 - lr: 3.8250e-04\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 0.0004050000000000001.\n",
      "Epoch 18/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5589 - accuracy: 0.7222\n",
      "Epoch 18: val_loss improved from 0.59242 to 0.58257, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.5572 - accuracy: 0.7260 - val_loss: 0.5826 - val_accuracy: 0.7329 - lr: 4.0500e-04\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 0.00042750000000000004.\n",
      "Epoch 19/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5465 - accuracy: 0.7240\n",
      "Epoch 19: val_loss improved from 0.58257 to 0.58239, saving model to model.h5\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.5455 - accuracy: 0.7243 - val_loss: 0.5824 - val_accuracy: 0.7260 - lr: 4.2750e-04\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 0.00045000000000000004.\n",
      "Epoch 20/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5373 - accuracy: 0.7326\n",
      "Epoch 20: val_loss improved from 0.58239 to 0.58160, saving model to model.h5\n",
      "19/19 [==============================] - 2s 95ms/step - loss: 0.5370 - accuracy: 0.7329 - val_loss: 0.5816 - val_accuracy: 0.7397 - lr: 4.5000e-04\n",
      "\n",
      "Epoch 21: LearningRateScheduler setting learning rate to 0.00045000000000000004.\n",
      "Epoch 21/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5304 - accuracy: 0.7274\n",
      "Epoch 21: val_loss improved from 0.58160 to 0.57546, saving model to model.h5\n",
      "19/19 [==============================] - 2s 96ms/step - loss: 0.5290 - accuracy: 0.7312 - val_loss: 0.5755 - val_accuracy: 0.7260 - lr: 4.5000e-04\n",
      "\n",
      "Epoch 22: LearningRateScheduler setting learning rate to 0.00045000000000000004.\n",
      "Epoch 22/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5239 - accuracy: 0.7552\n",
      "Epoch 22: val_loss did not improve from 0.57546\n",
      "19/19 [==============================] - 1s 74ms/step - loss: 0.5248 - accuracy: 0.7551 - val_loss: 0.5784 - val_accuracy: 0.7329 - lr: 4.5000e-04\n",
      "\n",
      "Epoch 23: LearningRateScheduler setting learning rate to 0.00045000000000000004.\n",
      "Epoch 23/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5166 - accuracy: 0.7413\n",
      "Epoch 23: val_loss improved from 0.57546 to 0.57203, saving model to model.h5\n",
      "19/19 [==============================] - 2s 95ms/step - loss: 0.5169 - accuracy: 0.7397 - val_loss: 0.5720 - val_accuracy: 0.7329 - lr: 4.5000e-04\n",
      "\n",
      "Epoch 24: LearningRateScheduler setting learning rate to 0.00045000000000000004.\n",
      "Epoch 24/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5141 - accuracy: 0.7587\n",
      "Epoch 24: val_loss did not improve from 0.57203\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5111 - accuracy: 0.7620 - val_loss: 0.5726 - val_accuracy: 0.7192 - lr: 4.5000e-04\n",
      "\n",
      "Epoch 25: LearningRateScheduler setting learning rate to 0.00045000000000000004.\n",
      "Epoch 25/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5048 - accuracy: 0.7691\n",
      "Epoch 25: val_loss did not improve from 0.57203\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5064 - accuracy: 0.7671 - val_loss: 0.5876 - val_accuracy: 0.7329 - lr: 4.5000e-04\n",
      "\n",
      "Epoch 26: LearningRateScheduler setting learning rate to 0.00045000000000000004.\n",
      "Epoch 26/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4977 - accuracy: 0.7656\n",
      "Epoch 26: val_loss improved from 0.57203 to 0.56963, saving model to model.h5\n",
      "19/19 [==============================] - 2s 95ms/step - loss: 0.4974 - accuracy: 0.7688 - val_loss: 0.5696 - val_accuracy: 0.7329 - lr: 4.5000e-04\n",
      "\n",
      "Epoch 27: LearningRateScheduler setting learning rate to 0.00045000000000000004.\n",
      "Epoch 27/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4887 - accuracy: 0.7760\n",
      "Epoch 27: val_loss did not improve from 0.56963\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.4908 - accuracy: 0.7740 - val_loss: 0.5874 - val_accuracy: 0.7192 - lr: 4.5000e-04\n",
      "\n",
      "Epoch 28: LearningRateScheduler setting learning rate to 0.00045000000000000004.\n",
      "Epoch 28/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4845 - accuracy: 0.7778\n",
      "Epoch 28: val_loss improved from 0.56963 to 0.56567, saving model to model.h5\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.4850 - accuracy: 0.7774 - val_loss: 0.5657 - val_accuracy: 0.7397 - lr: 4.5000e-04\n",
      "\n",
      "Epoch 29: LearningRateScheduler setting learning rate to 0.00045000000000000004.\n",
      "Epoch 29/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4829 - accuracy: 0.7743\n",
      "Epoch 29: val_loss did not improve from 0.56567\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.4818 - accuracy: 0.7757 - val_loss: 0.5661 - val_accuracy: 0.7534 - lr: 4.5000e-04\n",
      "\n",
      "Epoch 30: LearningRateScheduler setting learning rate to 0.00045000000000000004.\n",
      "Epoch 30/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4766 - accuracy: 0.7882\n",
      "Epoch 30: val_loss improved from 0.56567 to 0.56407, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.4774 - accuracy: 0.7877 - val_loss: 0.5641 - val_accuracy: 0.7055 - lr: 4.5000e-04\n",
      "\n",
      "Epoch 31: LearningRateScheduler setting learning rate to 0.0003375.\n",
      "Epoch 31/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4690 - accuracy: 0.7865\n",
      "Epoch 31: val_loss did not improve from 0.56407\n",
      "19/19 [==============================] - 1s 66ms/step - loss: 0.4693 - accuracy: 0.7860 - val_loss: 0.5642 - val_accuracy: 0.7123 - lr: 3.3750e-04\n",
      "\n",
      "Epoch 32: LearningRateScheduler setting learning rate to 0.0003165157446920551.\n",
      "Epoch 32/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4691 - accuracy: 0.7865\n",
      "Epoch 32: val_loss did not improve from 0.56407\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.4684 - accuracy: 0.7894 - val_loss: 0.5668 - val_accuracy: 0.7055 - lr: 3.1652e-04\n",
      "\n",
      "Epoch 33: LearningRateScheduler setting learning rate to 0.00029452882373436323.\n",
      "Epoch 33/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4593 - accuracy: 0.8090\n",
      "Epoch 33: val_loss improved from 0.56407 to 0.56107, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.4614 - accuracy: 0.8065 - val_loss: 0.5611 - val_accuracy: 0.7329 - lr: 2.9453e-04\n",
      "\n",
      "Epoch 34: LearningRateScheduler setting learning rate to 0.00027178013043399586.\n",
      "Epoch 34/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4669 - accuracy: 0.7934\n",
      "Epoch 34: val_loss improved from 0.56107 to 0.56099, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.4651 - accuracy: 0.7945 - val_loss: 0.5610 - val_accuracy: 0.7260 - lr: 2.7178e-04\n",
      "\n",
      "Epoch 35: LearningRateScheduler setting learning rate to 0.00024851890423522205.\n",
      "Epoch 35/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4572 - accuracy: 0.7986\n",
      "Epoch 35: val_loss did not improve from 0.56099\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.4571 - accuracy: 0.7979 - val_loss: 0.5660 - val_accuracy: 0.7055 - lr: 2.4852e-04\n",
      "\n",
      "Epoch 36: LearningRateScheduler setting learning rate to 0.00022500000000000002.\n",
      "Epoch 36/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4527 - accuracy: 0.8090\n",
      "Epoch 36: val_loss did not improve from 0.56099\n",
      "19/19 [==============================] - 1s 66ms/step - loss: 0.4521 - accuracy: 0.8099 - val_loss: 0.5623 - val_accuracy: 0.7260 - lr: 2.2500e-04\n",
      "\n",
      "Epoch 37: LearningRateScheduler setting learning rate to 0.00020148109576477802.\n",
      "Epoch 37/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4503 - accuracy: 0.8212\n",
      "Epoch 37: val_loss did not improve from 0.56099\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.4495 - accuracy: 0.8219 - val_loss: 0.5672 - val_accuracy: 0.7123 - lr: 2.0148e-04\n",
      "\n",
      "Epoch 38: LearningRateScheduler setting learning rate to 0.0001782198695660042.\n",
      "Epoch 38/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4460 - accuracy: 0.8177\n",
      "Epoch 38: val_loss did not improve from 0.56099\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.4457 - accuracy: 0.8202 - val_loss: 0.5643 - val_accuracy: 0.7123 - lr: 1.7822e-04\n",
      "\n",
      "Epoch 39: LearningRateScheduler setting learning rate to 0.00015547117626563687.\n",
      "Epoch 39/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4437 - accuracy: 0.8229\n",
      "Epoch 39: val_loss did not improve from 0.56099\n",
      "19/19 [==============================] - 1s 66ms/step - loss: 0.4435 - accuracy: 0.8236 - val_loss: 0.5674 - val_accuracy: 0.7192 - lr: 1.5547e-04\n",
      "\n",
      "Epoch 40: LearningRateScheduler setting learning rate to 0.000133484255307945.\n",
      "Epoch 40/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4443 - accuracy: 0.8212\n",
      "Epoch 40: val_loss did not improve from 0.56099\n",
      "19/19 [==============================] - 1s 71ms/step - loss: 0.4424 - accuracy: 0.8236 - val_loss: 0.5647 - val_accuracy: 0.7123 - lr: 1.3348e-04\n",
      "\n",
      "Epoch 41: LearningRateScheduler setting learning rate to 0.00011250000000000006.\n",
      "Epoch 41/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4418 - accuracy: 0.8194\n",
      "Epoch 41: val_loss did not improve from 0.56099\n",
      "19/19 [==============================] - 1s 66ms/step - loss: 0.4411 - accuracy: 0.8202 - val_loss: 0.5636 - val_accuracy: 0.7192 - lr: 1.1250e-04\n",
      "\n",
      "Epoch 42: LearningRateScheduler setting learning rate to 9.274831823419358e-05.\n",
      "Epoch 42/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4392 - accuracy: 0.8247\n",
      "Epoch 42: val_loss did not improve from 0.56099\n",
      "19/19 [==============================] - 1s 66ms/step - loss: 0.4397 - accuracy: 0.8253 - val_loss: 0.5635 - val_accuracy: 0.7260 - lr: 9.2748e-05\n",
      "\n",
      "Epoch 43: LearningRateScheduler setting learning rate to 7.444561356925698e-05.\n",
      "Epoch 43/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4401 - accuracy: 0.8194\n",
      "Epoch 43: val_loss did not improve from 0.56099\n",
      "19/19 [==============================] - 1s 66ms/step - loss: 0.4375 - accuracy: 0.8219 - val_loss: 0.5634 - val_accuracy: 0.7192 - lr: 7.4446e-05\n",
      "\n",
      "Epoch 44: LearningRateScheduler setting learning rate to 5.779241426758628e-05.\n",
      "Epoch 44/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4379 - accuracy: 0.8247\n",
      "Epoch 44: val_loss did not improve from 0.56099\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.4368 - accuracy: 0.8271 - val_loss: 0.5653 - val_accuracy: 0.7192 - lr: 5.7792e-05\n",
      "Epoch 44: early stopping\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 2.5e-05.\n",
      "Epoch 1/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.7174 - accuracy: 0.4931\n",
      "Epoch 1: val_loss improved from inf to 0.68919, saving model to model.h5\n",
      "19/19 [==============================] - 5s 157ms/step - loss: 0.7180 - accuracy: 0.4914 - val_loss: 0.6892 - val_accuracy: 0.5479 - lr: 2.5000e-05\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 5e-05.\n",
      "Epoch 2/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6998 - accuracy: 0.5035\n",
      "Epoch 2: val_loss improved from 0.68919 to 0.68575, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.7018 - accuracy: 0.5000 - val_loss: 0.6858 - val_accuracy: 0.5205 - lr: 5.0000e-05\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 7.500000000000001e-05.\n",
      "Epoch 3/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6896 - accuracy: 0.5156\n",
      "Epoch 3: val_loss improved from 0.68575 to 0.68076, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.6888 - accuracy: 0.5205 - val_loss: 0.6808 - val_accuracy: 0.5205 - lr: 7.5000e-05\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 4/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6776 - accuracy: 0.5677\n",
      "Epoch 4: val_loss improved from 0.68076 to 0.66472, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.6780 - accuracy: 0.5685 - val_loss: 0.6647 - val_accuracy: 0.6164 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.000125.\n",
      "Epoch 5/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6643 - accuracy: 0.6076\n",
      "Epoch 5: val_loss improved from 0.66472 to 0.65168, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.6654 - accuracy: 0.6027 - val_loss: 0.6517 - val_accuracy: 0.6370 - lr: 1.2500e-04\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.00015000000000000001.\n",
      "Epoch 6/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6556 - accuracy: 0.6215\n",
      "Epoch 6: val_loss improved from 0.65168 to 0.64009, saving model to model.h5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.6546 - accuracy: 0.6233 - val_loss: 0.6401 - val_accuracy: 0.6164 - lr: 1.5000e-04\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.000175.\n",
      "Epoch 7/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6457 - accuracy: 0.6389\n",
      "Epoch 7: val_loss improved from 0.64009 to 0.62712, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.6432 - accuracy: 0.6438 - val_loss: 0.6271 - val_accuracy: 0.6438 - lr: 1.7500e-04\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.0002.\n",
      "Epoch 8/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6335 - accuracy: 0.6632\n",
      "Epoch 8: val_loss improved from 0.62712 to 0.61752, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.6338 - accuracy: 0.6627 - val_loss: 0.6175 - val_accuracy: 0.6781 - lr: 2.0000e-04\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.00022500000000000002.\n",
      "Epoch 9/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6258 - accuracy: 0.6562\n",
      "Epoch 9: val_loss improved from 0.61752 to 0.61088, saving model to model.h5\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.6247 - accuracy: 0.6610 - val_loss: 0.6109 - val_accuracy: 0.6781 - lr: 2.2500e-04\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.00025.\n",
      "Epoch 10/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6187 - accuracy: 0.6684\n",
      "Epoch 10: val_loss improved from 0.61088 to 0.59946, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.6179 - accuracy: 0.6712 - val_loss: 0.5995 - val_accuracy: 0.7192 - lr: 2.5000e-04\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 0.00027499999999999996.\n",
      "Epoch 11/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6118 - accuracy: 0.6736\n",
      "Epoch 11: val_loss did not improve from 0.59946\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.6100 - accuracy: 0.6764 - val_loss: 0.6027 - val_accuracy: 0.6644 - lr: 2.7500e-04\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 0.00030000000000000003.\n",
      "Epoch 12/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6030 - accuracy: 0.6771\n",
      "Epoch 12: val_loss improved from 0.59946 to 0.59128, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.6026 - accuracy: 0.6781 - val_loss: 0.5913 - val_accuracy: 0.7123 - lr: 3.0000e-04\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 0.00032500000000000004.\n",
      "Epoch 13/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5945 - accuracy: 0.6753\n",
      "Epoch 13: val_loss improved from 0.59128 to 0.58893, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.5943 - accuracy: 0.6764 - val_loss: 0.5889 - val_accuracy: 0.7055 - lr: 3.2500e-04\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 0.00035.\n",
      "Epoch 14/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5917 - accuracy: 0.6892\n",
      "Epoch 14: val_loss did not improve from 0.58893\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5925 - accuracy: 0.6901 - val_loss: 0.5898 - val_accuracy: 0.7055 - lr: 3.5000e-04\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 0.000375.\n",
      "Epoch 15/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5827 - accuracy: 0.6979\n",
      "Epoch 15: val_loss did not improve from 0.58893\n",
      "19/19 [==============================] - 1s 66ms/step - loss: 0.5826 - accuracy: 0.6986 - val_loss: 0.5918 - val_accuracy: 0.6849 - lr: 3.7500e-04\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 0.0004.\n",
      "Epoch 16/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5765 - accuracy: 0.7049\n",
      "Epoch 16: val_loss improved from 0.58893 to 0.57862, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.5766 - accuracy: 0.7055 - val_loss: 0.5786 - val_accuracy: 0.7192 - lr: 4.0000e-04\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 0.00042500000000000003.\n",
      "Epoch 17/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5714 - accuracy: 0.6944\n",
      "Epoch 17: val_loss improved from 0.57862 to 0.57583, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.5691 - accuracy: 0.6969 - val_loss: 0.5758 - val_accuracy: 0.7329 - lr: 4.2500e-04\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 0.00045000000000000004.\n",
      "Epoch 18/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5632 - accuracy: 0.7083\n",
      "Epoch 18: val_loss did not improve from 0.57583\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5629 - accuracy: 0.7089 - val_loss: 0.5761 - val_accuracy: 0.7260 - lr: 4.5000e-04\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 0.000475.\n",
      "Epoch 19/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5522 - accuracy: 0.7240\n",
      "Epoch 19: val_loss did not improve from 0.57583\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5547 - accuracy: 0.7226 - val_loss: 0.5761 - val_accuracy: 0.6986 - lr: 4.7500e-04\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 0.0005.\n",
      "Epoch 20/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5511 - accuracy: 0.7292\n",
      "Epoch 20: val_loss improved from 0.57583 to 0.57557, saving model to model.h5\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.5505 - accuracy: 0.7295 - val_loss: 0.5756 - val_accuracy: 0.7055 - lr: 5.0000e-04\n",
      "\n",
      "Epoch 21: LearningRateScheduler setting learning rate to 0.0005.\n",
      "Epoch 21/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5428 - accuracy: 0.7344\n",
      "Epoch 21: val_loss did not improve from 0.57557\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.5460 - accuracy: 0.7295 - val_loss: 0.5795 - val_accuracy: 0.7055 - lr: 5.0000e-04\n",
      "\n",
      "Epoch 22: LearningRateScheduler setting learning rate to 0.0005.\n",
      "Epoch 22/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5365 - accuracy: 0.7361\n",
      "Epoch 22: val_loss improved from 0.57557 to 0.57266, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.5356 - accuracy: 0.7380 - val_loss: 0.5727 - val_accuracy: 0.7123 - lr: 5.0000e-04\n",
      "\n",
      "Epoch 23: LearningRateScheduler setting learning rate to 0.0005.\n",
      "Epoch 23/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5281 - accuracy: 0.7535\n",
      "Epoch 23: val_loss improved from 0.57266 to 0.56543, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.5264 - accuracy: 0.7551 - val_loss: 0.5654 - val_accuracy: 0.7329 - lr: 5.0000e-04\n",
      "\n",
      "Epoch 24: LearningRateScheduler setting learning rate to 0.0005.\n",
      "Epoch 24/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5224 - accuracy: 0.7483\n",
      "Epoch 24: val_loss did not improve from 0.56543\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5225 - accuracy: 0.7466 - val_loss: 0.5735 - val_accuracy: 0.7329 - lr: 5.0000e-04\n",
      "\n",
      "Epoch 25: LearningRateScheduler setting learning rate to 0.0005.\n",
      "Epoch 25/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5152 - accuracy: 0.7674\n",
      "Epoch 25: val_loss did not improve from 0.56543\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5164 - accuracy: 0.7654 - val_loss: 0.5705 - val_accuracy: 0.6986 - lr: 5.0000e-04\n",
      "\n",
      "Epoch 26: LearningRateScheduler setting learning rate to 0.0005.\n",
      "Epoch 26/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5130 - accuracy: 0.7587\n",
      "Epoch 26: val_loss improved from 0.56543 to 0.56321, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.5120 - accuracy: 0.7620 - val_loss: 0.5632 - val_accuracy: 0.7123 - lr: 5.0000e-04\n",
      "\n",
      "Epoch 27: LearningRateScheduler setting learning rate to 0.0005.\n",
      "Epoch 27/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5072 - accuracy: 0.7726\n",
      "Epoch 27: val_loss did not improve from 0.56321\n",
      "19/19 [==============================] - 1s 66ms/step - loss: 0.5056 - accuracy: 0.7723 - val_loss: 0.5699 - val_accuracy: 0.7397 - lr: 5.0000e-04\n",
      "\n",
      "Epoch 28: LearningRateScheduler setting learning rate to 0.0005.\n",
      "Epoch 28/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5070 - accuracy: 0.7604\n",
      "Epoch 28: val_loss did not improve from 0.56321\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5058 - accuracy: 0.7603 - val_loss: 0.5724 - val_accuracy: 0.7466 - lr: 5.0000e-04\n",
      "\n",
      "Epoch 29: LearningRateScheduler setting learning rate to 0.0005.\n",
      "Epoch 29/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4981 - accuracy: 0.7812\n",
      "Epoch 29: val_loss did not improve from 0.56321\n",
      "19/19 [==============================] - 1s 66ms/step - loss: 0.5006 - accuracy: 0.7791 - val_loss: 0.5845 - val_accuracy: 0.6849 - lr: 5.0000e-04\n",
      "\n",
      "Epoch 30: LearningRateScheduler setting learning rate to 0.0005.\n",
      "Epoch 30/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4945 - accuracy: 0.7847\n",
      "Epoch 30: val_loss did not improve from 0.56321\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.4945 - accuracy: 0.7842 - val_loss: 0.5713 - val_accuracy: 0.7397 - lr: 5.0000e-04\n",
      "\n",
      "Epoch 31: LearningRateScheduler setting learning rate to 0.000375.\n",
      "Epoch 31/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4808 - accuracy: 0.7986\n",
      "Epoch 31: val_loss improved from 0.56321 to 0.56041, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.4820 - accuracy: 0.7962 - val_loss: 0.5604 - val_accuracy: 0.6918 - lr: 3.7500e-04\n",
      "\n",
      "Epoch 32: LearningRateScheduler setting learning rate to 0.0003516841607689501.\n",
      "Epoch 32/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4788 - accuracy: 0.7934\n",
      "Epoch 32: val_loss did not improve from 0.56041\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.4768 - accuracy: 0.7928 - val_loss: 0.5622 - val_accuracy: 0.6918 - lr: 3.5168e-04\n",
      "\n",
      "Epoch 33: LearningRateScheduler setting learning rate to 0.00032725424859373687.\n",
      "Epoch 33/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4732 - accuracy: 0.7917\n",
      "Epoch 33: val_loss did not improve from 0.56041\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.4741 - accuracy: 0.7911 - val_loss: 0.5932 - val_accuracy: 0.6849 - lr: 3.2725e-04\n",
      "\n",
      "Epoch 34: LearningRateScheduler setting learning rate to 0.0003019779227044398.\n",
      "Epoch 34/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4911 - accuracy: 0.7795\n",
      "Epoch 34: val_loss improved from 0.56041 to 0.55995, saving model to model.h5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.4893 - accuracy: 0.7791 - val_loss: 0.5599 - val_accuracy: 0.7055 - lr: 3.0198e-04\n",
      "\n",
      "Epoch 35: LearningRateScheduler setting learning rate to 0.0002761321158169134.\n",
      "Epoch 35/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4661 - accuracy: 0.7934\n",
      "Epoch 35: val_loss did not improve from 0.55995\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.4666 - accuracy: 0.7945 - val_loss: 0.5623 - val_accuracy: 0.6918 - lr: 2.7613e-04\n",
      "\n",
      "Epoch 36: LearningRateScheduler setting learning rate to 0.00025.\n",
      "Epoch 36/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4645 - accuracy: 0.7986\n",
      "Epoch 36: val_loss improved from 0.55995 to 0.55948, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.4633 - accuracy: 0.7997 - val_loss: 0.5595 - val_accuracy: 0.6918 - lr: 2.5000e-04\n",
      "\n",
      "Epoch 37: LearningRateScheduler setting learning rate to 0.00022386788418308668.\n",
      "Epoch 37/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4583 - accuracy: 0.8003\n",
      "Epoch 37: val_loss did not improve from 0.55948\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.4602 - accuracy: 0.7997 - val_loss: 0.5658 - val_accuracy: 0.7397 - lr: 2.2387e-04\n",
      "\n",
      "Epoch 38: LearningRateScheduler setting learning rate to 0.0001980220772955602.\n",
      "Epoch 38/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4610 - accuracy: 0.8073\n",
      "Epoch 38: val_loss did not improve from 0.55948\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.4597 - accuracy: 0.8082 - val_loss: 0.5602 - val_accuracy: 0.6918 - lr: 1.9802e-04\n",
      "\n",
      "Epoch 39: LearningRateScheduler setting learning rate to 0.00017274575140626317.\n",
      "Epoch 39/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4582 - accuracy: 0.8177\n",
      "Epoch 39: val_loss did not improve from 0.55948\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.4573 - accuracy: 0.8168 - val_loss: 0.5624 - val_accuracy: 0.7055 - lr: 1.7275e-04\n",
      "\n",
      "Epoch 40: LearningRateScheduler setting learning rate to 0.00014831583923105.\n",
      "Epoch 40/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4555 - accuracy: 0.8090\n",
      "Epoch 40: val_loss did not improve from 0.55948\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.4560 - accuracy: 0.8116 - val_loss: 0.5603 - val_accuracy: 0.6781 - lr: 1.4832e-04\n",
      "\n",
      "Epoch 41: LearningRateScheduler setting learning rate to 0.00012500000000000006.\n",
      "Epoch 41/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4518 - accuracy: 0.8073\n",
      "Epoch 41: val_loss improved from 0.55948 to 0.55863, saving model to model.h5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.4521 - accuracy: 0.8082 - val_loss: 0.5586 - val_accuracy: 0.6849 - lr: 1.2500e-04\n",
      "\n",
      "Epoch 42: LearningRateScheduler setting learning rate to 0.00010305368692688174.\n",
      "Epoch 42/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4500 - accuracy: 0.8142\n",
      "Epoch 42: val_loss did not improve from 0.55863\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.4505 - accuracy: 0.8134 - val_loss: 0.5618 - val_accuracy: 0.6986 - lr: 1.0305e-04\n",
      "\n",
      "Epoch 43: LearningRateScheduler setting learning rate to 8.271734841028553e-05.\n",
      "Epoch 43/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4481 - accuracy: 0.8229\n",
      "Epoch 43: val_loss did not improve from 0.55863\n",
      "19/19 [==============================] - 1s 66ms/step - loss: 0.4497 - accuracy: 0.8202 - val_loss: 0.5589 - val_accuracy: 0.6849 - lr: 8.2717e-05\n",
      "\n",
      "Epoch 44: LearningRateScheduler setting learning rate to 6.421379363065141e-05.\n",
      "Epoch 44/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4487 - accuracy: 0.8160\n",
      "Epoch 44: val_loss did not improve from 0.55863\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.4484 - accuracy: 0.8151 - val_loss: 0.5614 - val_accuracy: 0.6849 - lr: 6.4214e-05\n",
      "\n",
      "Epoch 45: LearningRateScheduler setting learning rate to 4.7745751406263163e-05.\n",
      "Epoch 45/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4481 - accuracy: 0.8160\n",
      "Epoch 45: val_loss did not improve from 0.55863\n",
      "19/19 [==============================] - 1s 66ms/step - loss: 0.4474 - accuracy: 0.8151 - val_loss: 0.5617 - val_accuracy: 0.6849 - lr: 4.7746e-05\n",
      "\n",
      "Epoch 46: LearningRateScheduler setting learning rate to 3.3493649053890325e-05.\n",
      "Epoch 46/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4453 - accuracy: 0.8264\n",
      "Epoch 46: val_loss did not improve from 0.55863\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.4472 - accuracy: 0.8236 - val_loss: 0.5606 - val_accuracy: 0.6849 - lr: 3.3494e-05\n",
      "\n",
      "Epoch 47: LearningRateScheduler setting learning rate to 2.1613635589349755e-05.\n",
      "Epoch 47/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4458 - accuracy: 0.8194\n",
      "Epoch 47: val_loss did not improve from 0.55863\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.4466 - accuracy: 0.8185 - val_loss: 0.5614 - val_accuracy: 0.6849 - lr: 2.1614e-05\n",
      "\n",
      "Epoch 48: LearningRateScheduler setting learning rate to 1.2235870926211617e-05.\n",
      "Epoch 48/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4475 - accuracy: 0.8194\n",
      "Epoch 48: val_loss did not improve from 0.55863\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.4461 - accuracy: 0.8202 - val_loss: 0.5609 - val_accuracy: 0.6849 - lr: 1.2236e-05\n",
      "\n",
      "Epoch 49: LearningRateScheduler setting learning rate to 5.463099816548578e-06.\n",
      "Epoch 49/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4481 - accuracy: 0.8177\n",
      "Epoch 49: val_loss did not improve from 0.55863\n",
      "19/19 [==============================] - 1s 66ms/step - loss: 0.4459 - accuracy: 0.8202 - val_loss: 0.5604 - val_accuracy: 0.6849 - lr: 5.4631e-06\n",
      "\n",
      "Epoch 50: LearningRateScheduler setting learning rate to 1.3695261579316775e-06.\n",
      "Epoch 50/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4456 - accuracy: 0.8212\n",
      "Epoch 50: val_loss did not improve from 0.55863\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.4457 - accuracy: 0.8202 - val_loss: 0.5605 - val_accuracy: 0.6849 - lr: 1.3695e-06\n",
      "\n",
      "Epoch 51: LearningRateScheduler setting learning rate to 0.0.\n",
      "Epoch 51/60\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4432 - accuracy: 0.8247\n",
      "Epoch 51: val_loss did not improve from 0.55863\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.4457 - accuracy: 0.8236 - val_loss: 0.5605 - val_accuracy: 0.6849 - lr: 0.0000e+00\n",
      "Epoch 51: early stopping\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# MobileNetV2\n",
    "for i in range(10):\n",
    "\n",
    "    def lrate(epoch):\n",
    "        # Warmup\n",
    "        wrmup = 20\n",
    "        peak = 0.00005*(i+1)\n",
    "        stay = 10\n",
    "        lenght_decay = 60\n",
    "        if epoch < wrmup:\n",
    "            return peak * (epoch+1) / wrmup\n",
    "        elif epoch < wrmup + stay:\n",
    "            return peak\n",
    "        else:\n",
    "            return peak/2 * (1 + np.cos((epoch-wrmup) / (  lenght_decay - stay - wrmup) * np.pi))\n",
    "\n",
    "    train_model_mobilenetv2(X_train_o, X_test_o, y_train, y_test, X_test_o, y_test,  lrate, 'mobilenetv2-'+str(i), 60)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vdgaete/anaconda3/envs/py310/lib/python3.10/site-packages/vit_keras/utils.py:81: UserWarning: Resizing position embeddings from 24, 24 to 14, 14\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 2.2222222222222223e-05.\n",
      "Epoch 1/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 1.0114 - accuracy: 0.4914\n",
      "Epoch 1: val_loss improved from inf to 0.85286, saving model to vit-test-7.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vdgaete/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 26s 992ms/step - loss: 1.0114 - accuracy: 0.4914 - val_loss: 0.8529 - val_accuracy: 0.5205 - lr: 2.2222e-05\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 4.4444444444444447e-05.\n",
      "Epoch 2/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.9809 - accuracy: 0.4795\n",
      "Epoch 2: val_loss improved from 0.85286 to 0.81672, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 17s 885ms/step - loss: 0.9809 - accuracy: 0.4795 - val_loss: 0.8167 - val_accuracy: 0.5000 - lr: 4.4444e-05\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 6.666666666666667e-05.\n",
      "Epoch 3/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.9247 - accuracy: 0.4880\n",
      "Epoch 3: val_loss improved from 0.81672 to 0.77653, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 866ms/step - loss: 0.9247 - accuracy: 0.4880 - val_loss: 0.7765 - val_accuracy: 0.4863 - lr: 6.6667e-05\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 8.888888888888889e-05.\n",
      "Epoch 4/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.8564 - accuracy: 0.4743\n",
      "Epoch 4: val_loss improved from 0.77653 to 0.74502, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 857ms/step - loss: 0.8564 - accuracy: 0.4743 - val_loss: 0.7450 - val_accuracy: 0.5205 - lr: 8.8889e-05\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.00011111111111111112.\n",
      "Epoch 5/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.8078 - accuracy: 0.4760\n",
      "Epoch 5: val_loss improved from 0.74502 to 0.72770, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 17s 873ms/step - loss: 0.8078 - accuracy: 0.4760 - val_loss: 0.7277 - val_accuracy: 0.5274 - lr: 1.1111e-04\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.00013333333333333334.\n",
      "Epoch 6/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.7847 - accuracy: 0.4709\n",
      "Epoch 6: val_loss improved from 0.72770 to 0.72019, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 864ms/step - loss: 0.7847 - accuracy: 0.4709 - val_loss: 0.7202 - val_accuracy: 0.5753 - lr: 1.3333e-04\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.00015555555555555556.\n",
      "Epoch 7/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.7879 - accuracy: 0.4777\n",
      "Epoch 7: val_loss improved from 0.72019 to 0.71621, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 866ms/step - loss: 0.7879 - accuracy: 0.4777 - val_loss: 0.7162 - val_accuracy: 0.5890 - lr: 1.5556e-04\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.00017777777777777779.\n",
      "Epoch 8/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.7466 - accuracy: 0.5308\n",
      "Epoch 8: val_loss improved from 0.71621 to 0.71323, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 865ms/step - loss: 0.7466 - accuracy: 0.5308 - val_loss: 0.7132 - val_accuracy: 0.5890 - lr: 1.7778e-04\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.0002.\n",
      "Epoch 9/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.7610 - accuracy: 0.5068\n",
      "Epoch 9: val_loss improved from 0.71323 to 0.71155, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 865ms/step - loss: 0.7610 - accuracy: 0.5068 - val_loss: 0.7116 - val_accuracy: 0.5616 - lr: 2.0000e-04\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.00022222222222222223.\n",
      "Epoch 10/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.7493 - accuracy: 0.5308\n",
      "Epoch 10: val_loss improved from 0.71155 to 0.70424, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 865ms/step - loss: 0.7493 - accuracy: 0.5308 - val_loss: 0.7042 - val_accuracy: 0.5753 - lr: 2.2222e-04\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 0.0002444444444444445.\n",
      "Epoch 11/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.7402 - accuracy: 0.5205\n",
      "Epoch 11: val_loss improved from 0.70424 to 0.69425, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 868ms/step - loss: 0.7402 - accuracy: 0.5205 - val_loss: 0.6943 - val_accuracy: 0.5890 - lr: 2.4444e-04\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 0.0002666666666666667.\n",
      "Epoch 12/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.7481 - accuracy: 0.5274\n",
      "Epoch 12: val_loss improved from 0.69425 to 0.68764, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 868ms/step - loss: 0.7481 - accuracy: 0.5274 - val_loss: 0.6876 - val_accuracy: 0.6027 - lr: 2.6667e-04\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 0.00028888888888888893.\n",
      "Epoch 13/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.7282 - accuracy: 0.5325\n",
      "Epoch 13: val_loss improved from 0.68764 to 0.68077, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 867ms/step - loss: 0.7282 - accuracy: 0.5325 - val_loss: 0.6808 - val_accuracy: 0.6164 - lr: 2.8889e-04\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 0.0003111111111111111.\n",
      "Epoch 14/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.7178 - accuracy: 0.5445\n",
      "Epoch 14: val_loss improved from 0.68077 to 0.67522, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 17s 870ms/step - loss: 0.7178 - accuracy: 0.5445 - val_loss: 0.6752 - val_accuracy: 0.6164 - lr: 3.1111e-04\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 0.0003333333333333333.\n",
      "Epoch 15/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.7190 - accuracy: 0.5514\n",
      "Epoch 15: val_loss improved from 0.67522 to 0.66345, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 865ms/step - loss: 0.7190 - accuracy: 0.5514 - val_loss: 0.6634 - val_accuracy: 0.6301 - lr: 3.3333e-04\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 0.00035555555555555557.\n",
      "Epoch 16/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.7071 - accuracy: 0.5514\n",
      "Epoch 16: val_loss improved from 0.66345 to 0.65742, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 872ms/step - loss: 0.7071 - accuracy: 0.5514 - val_loss: 0.6574 - val_accuracy: 0.6370 - lr: 3.5556e-04\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 0.0003777777777777778.\n",
      "Epoch 17/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.7042 - accuracy: 0.5462\n",
      "Epoch 17: val_loss improved from 0.65742 to 0.64777, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 865ms/step - loss: 0.7042 - accuracy: 0.5462 - val_loss: 0.6478 - val_accuracy: 0.6507 - lr: 3.7778e-04\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 0.0004.\n",
      "Epoch 18/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6747 - accuracy: 0.6096\n",
      "Epoch 18: val_loss improved from 0.64777 to 0.64032, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 863ms/step - loss: 0.6747 - accuracy: 0.6096 - val_loss: 0.6403 - val_accuracy: 0.6575 - lr: 4.0000e-04\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 0.0004.\n",
      "Epoch 19/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6768 - accuracy: 0.5788\n",
      "Epoch 19: val_loss improved from 0.64032 to 0.63205, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 860ms/step - loss: 0.6768 - accuracy: 0.5788 - val_loss: 0.6320 - val_accuracy: 0.6712 - lr: 4.0000e-04\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 0.0004.\n",
      "Epoch 20/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6793 - accuracy: 0.5753\n",
      "Epoch 20: val_loss improved from 0.63205 to 0.62623, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 863ms/step - loss: 0.6793 - accuracy: 0.5753 - val_loss: 0.6262 - val_accuracy: 0.6644 - lr: 4.0000e-04\n",
      "\n",
      "Epoch 21: LearningRateScheduler setting learning rate to 0.0004.\n",
      "Epoch 21/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6941 - accuracy: 0.5719\n",
      "Epoch 21: val_loss improved from 0.62623 to 0.61841, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 864ms/step - loss: 0.6941 - accuracy: 0.5719 - val_loss: 0.6184 - val_accuracy: 0.6918 - lr: 4.0000e-04\n",
      "\n",
      "Epoch 22: LearningRateScheduler setting learning rate to 0.0004.\n",
      "Epoch 22/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6677 - accuracy: 0.5839\n",
      "Epoch 22: val_loss improved from 0.61841 to 0.61217, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 867ms/step - loss: 0.6677 - accuracy: 0.5839 - val_loss: 0.6122 - val_accuracy: 0.6918 - lr: 4.0000e-04\n",
      "\n",
      "Epoch 23: LearningRateScheduler setting learning rate to 0.0004.\n",
      "Epoch 23/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6707 - accuracy: 0.6130\n",
      "Epoch 23: val_loss improved from 0.61217 to 0.60342, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 868ms/step - loss: 0.6707 - accuracy: 0.6130 - val_loss: 0.6034 - val_accuracy: 0.6986 - lr: 4.0000e-04\n",
      "\n",
      "Epoch 24: LearningRateScheduler setting learning rate to 0.0004.\n",
      "Epoch 24/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6553 - accuracy: 0.6284\n",
      "Epoch 24: val_loss improved from 0.60342 to 0.59958, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 17s 875ms/step - loss: 0.6553 - accuracy: 0.6284 - val_loss: 0.5996 - val_accuracy: 0.7192 - lr: 4.0000e-04\n",
      "\n",
      "Epoch 25: LearningRateScheduler setting learning rate to 0.0004.\n",
      "Epoch 25/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6541 - accuracy: 0.6096\n",
      "Epoch 25: val_loss did not improve from 0.59958\n",
      "19/19 [==============================] - 14s 758ms/step - loss: 0.6541 - accuracy: 0.6096 - val_loss: 0.6019 - val_accuracy: 0.7123 - lr: 4.0000e-04\n",
      "\n",
      "Epoch 26: LearningRateScheduler setting learning rate to 0.0004.\n",
      "Epoch 26/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6538 - accuracy: 0.5976\n",
      "Epoch 26: val_loss improved from 0.59958 to 0.59574, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 871ms/step - loss: 0.6538 - accuracy: 0.5976 - val_loss: 0.5957 - val_accuracy: 0.7260 - lr: 4.0000e-04\n",
      "\n",
      "Epoch 27: LearningRateScheduler setting learning rate to 0.0004.\n",
      "Epoch 27/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6422 - accuracy: 0.6421\n",
      "Epoch 27: val_loss improved from 0.59574 to 0.59401, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 869ms/step - loss: 0.6422 - accuracy: 0.6421 - val_loss: 0.5940 - val_accuracy: 0.7260 - lr: 4.0000e-04\n",
      "\n",
      "Epoch 28: LearningRateScheduler setting learning rate to 0.0004.\n",
      "Epoch 28/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6402 - accuracy: 0.6233\n",
      "Epoch 28: val_loss improved from 0.59401 to 0.58531, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 859ms/step - loss: 0.6402 - accuracy: 0.6233 - val_loss: 0.5853 - val_accuracy: 0.7329 - lr: 4.0000e-04\n",
      "\n",
      "Epoch 29: LearningRateScheduler setting learning rate to 0.00031111404660392046.\n",
      "Epoch 29/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6542 - accuracy: 0.6062\n",
      "Epoch 29: val_loss improved from 0.58531 to 0.58208, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 870ms/step - loss: 0.6542 - accuracy: 0.6062 - val_loss: 0.5821 - val_accuracy: 0.7329 - lr: 3.1111e-04\n",
      "\n",
      "Epoch 30: LearningRateScheduler setting learning rate to 0.0002942793473651996.\n",
      "Epoch 30/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6342 - accuracy: 0.6353\n",
      "Epoch 30: val_loss did not improve from 0.58208\n",
      "19/19 [==============================] - 14s 755ms/step - loss: 0.6342 - accuracy: 0.6353 - val_loss: 0.5838 - val_accuracy: 0.7329 - lr: 2.9428e-04\n",
      "\n",
      "Epoch 31: LearningRateScheduler setting learning rate to 0.000276536686473018.\n",
      "Epoch 31/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6492 - accuracy: 0.6336\n",
      "Epoch 31: val_loss did not improve from 0.58208\n",
      "19/19 [==============================] - 14s 758ms/step - loss: 0.6492 - accuracy: 0.6336 - val_loss: 0.5828 - val_accuracy: 0.7329 - lr: 2.7654e-04\n",
      "\n",
      "Epoch 32: LearningRateScheduler setting learning rate to 0.00025805693545089247.\n",
      "Epoch 32/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6419 - accuracy: 0.6387\n",
      "Epoch 32: val_loss improved from 0.58208 to 0.58153, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 865ms/step - loss: 0.6419 - accuracy: 0.6387 - val_loss: 0.5815 - val_accuracy: 0.7260 - lr: 2.5806e-04\n",
      "\n",
      "Epoch 33: LearningRateScheduler setting learning rate to 0.0002390180644032257.\n",
      "Epoch 33/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6444 - accuracy: 0.6558\n",
      "Epoch 33: val_loss improved from 0.58153 to 0.57882, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 864ms/step - loss: 0.6444 - accuracy: 0.6558 - val_loss: 0.5788 - val_accuracy: 0.7260 - lr: 2.3902e-04\n",
      "\n",
      "Epoch 34: LearningRateScheduler setting learning rate to 0.0002196034280659122.\n",
      "Epoch 34/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6256 - accuracy: 0.6558\n",
      "Epoch 34: val_loss improved from 0.57882 to 0.57749, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 864ms/step - loss: 0.6256 - accuracy: 0.6558 - val_loss: 0.5775 - val_accuracy: 0.7260 - lr: 2.1960e-04\n",
      "\n",
      "Epoch 35: LearningRateScheduler setting learning rate to 0.0002.\n",
      "Epoch 35/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6188 - accuracy: 0.6507\n",
      "Epoch 35: val_loss improved from 0.57749 to 0.57687, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 863ms/step - loss: 0.6188 - accuracy: 0.6507 - val_loss: 0.5769 - val_accuracy: 0.7260 - lr: 2.0000e-04\n",
      "\n",
      "Epoch 36: LearningRateScheduler setting learning rate to 0.00018039657193408788.\n",
      "Epoch 36/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6181 - accuracy: 0.6490\n",
      "Epoch 36: val_loss improved from 0.57687 to 0.57678, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 867ms/step - loss: 0.6181 - accuracy: 0.6490 - val_loss: 0.5768 - val_accuracy: 0.7260 - lr: 1.8040e-04\n",
      "\n",
      "Epoch 37: LearningRateScheduler setting learning rate to 0.00016098193559677438.\n",
      "Epoch 37/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6105 - accuracy: 0.6592\n",
      "Epoch 37: val_loss improved from 0.57678 to 0.57524, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 867ms/step - loss: 0.6105 - accuracy: 0.6592 - val_loss: 0.5752 - val_accuracy: 0.7260 - lr: 1.6098e-04\n",
      "\n",
      "Epoch 38: LearningRateScheduler setting learning rate to 0.00014194306454910757.\n",
      "Epoch 38/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6223 - accuracy: 0.6541\n",
      "Epoch 38: val_loss improved from 0.57524 to 0.57402, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 862ms/step - loss: 0.6223 - accuracy: 0.6541 - val_loss: 0.5740 - val_accuracy: 0.7260 - lr: 1.4194e-04\n",
      "\n",
      "Epoch 39: LearningRateScheduler setting learning rate to 0.00012346331352698205.\n",
      "Epoch 39/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6367 - accuracy: 0.6267\n",
      "Epoch 39: val_loss improved from 0.57402 to 0.57263, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 854ms/step - loss: 0.6367 - accuracy: 0.6267 - val_loss: 0.5726 - val_accuracy: 0.7260 - lr: 1.2346e-04\n",
      "\n",
      "Epoch 40: LearningRateScheduler setting learning rate to 0.00010572065263480046.\n",
      "Epoch 40/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6231 - accuracy: 0.6336\n",
      "Epoch 40: val_loss improved from 0.57263 to 0.57181, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 856ms/step - loss: 0.6231 - accuracy: 0.6336 - val_loss: 0.5718 - val_accuracy: 0.7260 - lr: 1.0572e-04\n",
      "\n",
      "Epoch 41: LearningRateScheduler setting learning rate to 8.888595339607961e-05.\n",
      "Epoch 41/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6195 - accuracy: 0.6490\n",
      "Epoch 41: val_loss improved from 0.57181 to 0.57149, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 849ms/step - loss: 0.6195 - accuracy: 0.6490 - val_loss: 0.5715 - val_accuracy: 0.7260 - lr: 8.8886e-05\n",
      "\n",
      "Epoch 42: LearningRateScheduler setting learning rate to 7.312134316727093e-05.\n",
      "Epoch 42/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6212 - accuracy: 0.6627\n",
      "Epoch 42: val_loss did not improve from 0.57149\n",
      "19/19 [==============================] - 14s 753ms/step - loss: 0.6212 - accuracy: 0.6627 - val_loss: 0.5718 - val_accuracy: 0.7260 - lr: 7.3121e-05\n",
      "\n",
      "Epoch 43: LearningRateScheduler setting learning rate to 5.857864376269051e-05.\n",
      "Epoch 43/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6189 - accuracy: 0.6678\n",
      "Epoch 43: val_loss improved from 0.57149 to 0.57067, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 865ms/step - loss: 0.6189 - accuracy: 0.6678 - val_loss: 0.5707 - val_accuracy: 0.7329 - lr: 5.8579e-05\n",
      "\n",
      "Epoch 44: LearningRateScheduler setting learning rate to 4.53979093274526e-05.\n",
      "Epoch 44/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6166 - accuracy: 0.6627\n",
      "Epoch 44: val_loss improved from 0.57067 to 0.57027, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 862ms/step - loss: 0.6166 - accuracy: 0.6627 - val_loss: 0.5703 - val_accuracy: 0.7329 - lr: 4.5398e-05\n",
      "\n",
      "Epoch 45: LearningRateScheduler setting learning rate to 3.370607753949093e-05.\n",
      "Epoch 45/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6265 - accuracy: 0.6455\n",
      "Epoch 45: val_loss improved from 0.57027 to 0.56998, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 866ms/step - loss: 0.6265 - accuracy: 0.6455 - val_loss: 0.5700 - val_accuracy: 0.7329 - lr: 3.3706e-05\n",
      "\n",
      "Epoch 46: LearningRateScheduler setting learning rate to 2.3615747130329013e-05.\n",
      "Epoch 46/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6116 - accuracy: 0.6866\n",
      "Epoch 46: val_loss improved from 0.56998 to 0.56967, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 865ms/step - loss: 0.6116 - accuracy: 0.6866 - val_loss: 0.5697 - val_accuracy: 0.7329 - lr: 2.3616e-05\n",
      "\n",
      "Epoch 47: LearningRateScheduler setting learning rate to 1.5224093497742653e-05.\n",
      "Epoch 47/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6283 - accuracy: 0.6507\n",
      "Epoch 47: val_loss improved from 0.56967 to 0.56960, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 864ms/step - loss: 0.6283 - accuracy: 0.6507 - val_loss: 0.5696 - val_accuracy: 0.7329 - lr: 1.5224e-05\n",
      "\n",
      "Epoch 48: LearningRateScheduler setting learning rate to 8.611932853558236e-06.\n",
      "Epoch 48/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6136 - accuracy: 0.6712\n",
      "Epoch 48: val_loss improved from 0.56960 to 0.56953, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 864ms/step - loss: 0.6136 - accuracy: 0.6712 - val_loss: 0.5695 - val_accuracy: 0.7329 - lr: 8.6119e-06\n",
      "\n",
      "Epoch 49: LearningRateScheduler setting learning rate to 3.842943919353914e-06.\n",
      "Epoch 49/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6120 - accuracy: 0.6747\n",
      "Epoch 49: val_loss improved from 0.56953 to 0.56945, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 863ms/step - loss: 0.6120 - accuracy: 0.6747 - val_loss: 0.5695 - val_accuracy: 0.7329 - lr: 3.8429e-06\n",
      "\n",
      "Epoch 50: LearningRateScheduler setting learning rate to 9.630546655606364e-07.\n",
      "Epoch 50/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6024 - accuracy: 0.6490\n",
      "Epoch 50: val_loss did not improve from 0.56945\n",
      "19/19 [==============================] - 14s 753ms/step - loss: 0.6024 - accuracy: 0.6490 - val_loss: 0.5695 - val_accuracy: 0.7329 - lr: 9.6305e-07\n",
      "\n",
      "Epoch 51: LearningRateScheduler setting learning rate to 0.0.\n",
      "Epoch 51/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6342 - accuracy: 0.6404\n",
      "Epoch 51: val_loss did not improve from 0.56945\n",
      "19/19 [==============================] - 14s 755ms/step - loss: 0.6342 - accuracy: 0.6404 - val_loss: 0.5695 - val_accuracy: 0.7329 - lr: 0.0000e+00\n",
      "\n",
      "Epoch 52: LearningRateScheduler setting learning rate to 9.630546655606144e-07.\n",
      "Epoch 52/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6101 - accuracy: 0.6712\n",
      "Epoch 52: val_loss did not improve from 0.56945\n",
      "19/19 [==============================] - 14s 755ms/step - loss: 0.6101 - accuracy: 0.6712 - val_loss: 0.5695 - val_accuracy: 0.7329 - lr: 9.6305e-07\n",
      "\n",
      "Epoch 53: LearningRateScheduler setting learning rate to 3.842943919353914e-06.\n",
      "Epoch 53/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6295 - accuracy: 0.6438\n",
      "Epoch 53: val_loss improved from 0.56945 to 0.56942, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 866ms/step - loss: 0.6295 - accuracy: 0.6438 - val_loss: 0.5694 - val_accuracy: 0.7329 - lr: 3.8429e-06\n",
      "\n",
      "Epoch 54: LearningRateScheduler setting learning rate to 8.611932853558213e-06.\n",
      "Epoch 54/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6161 - accuracy: 0.6558\n",
      "Epoch 54: val_loss improved from 0.56942 to 0.56941, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 865ms/step - loss: 0.6161 - accuracy: 0.6558 - val_loss: 0.5694 - val_accuracy: 0.7329 - lr: 8.6119e-06\n",
      "\n",
      "Epoch 55: LearningRateScheduler setting learning rate to 1.5224093497742631e-05.\n",
      "Epoch 55/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6116 - accuracy: 0.6627\n",
      "Epoch 55: val_loss improved from 0.56941 to 0.56927, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 863ms/step - loss: 0.6116 - accuracy: 0.6627 - val_loss: 0.5693 - val_accuracy: 0.7329 - lr: 1.5224e-05\n",
      "\n",
      "Epoch 56: LearningRateScheduler setting learning rate to 2.3615747130328992e-05.\n",
      "Epoch 56/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6098 - accuracy: 0.6729\n",
      "Epoch 56: val_loss improved from 0.56927 to 0.56921, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 866ms/step - loss: 0.6098 - accuracy: 0.6729 - val_loss: 0.5692 - val_accuracy: 0.7329 - lr: 2.3616e-05\n",
      "\n",
      "Epoch 57: LearningRateScheduler setting learning rate to 3.370607753949091e-05.\n",
      "Epoch 57/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6192 - accuracy: 0.6575\n",
      "Epoch 57: val_loss did not improve from 0.56921\n",
      "19/19 [==============================] - 14s 757ms/step - loss: 0.6192 - accuracy: 0.6575 - val_loss: 0.5692 - val_accuracy: 0.7329 - lr: 3.3706e-05\n",
      "\n",
      "Epoch 58: LearningRateScheduler setting learning rate to 4.539790932745258e-05.\n",
      "Epoch 58/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6228 - accuracy: 0.6592\n",
      "Epoch 58: val_loss did not improve from 0.56921\n",
      "19/19 [==============================] - 14s 755ms/step - loss: 0.6228 - accuracy: 0.6592 - val_loss: 0.5694 - val_accuracy: 0.7260 - lr: 4.5398e-05\n",
      "\n",
      "Epoch 59: LearningRateScheduler setting learning rate to 5.857864376269047e-05.\n",
      "Epoch 59/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6189 - accuracy: 0.6507\n",
      "Epoch 59: val_loss improved from 0.56921 to 0.56901, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 865ms/step - loss: 0.6189 - accuracy: 0.6507 - val_loss: 0.5690 - val_accuracy: 0.7329 - lr: 5.8579e-05\n",
      "\n",
      "Epoch 60: LearningRateScheduler setting learning rate to 7.312134316727081e-05.\n",
      "Epoch 60/60\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6171 - accuracy: 0.6473\n",
      "Epoch 60: val_loss improved from 0.56901 to 0.56864, saving model to vit-test-7.h5\n",
      "19/19 [==============================] - 16s 854ms/step - loss: 0.6171 - accuracy: 0.6473 - val_loss: 0.5686 - val_accuracy: 0.7329 - lr: 7.3121e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# ViT\n",
    "\n",
    "\n",
    "def lrate(epoch):\n",
    "    # Warmup\n",
    "    wrmup = 18\n",
    "    peak = 0.0004\n",
    "    stay = 10\n",
    "    lenght_decay = 60\n",
    "    if epoch < wrmup:\n",
    "        return peak * (epoch+1) / wrmup\n",
    "    elif epoch < wrmup + stay:\n",
    "        return peak\n",
    "    else:\n",
    "        return peak/2 * (1 + np.cos((epoch-wrmup) / (  lenght_decay - stay - wrmup) * np.pi))\n",
    "\n",
    "train_model_vit(X_train_o, X_test_o, y_train, y_test, X_test_o, y_test,  lrate, 'vit-test-7', 60)\n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n",
      "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-base-patch4-window7-224-in22k and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([21841]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([21841, 1024]) in the checkpoint and torch.Size([2, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING: CPU random generator seem to be failing, disabling hardware random number generation\n",
      "WARNING: RDRND generated: 0xffffffff 0xffffffff 0xffffffff 0xffffffff\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=224x224 at 0x7F2FB0637A60>, 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "import torch\n",
    "import os\n",
    "UP_DIR = os.path.dirname(os.getcwd())\n",
    "UP_DIR = os.path.dirname(UP_DIR)\n",
    "DATASET_DIR = os.path.join(UP_DIR, 'data', 'dataset')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load processor\n",
    "processor = AutoImageProcessor.from_pretrained(\"microsoft/swin-base-patch4-window7-224-in22k\")\n",
    "# Load model with classification head for 2 classes\n",
    "model = AutoModelForImageClassification.from_pretrained(\"microsoft/swin-base-patch4-window7-224-in22k\", num_labels=2, ignore_mismatched_sizes=True)\n",
    "\n",
    "# train using trainer from hugging face\n",
    "from transformers import TrainingArguments, Trainer\n",
    "# mkdir results\n",
    "if not os.path.exists('results'):\n",
    "    os.makedirs('results')\n",
    "# mkdir logs\n",
    "if not os.path.exists('logs'):\n",
    "    os.makedirs('logs')\n",
    "    \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Load data\n",
    "\n",
    "from datasets import load_dataset\n",
    "import cv2\n",
    "dataset_train = load_dataset( 'imagefolder', data_dir=DATASET_DIR, split='train')\n",
    "dataset_test = load_dataset( 'imagefolder', data_dir=DATASET_DIR, split='test')\n",
    "dataset_val = load_dataset( 'imagefolder', data_dir=DATASET_DIR, split='validation')\n",
    "\n",
    "# print head \n",
    "print(dataset_train[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/525 [00:06<?, ? examples/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m: images, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: examples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Apply preprocess function\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m dataset_train \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m dataset_test \u001b[38;5;241m=\u001b[39m dataset_test\u001b[38;5;241m.\u001b[39mmap(preprocess_function, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     21\u001b[0m dataset_val \u001b[38;5;241m=\u001b[39m dataset_val\u001b[38;5;241m.\u001b[39mmap(preprocess_function, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/datasets/arrow_dataset.py:593\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    592\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 593\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    594\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/datasets/arrow_dataset.py:558\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    556\u001b[0m }\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 558\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/datasets/arrow_dataset.py:3105\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3100\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3101\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3102\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3103\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3104\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3105\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3106\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3107\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/datasets/arrow_dataset.py:3482\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3478\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   3479\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[1;32m   3480\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3481\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3482\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3485\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_same_num_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3486\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3487\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3488\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3489\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3490\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3491\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/datasets/arrow_dataset.py:3361\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3360\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3361\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3363\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3364\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3365\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m, in \u001b[0;36mpreprocess_function\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m      4\u001b[0m images \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m examples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Convert to numpy array\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marray(image)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# check shape\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(image\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Preprocess data\n",
    "def preprocess_function(examples):\n",
    "    # example image is PIL.PngImagePlugin.PngImageFile image mode=RGB size=224x224\n",
    "    images = []\n",
    "    for image in examples[\"image\"]:\n",
    "        # Convert to numpy array\n",
    "        image = np.array(image)\n",
    "        # check shape\n",
    "        print(image.shape)\n",
    "\n",
    "        images.append(image)\n",
    "    # Convert to tensor\n",
    "    images = torch.tensor(images)\n",
    "    # Normalize\n",
    "    images = images / 255.0\n",
    "    return {\"pixel_values\": images, \"label\": examples[\"label\"]}\n",
    "\n",
    "# Apply preprocess function\n",
    "dataset_train = dataset_train.map(preprocess_function, batched=True)\n",
    "dataset_test = dataset_test.map(preprocess_function, batched=True)\n",
    "dataset_val = dataset_val.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Verify tensor\n",
    "print(dataset_train['pixel_values'][0].shape)\n",
    "# Define trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_test,\n",
    "\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save model\n",
    "trainer.save_model(\"results\")\n",
    "\n",
    "# Evaluate model\n",
    "trainer.evaluate( eval_dataset=dataset_val)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f9016a0c940>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vdgaete/anaconda3/envs/py310/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdataset_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpixel_values\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "print(dataset_train['pixel_values'][0].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
